Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Cheng2021,
abstract = {With machine learning models being increasingly applied to various decision-making scenarios, people have spent growing efforts to make machine learning models more transparent and explainable. Among various explanation techniques, counterfactual explanations have the advantages of being human-friendly and actionable-a counterfactual explanation tells the user how to gain the desired prediction with minimal changes to the input. Besides, counterfactual explanations can also serve as efficient probes to the models' decisions. In this work, we exploit the potential of counterfactual explanations to understand and explore the behavior of machine learning models. We design DECE, an interactive visualization system that helps understand and explore a model's decisions on individual instances and data subsets, supporting users ranging from decision-subjects to model developers. DECE supports exploratory analysis of model decisions by combining the strengths of counterfactual explanations at instance- and subgroup-levels. We also introduce a set of interactions that enable users to customize the generation of counterfactual explanations to find more actionable ones that can suit their needs. Through three use cases and an expert interview, we demonstrate the effectiveness of DECE in supporting decision exploration tasks and instance explanations.},
archivePrefix = {arXiv},
arxivId = {2008.08353},
author = {Cheng, Furui and Ming, Yao and Qu, Huamin},
doi = {10.1109/TVCG.2020.3030342},
eprint = {2008.08353},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cheng, Ming, Qu - 2021 - DECE Decision Explorer with Counterfactual Explanations for Machine Learning Models(2).pdf:pdf},
issn = {19410506},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Counterfactual Explanation,Decision Making,Explainable Machine Learning,Tabular Data},
number = {2},
pages = {1438--1447},
pmid = {33074811},
title = {{DECE: Decision Explorer with Counterfactual Explanations for Machine Learning Models}},
volume = {27},
year = {2021}
}
@article{Nguyen2016,
abstract = {Deep neural networks (DNNs) have demonstrated state-of-the-art results on many pattern recognition tasks, especially vision classification problems. Understanding the inner workings of such computational brains is both fascinating basic science that is interesting in its own right - similar to why we study the human brain - and will enable researchers to further improve DNNs. One path to understanding how a neural network functions internally is to study what each of its neurons has learned to detect. One such method is called activation maximization (AM), which synthesizes an input (e.g. an image) that highly activates a neuron. Here we dramatically improve the qualitative state of the art of activation maximization by harnessing a powerful, learned prior: a deep generator network (DGN). The algorithm (1) generates qualitatively state-of-the-art synthetic images that look almost real, (2) reveals the features learned by each neuron in an interpretable way, (3) generalizes well to new datasets and somewhat well to different network architectures without requiring the prior to be relearned, and (4) can be considered as a high-quality generative method (in this case, by generating novel, creative, interesting, recognizable images).},
archivePrefix = {arXiv},
arxivId = {1605.09304},
author = {Nguyen, Anh and Dosovitskiy, Alexey and Yosinski, Jason and Brox, Thomas and Clune, Jeff},
eprint = {1605.09304},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nguyen et al. - 2016 - Synthesizing the preferred inputs for neurons in neural networks via deep generator networks.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {Nips},
pages = {3395--3403},
title = {{Synthesizing the preferred inputs for neurons in neural networks via deep generator networks}},
year = {2016}
}
@article{Selvaraju2016,
abstract = {We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing input regions that are 'important' for predictions -- or visual explanations. Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses class-specific gradient information to localize important regions. These localizations are combined with existing pixel-space visualizations to create a novel high-resolution and class-discriminative visualization called Guided Grad-CAM. These methods help better understand CNN-based models, including image captioning and visual question answering (VQA) models. We evaluate our visual explanations by measuring their ability to discriminate between classes, to inspire trust in humans, and their correlation with occlusion maps. Grad-CAM provides a new way to understand CNN-based models. We have released code, an online demo hosted on CloudCV, and a full version of this extended abstract.},
archivePrefix = {arXiv},
arxivId = {1611.07450},
author = {Selvaraju, Ramprasaath R and Das, Abhishek and Vedantam, Ramakrishna and Cogswell, Michael and Parikh, Devi and Batra, Dhruv},
eprint = {1611.07450},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Selvaraju et al. - 2016 - Grad-CAM Why did you say that.pdf:pdf},
pages = {1--4},
title = {{Grad-CAM: Why did you say that?}},
url = {http://arxiv.org/abs/1611.07450},
year = {2016}
}
@misc{Esper2020,
author = {Esper, M. T.},
title = {{AI ethical principles}},
url = {https://www.defense.gov/Newsroom/Releases/Release/Article/2091996/?dod-adoptsethical-principles-for-artificial-intelligence=, February 2020},
year = {2020}
}
@article{Alvarez-Melis2018,
abstract = {Most recent work on interpretability of complex machine learning models has focused on estimating a posteriori explanations for previously trained models around specific predictions. Self-explaining models where interpretability plays a key role already during learning have received much less attention. We propose three desiderata for explanations in general - explicitness, faithfulness, and stability - and show that existing methods do not satisfy them. In response, we design self-explaining models in stages, progressively generalizing linear classifiers to complex yet architecturally explicit models. Faithfulness and stability are enforced via regularization specifically tailored to such models. Experimental results across various benchmark datasets show that our framework offers a promising direction for reconciling model complexity and interpretability.},
archivePrefix = {arXiv},
arxivId = {1806.07538},
author = {Alvarez-Melis, David and Jaakkola, Tommi S.},
eprint = {1806.07538},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Alvarez-Melis, Jaakkola - 2018 - Towards robust interpretability with self-explaining neural networks(2).pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {NeurIPS},
pages = {7775--7784},
title = {{Towards robust interpretability with self-explaining neural networks}},
volume = {2018-Decem},
year = {2018}
}
@article{Weinzierl2020,
abstract = {Predictive business process monitoring (PBPM) is a class of techniques designed to predict behaviour, such as next activities, in running traces. PBPM techniques aim to improve process performance by providing predictions to process analysts, supporting them in their decision making. However, the PBPM techniques' limited predictive quality was considered as the essential obstacle for establishing such techniques in practice. With the use of deep neural networks (DNNs), the techniques' predictive quality could be improved for tasks like the next activity prediction. While DNNs achieve a promising predictive quality, they still lack comprehensibility due to their hierarchical approach of learning representations. Nevertheless, process analysts need to comprehend the cause of a prediction to identify intervention mechanisms that might affect the decision making to secure process performance. In this paper, we propose XNAP, the first explainable, DNN-based PBPM technique for the next activity prediction. XNAP integrates a layer-wise relevance propagation method from the field of explainable artificial intelligence to make predictions of a long short-term memory DNN explainable by providing relevance values for activities. We show the benefit of our approach through two real-life event logs.},
archivePrefix = {arXiv},
arxivId = {2008.07993},
author = {Weinzierl, Sven and Zilker, Sandra and Brunk, Jens and Revoredo, Kate and Matzner, Martin and Becker, J{\"{o}}rg},
doi = {10.1007/978-3-030-66498-5_10},
eprint = {2008.07993},
file = {:home/anna/Desktop/prediction explanation/attribution-based/propagation/lrp/XNAP - Making LSTM-based Next Activity Predictions Explainable by Using LRP.pdf:pdf},
isbn = {9783030664978},
issn = {18651356},
journal = {Lecture Notes in Business Information Processing},
keywords = {Business process management,Deep learning,Explainable artificial intelligence,Layer-wise relevance propagation,Predictive business process monitoring,Process mining},
pages = {129--141},
title = {{XNAP: Making LSTM-Based Next Activity Predictions Explainable by Using LRP}},
volume = {397},
year = {2020}
}
@article{Manuscript2020,
abstract = {Cladding austenitic stainless steels are trendy these days in pressure vessels to enhance surface qualities. In this work, austenitic stainless steel clad layers deposited by flux cored arc welding process on structural steel plates used in boiler construction are investigated. To facilitate this, composite clad layers are produced with 316L stainless steel deposits on low carbon structural steel plates based on the design of experiments. Results exhibit an incremental trend of thermal conductivity with respect to percentage of weld dilution. A linear mathematical relationship is established between the percentage of weld dilution and post-weld thermal conductivity of clad layers for the prediction of thermal conductivity of clad layer deposits. These results could be supportive in the fabrication industries for producing energy efficient thermal equipment},
author = {Manuscript, Accepted},
title = {{Ac ce us}},
year = {2020}
}
@article{Zednik2019,
abstract = {Many of the computing systems programmed using Machine Learning are opaque: it is difficult to know why they do what they do or how they work. The Explainable Artificial Intelligence research program aims to develop analytic techniques with which to render opaque computing systems transparent, but lacks a normative framework with which to evaluate these techniques' explanatory success. The aim of the present discussion is to develop such a framework, while paying particular attention to different stakeholders' distinct explanatory requirements. Building on an analysis of ‘opacity' from philosophy of science, this framework is modeled after David Marr's influential account of explanation in cognitive science. Thus, the framework distinguishes between the different questions that might be asked about an opaque computing system, and specifies the general way in which these questions should be answered. By applying this normative framework to current techniques such as input heatmapping, feature-detector identification, and diagnostic classification, it will be possible to determine whether and to what extent the Black Box Problem can be solved.},
archivePrefix = {arXiv},
arxivId = {1903.04361},
author = {Zednik, Carlos},
eprint = {1903.04361},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zednik - 2019 - Solving the Black Box Problem A Normative Framework for Explainable Artificial Intelligence.pdf:pdf},
issn = {23318422},
journal = {Philosophy & Technology},
keywords = {Artificial intelligence,Black box problem,Epistemi,artificial intelligence,black box problem,epistemic opacity,explainable,levels of analysis,machine learning,scientific explanation},
pages = {265--288},
publisher = {Philosophy & Technology},
title = {{Solving the Black Box Problem: A Normative Framework for Explainable Artificial Intelligence}},
volume = {34},
year = {2021}
}
@article{Mahajan2019,
abstract = {To construct interpretable explanations that are consistent with the original ML model, counterfactual examples---showing how the model's output changes with small perturbations to the input---have been proposed. This paper extends the work in counterfactual explanations by addressing the challenge of feasibility of such examples. For explanations of ML models in critical domains such as healthcare and finance, counterfactual examples are useful for an end-user only to the extent that perturbation of feature inputs is feasible in the real world. We formulate the problem of feasibility as preserving causal relationships among input features and present a method that uses (partial) structural causal models to generate actionable counterfactuals. When feasibility constraints cannot be easily expressed, we consider an alternative mechanism where people can label generated CF examples on feasibility: whether it is feasible to intervene and realize the candidate CF example from the original input. To learn from this labelled feasibility data, we propose a modified variational auto encoder loss for generating CF examples that optimizes for feasibility as people interact with its output. Our experiments on Bayesian networks and the widely used ''Adult-Income'' dataset show that our proposed methods can generate counterfactual explanations that better satisfy feasibility constraints than existing methods.. Code repository can be accessed here: \textit{https://github.com/divyat09/cf-feasibility}},
archivePrefix = {arXiv},
arxivId = {1912.03277},
author = {Mahajan, Divyat and Tan, Chenhao and Sharma, Amit},
eprint = {1912.03277},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mahajan, Tan, Sharma - 2019 - Preserving Causal Constraints in Counterfactual Explanations for Machine Learning Classifiers(2).pdf:pdf},
number = {NeurIPS},
title = {{Preserving Causal Constraints in Counterfactual Explanations for Machine Learning Classifiers}},
url = {http://arxiv.org/abs/1912.03277},
year = {2019}
}
@article{Schutt2018,
abstract = {Deep learning has led to a paradigm shift in artificial intelligence, including web, text, and image search, speech recognition, as well as bioinformatics, with growing impact in chemical physics. Machine learning, in general, and deep learning, in particular, are ideally suitable for representing quantum-mechanical interactions, enabling us to model nonlinear potential-energy surfaces or enhancing the exploration of chemical compound space. Here we present the deep learning architecture SchNet that is specifically designed to model atomistic systems by making use of continuous-filter convolutional layers. We demonstrate the capabilities of SchNet by accurately predicting a range of properties across chemical space for molecules and materials, where our model learns chemically plausible embeddings of atom types across the periodic table. Finally, we employ SchNet to predict potential-energy surfaces and energy-conserving force fields for molecular dynamics simulations of small molecules and perform an exemplary study on the quantum-mechanical properties of C20-fullerene that would have been infeasible with regular ab initio molecular dynamics.},
archivePrefix = {arXiv},
arxivId = {1712.06113},
author = {Sch{\"{u}}tt, K. T. and Sauceda, H. E. and Kindermans, P. J. and Tkatchenko, A. and M{\"{u}}ller, K. R.},
doi = {10.1063/1.5019779},
eprint = {1712.06113},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sch{\"{u}}tt et al. - 2018 - SchNet - A deep learning architecture for molecules and materials.pdf:pdf},
issn = {00219606},
journal = {Journal of Chemical Physics},
number = {24},
pages = {1--10},
pmid = {29960322},
title = {{SchNet - A deep learning architecture for molecules and materials}},
volume = {148},
year = {2018}
}
@article{Chen2019b,
abstract = {This paper presents a method to pursue a semantic and quantitative explanation for the knowledge encoded in a convolutional neural network (CNN). The estimation of the specific rationale of each prediction made by the CNN presents a key issue of understanding neural networks, and it is of significant values in real applications. In this study, we propose to distill knowledge from the CNN into an explainable additive model, which explains the CNN prediction quantitatively. We discuss the problem of the biased interpretation of CNN predictions. To overcome the biased interpretation, we develop prior losses to guide the learning of the explainable additive model. Experimental results have demonstrated the effectiveness of our method.},
archivePrefix = {arXiv},
arxivId = {1812.07169},
author = {Chen, Runjin and Chen, Hao and Huang, Ge and Ren, Jie and Zhang, Quanshi},
doi = {10.1109/ICCV.2019.00928},
eprint = {1812.07169},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2019 - Explaining neural networks semantically and quantitatively.pdf:pdf},
isbn = {9781728148038},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {9186--9195},
title = {{Explaining neural networks semantically and quantitatively}},
volume = {2019-Octob},
year = {2019}
}
@article{Ancona2017a,
abstract = {Understanding the flow of information in Deep Neural Networks (DNNs) is a challenging problem that has gain increasing attention over the last few years. While several methods have been proposed to explain network predictions, there have been only a few attempts to compare them from a theoretical perspective. What is more, no exhaustive empirical comparison has been performed in the past. In this work, we analyze four gradient-based attribution methods and formally prove conditions of equivalence and approximation between them. By reformulating two of these methods, we construct a unified framework which enables a direct comparison, as well as an easier implementation. Finally, we propose a novel evaluation metric, called Sensitivity-n and test the gradient-based attribution methods alongside with a simple perturbation-based attribution method on several datasets in the domains of image and text classification, using various network architectures.},
archivePrefix = {arXiv},
arxivId = {1711.06104},
author = {Ancona, Marco and Ceolini, Enea and {\"{O}}ztireli, Cengiz and Gross, Markus},
eprint = {1711.06104},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ancona et al. - 2017 - Towards better understanding of gradient-based attribution methods for deep neural networks.pdf:pdf},
issn = {23318422},
journal = {arXiv},
pages = {1--16},
title = {{Towards better understanding of gradient-based attribution methods for deep neural networks}},
year = {2017}
}
@article{Selvaraju2016,
abstract = {We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing input regions that are 'important' for predictions -- or visual explanations. Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses class-specific gradient information to localize important regions. These localizations are combined with existing pixel-space visualizations to create a novel high-resolution and class-discriminative visualization called Guided Grad-CAM. These methods help better understand CNN-based models, including image captioning and visual question answering (VQA) models. We evaluate our visual explanations by measuring their ability to discriminate between classes, to inspire trust in humans, and their correlation with occlusion maps. Grad-CAM provides a new way to understand CNN-based models. We have released code, an online demo hosted on CloudCV, and a full version of this extended abstract.},
archivePrefix = {arXiv},
arxivId = {1611.07450},
author = {Selvaraju, Ramprasaath R and Das, Abhishek and Vedantam, Ramakrishna and Cogswell, Michael and Parikh, Devi and Batra, Dhruv},
eprint = {1611.07450},
pages = {1--4},
title = {{Grad-CAM: Why did you say that?}},
url = {http://arxiv.org/abs/1611.07450},
year = {2016}
}
@article{Simonyan2014,
abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [5], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [13].},
archivePrefix = {arXiv},
arxivId = {1312.6034},
author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
eprint = {1312.6034},
journal = {2nd International Conference on Learning Representations, ICLR 2014 - Workshop Track Proceedings},
pages = {1--8},
title = {{Deep inside convolutional networks: Visualising image classification models and saliency maps}},
year = {2014}
}
@article{Meyes2019,
abstract = {Ablation studies have been widely used in the field of neuroscience to tackle complex biological systems such as the extensively studied Drosophila central nervous system, the vertebrate brain and more interestingly and most delicately, the human brain. In the past, these kinds of studies were utilized to uncover structure and organization in the brain, i.e. a mapping of features inherent to external stimuli onto different areas of the neocortex. considering the growth in size and complexity of state-of-the-art artificial neural networks (ANNs) and the corresponding growth in complexity of the tasks that are tackled by these networks, the question arises whether ablation studies may be used to investigate these networks for a similar organization of their inner representations. In this paper, we address this question and performed two ablation studies in two fundamentally different ANNs to investigate their inner representations of two well-known benchmark datasets from the computer vision domain. We found that features distinct to the local and global structure of the data are selectively represented in specific parts of the network. Furthermore, some of these representations are redundant, awarding the network a certain robustness to structural damages. We further determined the importance of specific parts of the network for the classification task solely based on the weight structure of single units. Finally, we examined the ability of damaged networks to recover from the consequences of ablations by means of recovery training. We argue that ablations studies are a feasible method to investigate knowledge representations in ANNs and are especially helpful to examine a networks robustness to structural damages, a feature of ANNs that will become increasingly important for future safety-critical applications.},
archivePrefix = {arXiv},
arxivId = {1901.08644},
author = {Meyes, Richard and Lu, Melanie and de Puiseau, Constantin Waubert and Meisen, Tobias},
eprint = {1901.08644},
file = {:home/anna/Desktop/network explanation/class distribution/Ablation Studies in Artificial Neural Networks.pdf:pdf},
pages = {1--19},
title = {{Ablation Studies in Artificial Neural Networks}},
url = {http://arxiv.org/abs/1901.08644},
year = {2019}
}
@article{Agarwal2021,
abstract = {Perturbation-based explanation methods often measure the contribution of an input feature to an image classifier's outputs by heuristically removing it via e.g. blurring, adding noise, or graying out, which often produce unrealistic, out-of-samples. Instead, we propose to integrate a generative inpainter into three representative attribution methods to remove an input feature. Our proposed change improved all three methods in (1) generating more plausible counterfactual samples under the true data distribution; (2) being more accurate according to three metrics: object localization, deletion, and saliency metrics; and (3) being more robust to hyperparameter changes. Our findings were consistent across both ImageNet and Places365 datasets and two different pairs of classifiers and inpainters.},
archivePrefix = {arXiv},
arxivId = {1910.04256},
author = {Agarwal, Chirag and Nguyen, Anh},
doi = {10.1007/978-3-030-69544-6_7},
eprint = {1910.04256},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Agarwal, Nguyen - 2021 - Explaining Image Classifiers by Removing Input Features Using Generative Models.pdf:pdf},
isbn = {9783030695439},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {1},
pages = {101--118},
title = {{Explaining Image Classifiers by Removing Input Features Using Generative Models}},
volume = {12627 LNCS},
year = {2021}
}
@article{Frye2019,
abstract = {Explaining AI systems is fundamental both to the development of high performing models and to the trust placed in them by their users. The Shapley framework for explainability has strength in its general applicability combined with its precise, rigorous foundation: it provides a common, model-agnostic language for AI explainability and uniquely satisfies a set of intuitive mathematical axioms. However, Shapley values are too restrictive in one significant regard: they ignore all causal structure in the data. We introduce a less restrictive framework, Asymmetric Shapley values (ASVs), which are rigorously founded on a set of axioms, applicable to any AI system, and flexible enough to incorporate any causal structure known to be respected by the data. We demonstrate that ASVs can (i) improve model explanations by incorporating causal information, (ii) provide an unambiguous test for unfair discrimination in model predictions, (iii) enable sequentially incremental explanations in time-series models, and (iv) support feature-selection studies without the need for model retraining.},
archivePrefix = {arXiv},
arxivId = {1910.06358},
author = {Frye, Christopher and Rowat, Colin and Feige, Ilya},
eprint = {1910.06358},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Frye, Rowat, Feige - 2019 - Asymmetric Shapley values incorporating causal knowledge into model-agnostic explainability.pdf:pdf},
issn = {10495258},
number = {NeurIPS},
title = {{Asymmetric Shapley values: incorporating causal knowledge into model-agnostic explainability}},
url = {http://arxiv.org/abs/1910.06358},
year = {2019}
}
@article{Manuscript2020,
abstract = {Cladding austenitic stainless steels are trendy these days in pressure vessels to enhance surface qualities. In this work, austenitic stainless steel clad layers deposited by flux cored arc welding process on structural steel plates used in boiler construction are investigated. To facilitate this, composite clad layers are produced with 316L stainless steel deposits on low carbon structural steel plates based on the design of experiments. Results exhibit an incremental trend of thermal conductivity with respect to percentage of weld dilution. A linear mathematical relationship is established between the percentage of weld dilution and post-weld thermal conductivity of clad layers for the prediction of thermal conductivity of clad layer deposits. These results could be supportive in the fabrication industries for producing energy efficient thermal equipment},
author = {Manuscript, Accepted},
title = {{Ac ce us}},
year = {2020}
}
@article{Ming2018,
abstract = {Recurrent neural networks (RNNs) have been successfully applied to various natural language processing (NLP) tasks and achieved better results than conventional methods. However, the lack of understanding of the mechanisms behind their effectiveness limits further improvements on their architectures. In this paper, we present a visual analytics method for understanding and comparing RNN models for NLP tasks. We propose a technique to explain the function of individual hidden state units based on their expected response to input texts. We then co-cluster hidden state units and words based on the expected response and visualize co-clustering results as memory chips and word clouds to provide more structured knowledge on RNNs' hidden states. We also propose a glyph-based sequence visualization based on aggregate information to analyze the behavior of an RNN's hidden state at the sentence-level. The usability and effectiveness of our method are demonstrated through case studies and reviews from domain experts.},
archivePrefix = {arXiv},
arxivId = {1710.10777},
author = {Ming, Yao and Cao, Shaozu and Zhang, Ruixiang and Li, Zhen and Chen, Yuanzhe and Song, Yangqiu and Qu, Huamin},
doi = {10.1109/VAST.2017.8585721},
eprint = {1710.10777},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ming et al. - 2018 - Understanding Hidden Memories of Recurrent Neural Networks(2).pdf:pdf},
isbn = {9781538631638},
journal = {2017 IEEE Conference on Visual Analytics Science and Technology, VAST 2017 - Proceedings},
keywords = {Co-clustering,Recurrent neural networks,Understanding neural model,Visual analytics},
pages = {13--24},
publisher = {IEEE},
title = {{Understanding Hidden Memories of Recurrent Neural Networks}},
year = {2018}
}
@article{Shrikumar2017,
abstract = {The purported "black box" nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLlFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLlFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLlFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLlFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. Video tutorial: http://goo.gl/qKb7pL, code: http://goo.gl/RM8jvH.},
archivePrefix = {arXiv},
arxivId = {1704.02685},
author = {Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
eprint = {1704.02685},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shrikumar, Greenside, Kundaje - 2017 - Not just a black box Learning important features through propagating activation differences.pdf:pdf},
isbn = {9781510855144},
journal = {34th International Conference on Machine Learning, ICML 2017},
pages = {4844--4866},
title = {{Not just a black box: Learning important features through propagating activation differences}},
volume = {7},
year = {2017}
}
@inproceedings{Kanamori2021,
archivePrefix = {arXiv},
arxivId = {arXiv:2012.11782v1},
author = {Kanamori, Kentaro and Takagi, Takuya and Kobayashi, Ken},
booktitle = {35th AAAI Conference on Artificial Intelligence (AAAI 2021)},
eprint = {arXiv:2012.11782v1},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kanamori, Takagi, Kobayashi - 2021 - Ordered Counterfactual Explanation by Mixed-Integer Linear Optimization(2).pdf:pdf},
title = {{Ordered Counterfactual Explanation by Mixed-Integer Linear Optimization}},
year = {2021}
}
@article{Puri2017,
abstract = {Explaining the behavior of a black box machine learning model at the instance level is useful for building trust. However, it is also important to understand how the model behaves globally. Such an understanding provides insight into both the data on which the model was trained and the patterns that it learned. We present here an approach that learns if-then rules to globally explain the behavior of black box machine learning models that have been used to solve classification problems. The approach works by first extracting conditions that were important at the instance level and then evolving rules through a genetic algorithm with an appropriate fitness function. Collectively, these rules represent the patterns followed by the model for decisioning and are useful for understanding its behavior. We demonstrate the validity and usefulness of the approach by interpreting black box models created using publicly available data sets as well as a private digital marketing data set.},
archivePrefix = {arXiv},
arxivId = {1706.07160},
author = {Puri, Nikaash and Gupta, Piyush and Agarwal, Pratiksha and Verma, Sukriti and Krishnamurthy, Balaji},
eprint = {1706.07160},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Puri et al. - 2017 - MAGIX Model Agnostic Globally Interpretable Explanations.pdf:pdf},
title = {{MAGIX: Model Agnostic Globally Interpretable Explanations}},
url = {http://arxiv.org/abs/1706.07160},
year = {2017}
}
@article{Gu2021,
abstract = {Accurate medical image segmentation is essential for diagnosis and treatment planning of diseases. Convolutional Neural Networks (CNNs) have achieved state-of-The-Art performance for automatic medical image segmentation. However, they are still challenged by complicated conditions where the segmentation target has large variations of position, shape and scale, and existing CNNs have a poor explainability that limits their application to clinical decisions. In this work, we make extensive use of multiple attentions in a CNN architecture and propose a comprehensive attention-based CNN (CA-Net) for more accurate and explainable medical image segmentation that is aware of the most important spatial positions, channels and scales at the same time. In particular, we first propose a joint spatial attention module to make the network focus more on the foreground region. Then, a novel channel attention module is proposed to adaptively recalibrate channel-wise feature responses and highlight the most relevant feature channels. Also, we propose a scale attention module implicitly emphasizing the most salient feature maps among multiple scales so that the CNN is adaptive to the size of an object. Extensive experiments on skin lesion segmentation from ISIC 2018 and multi-class segmentation of fetal MRI found that our proposed CA-Net significantly improved the average segmentation Dice score from 87.77% to 92.08% for skin lesion, 84.79% to 87.08% for the placenta and 93.20% to 95.88% for the fetal brain respectively compared with U-Net. It reduced the model size to around 15 times smaller with close or even better accuracy compared with state-of-The-Art DeepLabv3+. In addition, it has a much higher explainability than existing networks by visualizing the attention weight maps. Our code is available at https://github.com/HiLab-git/CA-Net.},
archivePrefix = {arXiv},
arxivId = {2009.10549},
author = {Gu, Ran and Wang, Guotai and Song, Tao and Huang, Rui and Aertsen, Michael and Deprest, Jan and Ourselin, Sebastien and Vercauteren, Tom and Zhang, Shaoting},
doi = {10.1109/TMI.2020.3035253},
eprint = {2009.10549},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gu et al. - 2021 - CA-Net Comprehensive Attention Convolutional Neural Networks for Explainable Medical Image Segmentation.pdf:pdf},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Attention,convolutional neural network,explainability,medical image segmentation},
number = {2},
pages = {699--711},
pmid = {33136540},
title = {{CA-Net: Comprehensive Attention Convolutional Neural Networks for Explainable Medical Image Segmentation}},
volume = {40},
year = {2021}
}
@book{Mencar2019,
abstract = {Explainable Artificial Intelligence (XAI) is a relatively new approach to AI with special emphasis to the ability of machines to give sound motivations about their decisions and behavior. Since XAI is human-centered, it has tight connections with Granular Computing (GrC) in general, and Fuzzy Modeling (FM) in particular. However, although FM has been originally conceived to provide easily understandable models to users, this property cannot be taken for grant but it requires careful design choices. Furthermore, full integration of FM into XAI requires further processing, such as Natural Language Generation (NLG), which is a matter of current research.},
author = {Mencar, Corrado and Alonso, Jos{\'{e}} M.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-12544-8_17},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mencar, Alonso - 2019 - Paving the Way to Explainable Artificial Intelligence with Fuzzy Modeling Tutorial(2).pdf:pdf},
isbn = {9783030125431},
issn = {16113349},
pages = {215--227},
publisher = {Springer International Publishing},
title = {{Paving the Way to Explainable Artificial Intelligence with Fuzzy Modeling: Tutorial}},
url = {http://dx.doi.org/10.1007/978-3-030-12544-8_17},
volume = {11291 LNAI},
year = {2019}
}
@article{Selvaraju2016,
abstract = {We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing input regions that are 'important' for predictions -- or visual explanations. Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses class-specific gradient information to localize important regions. These localizations are combined with existing pixel-space visualizations to create a novel high-resolution and class-discriminative visualization called Guided Grad-CAM. These methods help better understand CNN-based models, including image captioning and visual question answering (VQA) models. We evaluate our visual explanations by measuring their ability to discriminate between classes, to inspire trust in humans, and their correlation with occlusion maps. Grad-CAM provides a new way to understand CNN-based models. We have released code, an online demo hosted on CloudCV, and a full version of this extended abstract.},
archivePrefix = {arXiv},
arxivId = {1611.07450},
author = {Selvaraju, Ramprasaath R and Das, Abhishek and Vedantam, Ramakrishna and Cogswell, Michael and Parikh, Devi and Batra, Dhruv},
eprint = {1611.07450},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Selvaraju et al. - 2016 - Grad-CAM Why did you say that.pdf:pdf},
pages = {1--4},
title = {{Grad-CAM: Why did you say that?}},
url = {http://arxiv.org/abs/1611.07450},
year = {2016}
}
@article{Rebuffi2020,
abstract = {Saliency methods seek to explain the predictions of a model by producing an importance map across each input sample. A popular class of such methods is based on backpropagating a signal and analyzing the resulting gradient. Despite much research on such methods, relatively little work has been done to clarify the differences between such methods as well as the desiderata of these techniques. Thus, there is a need for rigorously understanding the relationships between different methods as well as their failure modes. In this work, we conduct a thorough analysis of backpropagation-based saliency methods and propose a single framework under which several such methods can be unified. As a result of our study, we make three additional contributions. First, we use our framework to propose NormGrad, a novel saliency method based on the spatial contribution of gradients of convolutional weights. Second, we combine saliency maps at different layers to test the ability of saliency methods to extract complementary information at different network levels (e.g.$\sim$trading off spatial resolution and distinctiveness) and we explain why some methods fail at specific layers (e.g., Grad-CAM anywhere besides the last convolutional layer). Third, we introduce a class-sensitivity metric and a meta-learning inspired paradigm applicable to any saliency method for improving sensitivity to the output class being explained.},
archivePrefix = {arXiv},
arxivId = {2004.02866},
author = {Rebuffi, Sylvestre Alvise and Fong, Ruth and Ji, Xu and Vedaldi, Andrea},
doi = {10.1109/CVPR42600.2020.00886},
eprint = {2004.02866},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rebuffi et al. - 2020 - There and Back Again Revisiting Backpropagation Saliency Methods.pdf:pdf},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {8836--8845},
title = {{There and Back Again: Revisiting Backpropagation Saliency Methods}},
year = {2020}
}
@article{Laugel2020,
abstract = {Post-hoc interpretability approaches, although powerful tools to generate explanations for predictions made by a trained black-box model, have been shown to be vulnerable to issues caused by lack of robustness of the classifier. In particular, this paper focuses on the notion of explanation justification, defined as connectedness to ground-truth data, in the context of counterfactuals. In this work, we explore the extent of the risk of generating unjustified explanations. We propose an empirical study to assess the vulnerability of classifiers and show that the chosen learning algorithm heavily impacts the vulnerability of the model. Additionally, we show that state-of-the-art post-hoc counterfactual approaches can minimize the impact of this risk by generating less local explanations (Source code available at: https://github.com/thibaultlaugel/truce).},
author = {Laugel, Thibault and Lesot, Marie Jeanne and Marsala, Christophe and Renard, Xavier and Detyniecki, Marcin},
doi = {10.1007/978-3-030-46147-8_3},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Laugel et al. - 2020 - Unjustified Classification Regions and Counterfactual Explanations in Machine Learning(2).pdf:pdf},
isbn = {9783030461461},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Counterfactual explanations,Machine learning interpretability},
pages = {37--54},
title = {{Unjustified Classification Regions and Counterfactual Explanations in Machine Learning}},
volume = {11907 LNAI},
year = {2020}
}
@article{Kauffmann2020,
abstract = {Detecting anomalies in the data is a common machine learning task, with numerous applications in the sciences and industry. In practice, it is not always sufficient to reach high detection accuracy, one would also like to be able to understand why a given data point has been predicted to be anomalous. We propose a principled approach for one-class SVMs (OC-SVM), that draws on the novel insight that these models can be rewritten as distance/pooling neural networks. This ‘neuralization' step lets us apply deep Taylor decomposition (DTD), a methodology that leverages the model structure in order to quickly and reliably explain decisions in terms of input features. The proposed method (called ‘OC-DTD') is applicable to a number of common distance-based kernel functions, and it outperforms baselines such as sensitivity analysis, distance to nearest neighbor, or edge detection.},
archivePrefix = {arXiv},
arxivId = {1805.06230},
author = {Kauffmann, Jacob and M{\"{u}}ller, Klaus Robert and Montavon, Gr{\'{e}}goire},
doi = {10.1016/j.patcog.2020.107198},
eprint = {1805.06230},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kauffmann, M{\"{u}}ller, Montavon - 2020 - Towards explaining anomalies A deep Taylor decomposition of one-class models.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Deep Taylor decomposition,Explainable machine learning,Kernel machines,Outlier detection,Unsupervised learning},
title = {{Towards explaining anomalies: A deep Taylor decomposition of one-class models}},
volume = {101},
year = {2020}
}
@article{Arras2018,
abstract = {Recently, a technique called Layer-wise Relevance Propagation (LRP) was shown to deliver insightful explanations in the form of input space relevances for understanding feed-forward neural network classification decisions. In the present work, we extend the usage of LRP to recurrent neural networks. We propose a specific propagation rule applicable to multiplicative connections as they arise in recurrent network architectures such as LSTMs and GRUs. We apply our technique to a word-based bi-directional LSTM model on a five-class sentiment prediction task, and evaluate the resulting LRP relevances both qualitatively and quantitatively, obtaining better results than a gradient-based related method which was used in previous work.},
archivePrefix = {arXiv},
arxivId = {1706.07206},
author = {Arras, Leila and Montavon, Gr{\'{e}}goire and M{\"{u}}ller, Klaus-Robert and Samek, Wojciech},
doi = {10.18653/v1/w17-5221},
eprint = {1706.07206},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Arras et al. - 2018 - Explaining Recurrent Neural Network Predictions in Sentiment Analysis.pdf:pdf},
pages = {159--168},
title = {{Explaining Recurrent Neural Network Predictions in Sentiment Analysis}},
year = {2018}
}
@article{Shu2019,
abstract = {In recent years, to mitigate the problem of fake news, computational detection of fake news has been studied, producing some promising early results. While important, however, we argue that a critical missing piece of the study be the explainability of such detection, i.e., why a particular piece of news is detected as fake. In this paper, therefore, we study the explainable detection of fake news. We develop a sentence-comment co-attention sub-network to exploit both news contents and user comments to jointly capture explainable top-k check-worthy sentences and user comments for fake news detection. We conduct extensive experiments on real-world datasets and demonstrate that the proposed method not only significantly outperforms 7 state-of-the-art fake news detection methods by at least 5.33% in F1-score, but also (concurrently) identifies top-k user comments that explain why a news piece is fake, better than baselines by 28.2% in NDCG and 30.7% in Precision.},
author = {Shu, Kai and Cui, Limeng and Wang, Suhang and Lee, Dongwon and Liu, Huan},
doi = {10.1145/3292500.3330935},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shu et al. - 2019 - Defend Explainable fake news detection.pdf:pdf},
isbn = {9781450362016},
journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
keywords = {Explainable machine learning,Fake news,Social network},
pages = {395--405},
title = {{Defend: Explainable fake news detection}},
year = {2019}
}
@article{Selvaraju2016,
abstract = {We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing input regions that are 'important' for predictions -- or visual explanations. Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses class-specific gradient information to localize important regions. These localizations are combined with existing pixel-space visualizations to create a novel high-resolution and class-discriminative visualization called Guided Grad-CAM. These methods help better understand CNN-based models, including image captioning and visual question answering (VQA) models. We evaluate our visual explanations by measuring their ability to discriminate between classes, to inspire trust in humans, and their correlation with occlusion maps. Grad-CAM provides a new way to understand CNN-based models. We have released code, an online demo hosted on CloudCV, and a full version of this extended abstract.},
archivePrefix = {arXiv},
arxivId = {1611.07450},
author = {Selvaraju, Ramprasaath R and Das, Abhishek and Vedantam, Ramakrishna and Cogswell, Michael and Parikh, Devi and Batra, Dhruv},
eprint = {1611.07450},
pages = {1--4},
title = {{Grad-CAM: Why did you say that?}},
url = {http://arxiv.org/abs/1611.07450},
year = {2016}
}
@article{Heskes2020,
abstract = {Shapley values underlie one of the most popular model-agnostic methods within explainable artificial intelligence. These values are designed to attribute the difference between a model's prediction and an average baseline to the different features used as input to the model. Being based on solid game-theoretic principles, Shapley values uniquely satisfy several desirable properties, which is why they are increasingly used to explain the predictions of possibly complex and highly non-linear machine learning models. Shapley values are well calibrated to a user's intuition when features are independent, but may lead to undesirable, counterintuitive explanations when the independence assumption is violated. In this paper, we propose a novel framework for computing Shapley values that generalizes recent work that aims to circumvent the independence assumption. By employing Pearl's do-calculus, we show how these ‘causal' Shapley values can be derived for general causal graphs without sacrificing any of their desirable properties. Moreover, causal Shapley values enable us to separate the contribution of direct and indirect effects. We provide a practical implementation for computing causal Shapley values based on causal chain graphs when only partial information is available and illustrate their utility on a real-world example.},
archivePrefix = {arXiv},
arxivId = {2011.01625},
author = {Heskes, Tom and Sijben, Evi and Bucur, Ioan Gabriel and Claassen, Tom},
eprint = {2011.01625},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Heskes et al. - 2020 - Causal Shapley Values Exploiting Causal Knowledge to Explain Individual Predictions of Complex Models.pdf:pdf},
issn = {10495258},
number = {NeurIPS},
title = {{Causal Shapley Values: Exploiting Causal Knowledge to Explain Individual Predictions of Complex Models}},
url = {http://arxiv.org/abs/2011.01625},
year = {2020}
}
@article{Manuscript2020,
abstract = {Cladding austenitic stainless steels are trendy these days in pressure vessels to enhance surface qualities. In this work, austenitic stainless steel clad layers deposited by flux cored arc welding process on structural steel plates used in boiler construction are investigated. To facilitate this, composite clad layers are produced with 316L stainless steel deposits on low carbon structural steel plates based on the design of experiments. Results exhibit an incremental trend of thermal conductivity with respect to percentage of weld dilution. A linear mathematical relationship is established between the percentage of weld dilution and post-weld thermal conductivity of clad layers for the prediction of thermal conductivity of clad layer deposits. These results could be supportive in the fabrication industries for producing energy efficient thermal equipment},
author = {Manuscript, Accepted},
title = {{Ac ce us}},
year = {2020}
}
@article{Saxe2019,
abstract = {The practical successes of deep neural networks have not been matched by theoretical progress that satisfyingly explains their behavior. In this work, we study the information bottleneck (IB) theory of deep learning, which makes three specific claims: first, that deep networks undergo two distinct phases consisting of an initial fitting phase and a subsequent compression phase; second, that the compression phase is causally related to the excellent generalization performance of deep networks; and third, that the compression phase occurs due to the diffusion-like behavior of stochastic gradient descent. Here we show that none of these claims hold true in the general case, and instead reflect assumptions made to compute a finite mutual information metric in deterministic networks. When computed using simple binning, we demonstrate through a combination of analytical results and simulation that the information plane trajectory observed in prior work is predominantly a function of the neural nonlinearity employed: double-sided saturating nonlinearities like yield a compression phase as neural activations enter the saturation regime, but linear activation functions and single-sided saturating nonlinearities like the widely used ReLU in fact do not. Moreover, we find that there is no evident causal connection between compression and generalization: networks that do not compress are still capable of generalization, and vice versa. Next, we show that the compression phase, when it exists, does not arise from stochasticity in training by demonstrating that we can replicate the IB findings using full batch gradient descent rather than stochastic gradient descent. Finally, we show that when an input domain consists of a subset of task-relevant and task-irrelevant information, hidden representations do compress the task-irrelevant information, although the overall information about the input may monotonically increase with training time, and that this compression happens concurrently with the fitting process rather than during a subsequent compression period.},
author = {Saxe, Andrew M. and Bansal, Yamini and Dapello, Joel and Advani, Madhu and Kolchinsky, Artemy and Tracey, Brendan D. and Cox, David D.},
doi = {10.1088/1742-5468/ab3985},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Saxe et al. - 2019 - On the information bottleneck theory of deep learning.pdf:pdf},
issn = {17425468},
journal = {Journal of Statistical Mechanics: Theory and Experiment},
keywords = {machine learning},
number = {12},
publisher = {IOP Publishing},
title = {{On the information bottleneck theory of deep learning}},
volume = {2019},
year = {2019}
}
@article{Samek2017b,
abstract = {Deep neural networks (DNNs) have demonstrated impressive performance in complex machine learning tasks such as image classification or speech recognition. However, due to their multilayer nonlinear structure, they are not transparent, i.e., it is hard to grasp what makes them arrive at a particular classification or recognition decision, given a new unseen data sample. Recently, several approaches have been proposed enabling one to understand and interpret the reasoning embodied in a DNN for a single test image. These methods quantify the 'importance' of individual pixels with respect to the classification decision and allow a visualization in terms of a heatmap in pixel/input space. While the usefulness of heatmaps can be judged subjectively by a human, an objective quality measure is missing. In this paper, we present a general methodology based on region perturbation for evaluating ordered collections of pixels such as heatmaps. We compare heatmaps computed by three different methods on the SUN397, ILSVRC2012, and MIT Places data sets. Our main result is that the recently proposed layer-wise relevance propagation algorithm qualitatively and quantitatively provides a better explanation of what made a DNN arrive at a particular classification decision than the sensitivity-based approach or the deconvolution method. We provide theoretical arguments to explain this result and discuss its practical implications. Finally, we investigate the use of heatmaps for unsupervised assessment of the neural network performance.},
archivePrefix = {arXiv},
arxivId = {1509.06321},
author = {Samek, Wojciech and Binder, Alexander and Montavon, Gr{\'{e}}goire and Lapuschkin, Sebastian and M{\"{u}}ller, Klaus Robert},
doi = {10.1109/TNNLS.2016.2599820},
eprint = {1509.06321},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Samek et al. - 2017 - Evaluating the visualization of what a deep neural network has learned(2).pdf:pdf},
issn = {21622388},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {Convolutional neural networks,Explaining classification,Image classification,Interpretable machine learning,Relevance models},
number = {11},
pages = {2660--2673},
pmid = {27576267},
title = {{Evaluating the visualization of what a deep neural network has learned}},
volume = {28},
year = {2017}
}
@article{Peltola2018,
abstract = {We introduce a method, KL-LIME, for explaining predictions of Bayesian predictive models by projecting the information in the predictive distribution locally to a simpler, interpretable explanation model. The proposed approach combines the recent Local Interpretable Model-agnostic Explanations (LIME) method with ideas from Bayesian projection predictive variable selection methods. The information theoretic basis helps in navigating the trade-off between explanation fidelity and complexity. We demonstrate the method in explaining MNIST digit classifications made by a Bayesian deep convolutional neural network.},
archivePrefix = {arXiv},
arxivId = {1810.02678},
author = {Peltola, Tomi},
eprint = {1810.02678},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Peltola - 2018 - Local Interpretable Model-agnostic Explanations of Bayesian Predictive Models via Kullback–Leibler Projections.pdf:pdf},
issn = {23318422},
journal = {arXiv},
title = {{Local Interpretable Model-agnostic Explanations of Bayesian Predictive Models via Kullback–Leibler Projections}},
year = {2018}
}
@article{Goode2020,
abstract = {Optical spectral-temporal signatures extracted from videos of explosions provide information for identifying characteristics of the corresponding explosive devices. Currently, the identification is done using heuristic algorithms and direct subject matter expert review. An improvement in predictive performance may be obtained by using machine learning, but this application lends itself to high consequence national security decisions, so it is not only important to provide high accuracy but clear explanations for the predictions to garner confidence in the model. While much work has been done to develop explainability methods for machine learning models, not much of the work focuses on situations with input variables of the form of functional data such optical spectral-temporal signatures. We propose a procedure for explaining machine learning models fit using functional data that accounts for the functional nature the data. Our approach makes use of functional principal component analysis (fPCA) and permutation feature importance (PFI). fPCA is used to transform the functions to create uncorrelated functional principal components (fPCs). The model is trained using the fPCs as inputs, and PFI is applied to identify the fPCs important to the model for prediction. Visualizations are used to interpret the variability explained by the fPCs that are found to be important by PFI to determine the aspects of the functions that are important for prediction. We demonstrate the technique by explaining neural networks fit to explosion optical spectral-temporal signatures for predicting characteristics of the explosive devices.},
archivePrefix = {arXiv},
arxivId = {2010.12063},
author = {Goode, Katherine and Ries, Daniel and Zollweg, Joshua},
eprint = {2010.12063},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Goode, Ries, Zollweg - 2020 - Explaining Neural Network Predictions for Functional Data Using Principal Component Analysis and Featur(2).pdf:pdf},
title = {{Explaining Neural Network Predictions for Functional Data Using Principal Component Analysis and Feature Importance}},
url = {http://arxiv.org/abs/2010.12063},
year = {2020}
}
@article{Molnar2020,
abstract = {We present a brief history of the field of interpretable machine learning (IML), give an overview of state-of-the-art interpretation methods and discuss challenges. Research in IML has boomed in recent years. As young as the field is, it has over 200 years old roots in regression modeling and rule-based machine learning, starting in the 1960s. Recently, many new IML methods have been proposed, many of them model-agnostic, but also interpretation techniques specific to deep learning and tree-based ensembles. IML methods either directly analyze model components, study sensitivity to input perturbations, or analyze local or global surrogate approximations of the ML model. The field approaches a state of readiness and stability, with many methods not only proposed in research, but also implemented in open-source software. But many important challenges remain for IML, such as dealing with dependent features, causal interpretation, and uncertainty estimation, which need to be resolved for its successful application to scientific problems. A further challenge is a missing rigorous definition of interpretability, which is accepted by the community. To address the challenges and advance the field, we urge to recall our roots of interpretable, data-driven modeling in statistics and (rule-based) ML, but also to consider other areas such as sensitivity analysis, causal inference, and the social sciences.},
archivePrefix = {arXiv},
arxivId = {2010.09337},
author = {Molnar, Christoph and Casalicchio, Giuseppe and Bischl, Bernd},
doi = {10.1007/978-3-030-65965-3_28},
eprint = {2010.09337},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Molnar, Casalicchio, Bischl - 2020 - Interpretable Machine Learning – A Brief History, State-of-the-Art and Challenges.pdf:pdf},
isbn = {9783030659646},
issn = {18650937},
journal = {Communications in Computer and Information Science},
keywords = {Explainable artificial intelligence,Interpretable Machine Learning},
number = {01},
pages = {417--431},
title = {{Interpretable Machine Learning – A Brief History, State-of-the-Art and Challenges}},
volume = {1323},
year = {2020}
}
@article{Yang2019b,
abstract = {In this work, we propose a simple but effective method to interpret black-box machine learning models globally. That is, we use a compact binary tree, the interpretation tree, to explicitly represent the most important decision rules that are implicitly contained in the black-box machine learning models. This tree is learned from the contribution matrix which consists of the contributions of input variables to predicted scores for each single prediction. To generate the interpretation tree, a unified process recursively partitions the input variable space by maximizing the difference in the average contribution of the split variable between the divided spaces. We demonstrate the effectiveness of our method in diagnosing machine learning models on multiple tasks. Also, it is useful for new knowledge discovery as such insights are not easily identifiable when only looking at single predictions. In general, our work makes it easier and more efficient for human beings to understand machine learning models.},
archivePrefix = {arXiv},
arxivId = {1802.04253},
author = {Yang, Chengliang and Rangarajan, Anand and Ranka, Sanjay},
doi = {10.1109/HPCC/SmartCity/DSS.2018.00256},
eprint = {1802.04253},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang, Rangarajan, Ranka - 2019 - Global Model Interpretation Via Recursive Partitioning.pdf:pdf},
isbn = {9781538666142},
journal = {Proceedings - 20th International Conference on High Performance Computing and Communications, 16th International Conference on Smart City and 4th International Conference on Data Science and Systems, HPCC/SmartCity/DSS 2018},
keywords = {Interpretable machine learning,Knowledge discovery,Model diagnosis},
pages = {1563--1570},
title = {{Global Model Interpretation Via Recursive Partitioning}},
year = {2019}
}
@article{Schwab2019,
abstract = {Feature importance estimates that inform users about the degree to which given inputs influence the output of a predictive model are crucial for understanding, validating, and interpreting machine-learning models. However, providing fast and accurate estimates of feature importance for high-dimensional data, and quantifying the uncertainty of such estimates remain open challenges. Here, we frame the task of providing explanations for the decisions of machine-learning models as a causal learning task, and train causal explanation (CXPlain) models that learn to estimate to what degree certain inputs cause outputs in another machine-learning model. CXPlain can, once trained, be used to explain the target model in little time, and enables the quantification of the uncertainty associated with its feature importance estimates via bootstrap ensembling. We present experiments that demonstrate that CXPlain is significantly more accurate and faster than existing model-agnostic methods for estimating feature importance. In addition, we confirm that the uncertainty estimates provided by CXPlain ensembles are strongly correlated with their ability to accurately estimate feature importance on held-out data.},
archivePrefix = {arXiv},
arxivId = {1910.12336},
author = {Schwab, Patrick and Karlen, Walter},
eprint = {1910.12336},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schwab, Karlen - 2019 - CXPlain Causal explanations for model interpretation under uncertainty.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {NeurIPS},
title = {{CXPlain: Causal explanations for model interpretation under uncertainty}},
volume = {32},
year = {2019}
}
@article{Yu2019,
abstract = {In this paper, we develop a neural attentive interpretable recommendation system, named NAIRS. A self-attention network, as a key component of the system, is designed to assign attention weights to interacted items of a user. This attention mechanism can distinguish the importance of the various interacted items in contributing to a user profile. Based on the user profiles obtained by the self-attention network, NAIRS offers personalized high-quality recommendation. Moreover, it develops visual cues to interpret recommendations. This demo application with the implementation of NAIRS enables users to interact with a recommendation system, and it persistently collects training data to improve the system. The demonstration and experimental results show the effectiveness of NAIRS.},
archivePrefix = {arXiv},
arxivId = {1902.07494},
author = {Yu, Shuai and Wang, Yongbo and Yang, Min and Li, Baocheng and Qu, Qiang and Shen, Jialie},
doi = {10.1145/3289600.3290609},
eprint = {1902.07494},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yu et al. - 2019 - NAIRS A neural attentive interpretable recommendation system.pdf:pdf},
isbn = {9781450359405},
journal = {WSDM 2019 - Proceedings of the 12th ACM International Conference on Web Search and Data Mining},
keywords = {Collaborative filtering,Interpretable recommendation,Item-based recommendation,Self-attention network},
pages = {786--789},
title = {{NAIRS: A neural attentive interpretable recommendation system}},
year = {2019}
}
@book{Nauta2020,
abstract = {Image recognition with prototypes is considered an interpretable alternative for black box deep learning models. Classification depends on the extent to which a test image "looks like" a prototype. However, perceptual similarity for humans can be different from the similarity learned by the classification model. Hence, only visualising prototypes can be insufficient for a user to understand what a prototype exactly represents, and why the model considers a prototype and an image to be similar. We address this ambiguity and argue that prototypes should be explained. We improve interpretability by automatically enhancing visual prototypes with textual quantitative information about visual characteristics deemed important by the classification model. Specifically, our method clarifies the meaning of a prototype by quantifying the influence of colour hue, shape, texture, contrast and saturation and can generate both global and local explanations. Because of the generality of our approach, it can improve the interpretability of any similarity-based method for prototypical image recognition. In our experiments, we apply our method to the existing Prototypical Part Network (ProtoPNet). Our analysis confirms that the global explanations are generalisable, and often correspond to the visually perceptible properties of a prototype. Our explanations are especially relevant for prototypes which might have been interpreted incorrectly otherwise. By explaining such 'misleading' prototypes, we improve the interpretability and simulatability of a prototype-based classification model. We also use our method to check whether visually similar prototypes have similar explanations, and are able to discover redundancy. Code is available at https://github.com/M-Nauta/Explaining_Prototypes .},
archivePrefix = {arXiv},
arxivId = {2011.02863},
author = {Nauta, Meike and Jutte, Annemarie and Provoost, Jesper and Seifert, Christin},
booktitle = {Proceedings of ACM Conference (Conference'17)},
eprint = {2011.02863},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nauta et al. - 2020 - This Looks Like That, Because ... Explaining Prototypes for Interpretable Image Recognition.pdf:pdf},
number = {1},
publisher = {Association for Computing Machinery},
title = {{This Looks Like That, Because ... Explaining Prototypes for Interpretable Image Recognition}},
url = {http://arxiv.org/abs/2011.02863},
volume = {1},
year = {2020}
}
@article{Zhang2017a,
abstract = {This paper learns a graphical model, namely an explanatory graph, which reveals the knowledge hierarchy hidden inside a pre-trained CNN. Considering that each filter1 in a conv-layer of a pre-trained CNN usually represents a mixture of object parts, we propose a simple yet efficient method to automatically disentangles different part patterns from each filter, and construct an explanatory graph. In the explanatory graph, each node represents a part pattern, and each edge encodes co-activation relationships and spatial relationships between patterns. More importantly, we learn the explanatory graph for a pre-trained CNN in an unsupervised manner, i.e. without a need of annotating object parts. Experiments show that each graph node consistently represents the same object part through different images. We transfer part patterns in the explanatory graph to the task of part localization, and our method significantly outperforms other approaches.},
author = {Zhang, Quanshi and Cao, Ruiming and Shi, Feng and Wu, Ying Nian and Zhu, Song Chun},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2017 - Interpreting CNN knowledge via an explanatory graph.pdf:pdf},
issn = {23318422},
journal = {arXiv},
keywords = {Machine Learning Methods Track},
pages = {4454--4463},
title = {{Interpreting CNN knowledge via an explanatory graph}},
year = {2017}
}
@article{Zilke2016,
abstract = {Neural network classifiers are known to be able to learn very accurate models. In the recent past, researchers have even been able to train neural networks with multiple hidden layers (deep neural networks) more effectively and efficiently. However, the major downside of neural networks is that it is not trivial to understand the way how they derive their classification decisions. To solve this problem, there has been research on extracting better understandable rules from neural networks. However, most authors focus on nets with only one single hidden layer. The present paper introduces a new decompositional algorithm – DeepRED – that is able to extract rules from deep neural networks. The evaluation of the proposed algorithm shows its ability to outperform a pedagogical baseline on several tasks, including the successful extraction of rules from a neural network realizing the XOR function.},
author = {Zilke, Jan Ruben and Menc{\'{i}}a, Eneldo Loza and Janssen, Frederik},
doi = {10.1007/978-3-319-46307-0_29},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zilke, Menc{\'{i}}a, Janssen - 2016 - DeepRED – Rule extraction from deep neural networks.pdf:pdf},
isbn = {9783319463063},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {457--473},
title = {{DeepRED – Rule extraction from deep neural networks}},
volume = {9956 LNAI},
year = {2016}
}
@article{Seeliger2019,
abstract = {Due to their tremendous potential in predictive tasks, Machine Learning techniques such as Artificial Neural Networks have received great attention from both research and practice. However, often these models do not provide explainable outcomes which is a crucial requirement in many high stakes domains such as health care or transport. Regarding explainability, Semantic Web Technologies offer semantically interpretable tools which allow reasoning on knowledge bases. Hence, the question arises how Semantic Web Technologies and related concepts can facilitate explanations in Machine Learning systems. To address this topic, we present current approaches of combining Machine Learning with Semantic Web Technologies in the context of model explainability based on a systematic literature review. In doing so, we also highlight domains and applications driving the research field and discuss the ways in which explanations are given to the user. Drawing upon these insights, we suggest directions for further research on combining Semantic Web Technologies with Machine Learning.},
author = {Seeliger, Arne and Pfaff, Matthias and Krcmar, Helmut},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Seeliger, Pfaff, Krcmar - 2019 - Semantic web technologies for explainable machine learning models A literature review(2).pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
keywords = {Explainability,Machine learning,Semantic web technologies,XAI},
pages = {30--45},
title = {{Semantic web technologies for explainable machine learning models: A literature review}},
volume = {2465},
year = {2019}
}
@article{Selvaraju2016,
abstract = {We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing input regions that are 'important' for predictions -- or visual explanations. Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses class-specific gradient information to localize important regions. These localizations are combined with existing pixel-space visualizations to create a novel high-resolution and class-discriminative visualization called Guided Grad-CAM. These methods help better understand CNN-based models, including image captioning and visual question answering (VQA) models. We evaluate our visual explanations by measuring their ability to discriminate between classes, to inspire trust in humans, and their correlation with occlusion maps. Grad-CAM provides a new way to understand CNN-based models. We have released code, an online demo hosted on CloudCV, and a full version of this extended abstract.},
archivePrefix = {arXiv},
arxivId = {1611.07450},
author = {Selvaraju, Ramprasaath R and Das, Abhishek and Vedantam, Ramakrishna and Cogswell, Michael and Parikh, Devi and Batra, Dhruv},
eprint = {1611.07450},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Selvaraju et al. - 2016 - Grad-CAM Why did you say that.pdf:pdf},
pages = {1--4},
title = {{Grad-CAM: Why did you say that?}},
url = {http://arxiv.org/abs/1611.07450},
year = {2016}
}
@article{Shukla2020,
abstract = {This paper aims to look at the value and the necessity of XAI (Explainable Artificial Intelligence) when using DNNs (Deep Neural Networks) in PM (Predictive Maintenance). The context will be the field of Aerospace IVHM (Integrated Vehicle Health Management) when using DNNs. An XAI (Explainable Artificial Intelligence) system is necessary so that the result of an AI (Artificial Intelligence) solution is clearly explained and understood by a human expert. This would allow the IVHM system to use XAI based PM to improve effectiveness of predictive model. An IVHM system would be able to utilize the information to assess the health of the subsystems, and their effect on the aircraft. Even if the underlying mathematical principles are understood, they lack an understandable insight, hence have difficulty in generating the underlying explanatory structures (i.e. black box). This calls for a process, or system, that enables decisions to be explainable, transparent, and understandable. It is argued that research in XAI would generally help to accelerate the implementation of AI/ML (Machine Learning) in the aerospace domain, and specifically help to facilitate compliance, transparency, and trust.},
author = {Shukla, Bibhudhendu and Fan, Ip-Shing and Jennions, Ian},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shukla, Fan, Jennions - 2020 - Opportunities for Explainable Artificial Intelligence in Aerospace Predictive Maintenance(2).pdf:pdf},
issn = {2325-016X},
journal = {PHM Society European Conference},
keywords = {AI,DNNs,EXP,IVHM,Predictive Maintenance,XAI},
number = {1},
pages = {11--11},
title = {{Opportunities for Explainable Artificial Intelligence in Aerospace Predictive Maintenance}},
url = {https://phmpapers.org/index.php/phme/article/view/1231},
volume = {5},
year = {2020}
}
@article{Leino2019,
abstract = {We study the problem of explaining a rich class of behavioral properties of deep neural networks. Distinctively, our influence-directed explanations approach this problem by peering inside the network to identify neurons with high influence on a quantity and distribution of interest, using an axiomatically-justified influence measure, and then providing an interpretation for the concepts these neurons represent. We evaluate our approach by demonstrating a number of its unique capabilities on convolutional neural networks trained on ImageNet. Our evaluation demonstrates that influence-directed explanations (1) identify influential concepts that generalize across instances, (2) can be used to extract the 'essence' of what the network learned about a class, and (3) isolate individual features the network uses to make decisions and distinguish related classes.},
archivePrefix = {arXiv},
arxivId = {1802.03788},
author = {Leino, Klas and Sen, Shayak and Datta, Anupam and Fredrikson, Matt and Li, Linyi},
doi = {10.1109/TEST.2018.8624792},
eprint = {1802.03788},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Leino et al. - 2019 - Influence-Directed Explanations for Deep Convolutional Networks(2).pdf:pdf},
isbn = {9781538683828},
issn = {10893539},
journal = {Proceedings - International Test Conference},
number = {1},
pages = {1--8},
title = {{Influence-Directed Explanations for Deep Convolutional Networks}},
volume = {2018-Octob},
year = {2019}
}
@article{Tishby2015,
abstract = {Deep Neural Networks (DNNs) are analyzed via the theoretical framework of the information bottleneck (IB) principle. We first show that any DNN can be quantified by the mutual information between the layers and the input and output variables. Using this representation we can calculate the optimal information theoretic limits of the DNN and obtain finite sample generalization bounds. The advantage of getting closer to the theoretical limit is quantifiable both by the generalization bound and by the network's simplicity. We argue that both the optimal architecture, number of layers and features/connections at each layer, are related to the bifurcation points of the information bottleneck tradeoff, namely, relevant compression of the input layer with respect to the output layer. The hierarchical representations at the layered network naturally correspond to the structural phase transitions along the information curve. We believe that this new insight can lead to new optimality bounds and deep learning algorithms.},
archivePrefix = {arXiv},
arxivId = {1503.02406},
author = {Tishby, Naftali and Zaslavsky, Noga},
doi = {10.1109/ITW.2015.7133169},
eprint = {1503.02406},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tishby, Zaslavsky - 2015 - Deep learning and the information bottleneck principle.pdf:pdf},
isbn = {9781479955268},
journal = {2015 IEEE Information Theory Workshop, ITW 2015},
title = {{Deep learning and the information bottleneck principle}},
year = {2015}
}
@inproceedings{Zhang2017b,
author = {Zhang, Zizhao and Xie, Yuanpu and Xing, Fuyong and Mcgough, Mason and Yang, Lin},
booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2017 - MDNet A Semantically and Visually Interpretable Medical Image Diagnosis Network.pdf:pdf},
title = {{MDNet: A Semantically and Visually Interpretable Medical Image Diagnosis Network}},
year = {2017}
}
@article{Erhan2009,
abstract = {Deep architectures have demonstrated state-of-the-art results in a variety of settings, especially with vision datasets. Beyond the model definitions and the quantitative analyses, there is a need for qualitative comparisons of the solutions learned by various deep architectures. The goal of this paper is to find good qualitative interpretations of high level features represented by such models. To this end, we contrast and compare several techniques applied on Stacked Denoising Autoencoders and Deep Belief Networks, trained on several vision datasets. We show that, perhaps counter-intuitively, such interpretation is possible at the unit level, that it is simple to accomplish and that the results are consistent across various techniques. We hope that such techniques will allow researchers in deep architectures to understand more of how and why deep architectures work},
author = {Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Erhan et al. - 2009 - Visualizing higher-layer features of a deep network.pdf:pdf},
journal = {Bernoulli},
number = {1341},
pages = {1--13},
title = {{Visualizing higher-layer features of a deep network}},
url = {http://igva2012.wikispaces.asu.edu/file/view/Erhan+2009+Visualizing+higher+layer+features+of+a+deep+network.pdf},
year = {2009}
}
@article{Altmann2010,
abstract = {Motivation: In life sciences, interpretability of machine learning models is as important as their prediction accuracy. Linear models are probably the most frequently used methods for assessing feature relevance, despite their relative inflexibility. However, in the past years effective estimators of feature relevance have been derived for highly complex or non-parametric models such as support vector machines and RandomForest (RF) models. Recently, it has been observed that RF models are biased in such a way that categorical variables with a large number of categories are preferred. Results: In this work, we introduce a heuristic for normalizing feature importance measures that can correct the feature importance bias. The method is based on repeated permutations of the outcome vector for estimating the distribution of measured importance for each variable in a non-informative setting. The P-value of the observed importance provides a corrected measure of feature importance. We apply our method to simulated data and demonstrate that (i) non-informative predictors do not receive significant P-values, (ii) informative variables can successfully be recovered among non-informative variables and (iii) P-values computed with permutation importance (PIMP) are very helpful for deciding the significance of variables, and therefore improve model interpretability. Furthermore, PIMP was used to correct RF-based importance measures for two real-world case studies. We propose an improved RF model that uses the significant variables with respect to the PIMP measure and show that its prediction accuracy is superior to that of other existing models. Availability: R code for the method presented in this article is available at http://www.mpi-inf.mpg.de/∼altmann/download/PIMP.R. Contact: altmann@mpi-inf.mpg.de, laura.tolosi@mpi-inf.mpg.de. Supplementary information: Supplementary data are available at Bioinformatics online. {\textcopyright} The Author 2010. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oxfordjournals.org.},
author = {Altmann, Andr{\'{e}} and Toloşi, Laura and Sander, Oliver and Lengauer, Thomas},
doi = {10.1093/bioinformatics/btq134},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Altmann et al. - 2010 - Permutation importance A corrected feature importance measure.pdf:pdf},
issn = {13674803},
journal = {Bioinformatics},
number = {10},
pages = {1340--1347},
pmid = {20385727},
title = {{Permutation importance: A corrected feature importance measure}},
volume = {26},
year = {2010}
}
@article{Kim2018a,
abstract = {Neural networks commonly offer high utility but remain difficult to interpret. De- veloping methods to explain their decisions is challenging due to their large size, complex structure, and inscrutable internal representations. This work argues that the language of explanations should be expanded from that of input features (e.g., assigning importance weightings to pixels) to include that of higher-level, human- friendly concepts. For example, an understandable explanation of why an im- age classifier outputs the label “zebra” would ideally relate to concepts such as “stripes” rather than a set of particular pixel values. This paper introduces the “concept activation vector” (CAV) which allows quantitative analysis of a con- cept's relative importance to classification, with a user-provided set of input data examples defining the concept. CAVs may be easily used by non-experts, who need only provide examples, and with CAVs the high-dimensional structure of neural networks turns into an aid to interpretation, rather than an obstacle. Using the domain of image classification as a testing ground, we describe how CAVs may be used to test hypotheses about classifiers and also generate insights into the deficiencies and correlations in training data. CAVs also provide us a directed approach to choose the combinations of neurons to visualize with the DeepDream technique, which traditionally has chosen neurons or linear combinations of neu- rons at random to visualize.},
archivePrefix = {arXiv},
arxivId = {1711.11279},
author = {Kim, Been and Gilmer, Justin and Viegas, Fernanda and Erlingsson, Ulfar and Wattenberg, Martin},
eprint = {1711.11279},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim et al. - 2017 - TCAV RELATIVE CONCEPT IMPORTANCE TESTING WITH LINEAR CONCEPT ACTIVATION VECTORS(2).pdf:pdf},
isbn = {9781510867963},
title = {{TCAV RELATIVE CONCEPT IMPORTANCE TESTING WITH LINEAR CONCEPT ACTIVATION VECTORS}},
year = {2017}
}
@article{Huang2020,
abstract = {Graph structured data has wide applicability in various domains such as physics, chemistry, biology, computer vision, and social networks, to name a few. Recently, graph neural networks (GNN) were shown to be successful in effectively representing graph structured data because of their good performance and generalization ability. GNN is a deep learning based method that learns a node representation by combining specific nodes and the structural/topological information of a graph. However, like other deep models, explaining the effectiveness of GNN models is a challenging task because of the complex nonlinear transformations made over the iterations. In this paper, we propose GraphLIME, a local interpretable model explanation for graphs using the Hilbert-Schmidt Independence Criterion (HSIC) Lasso, which is a nonlinear feature selection method. GraphLIME is a generic GNN-model explanation framework that learns a nonlinear interpretable model locally in the subgraph of the node being explained. More specifically, to explain a node, we generate a nonlinear interpretable model from its N-hop neighborhood and then compute theK most representative features as the explanations of its prediction using HSIC Lasso. Through experiments on two real-world datasets, the explanations of GraphLIME are found to be of extraordinary degree and more descriptive in comparison to the existing explanation methods.},
archivePrefix = {arXiv},
arxivId = {2001.06216},
author = {Huang, Qiang and Yamada, Makoto and Tian, Yuan and Singh, Dinesh and Yin, Dawei and Chang, Yi},
eprint = {2001.06216},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang et al. - 2020 - Graphlime Local interpretable model explanations for graph neural networks.pdf:pdf},
issn = {23318422},
journal = {arXiv},
pages = {1--11},
title = {{Graphlime: Local interpretable model explanations for graph neural networks}},
year = {2020}
}
@article{Dhurandhar2018,
abstract = {In this paper we propose a novel method that provides contrastive explanations justifying the classification of an input by a black box classifier such as a deep neural network. Given an input we find what should be minimally and sufficiently present (viz. important object pixels in an image) to justify its classification and analogously what should be minimally and necessarily absent (viz. certain background pixels). We argue that such explanations are natural for humans and are used commonly in domains such as health care and criminology. What is minimally but critically absent is an important part of an explanation, which to the best of our knowledge, has not been explicitly identified by current explanation methods that explain predictions of neural networks. We validate our approach on three real datasets obtained from diverse domains; namely, a handwritten digits dataset MNIST, a large procurement fraud dataset and a brain activity strength dataset. In all three cases, we witness the power of our approach in generating precise explanations that are also easy for human experts to understand and evaluate.},
archivePrefix = {arXiv},
arxivId = {1802.07623},
author = {Dhurandhar, Amit and Chen, Pin Yu and Luss, Ronny and Tu, Chun Chen and Ting, Paishun and Shanmugam, Karthikeyan and Das, Payel},
eprint = {1802.07623},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dhurandhar et al. - 2018 - Explanations based on the Missing Towards contrastive explanations with pertinent negatives(2).pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {592--603},
title = {{Explanations based on the Missing: Towards contrastive explanations with pertinent negatives}},
volume = {2018-Decem},
year = {2018}
}
@article{Du2018,
abstract = {Interpretable machine learning tackles the important problem that humans cannot understand the behaviors of complex machine learning models and how these models arrive at a particular decision. Although many approaches have been proposed, a comprehensive understanding of the achievements and challenges is still lacking. We provide a survey covering existing techniques to increase the interpretability of machine learning models. We also discuss crucial issues that the community should consider in future work such as designing user-friendly explanations and developing comprehensive evaluation metrics to further push forward the area of interpretable machine learning.},
archivePrefix = {arXiv},
arxivId = {1808.00033},
author = {Du, Mengnan and Liu, Ninghao and Hu, Xia},
eprint = {1808.00033},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Du, Liu, Hu - 2018 - Techniques for interpretable machine learning.pdf:pdf},
issn = {23318422},
journal = {arXiv},
title = {{Techniques for interpretable machine learning}},
year = {2018}
}
@inproceedings{Wei2017,
author = {{Wei Koh}, Pang and Liang, Percy},
booktitle = {Proceedings of the 34th International Conference on Machine Learning},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wei Koh, Liang - 2017 - Understanding Black-box Predictions via Influence Functions.pdf:pdf},
issn = {00258512},
pages = {1885--1894},
pmid = {4774858},
title = {{Understanding Black-box Predictions via Influence Functions}},
volume = {70},
year = {2017}
}
@article{Dabkowski2017,
abstract = {In this work we develop a fast saliency detection method that can be applied to any differentiable image classifier. We train a masking model to manipulate the scores of the classifier by masking salient parts of the input image. Our model generalises well to unseen images and requires a single forward pass to perform saliency detection, therefore suitable for use in real-time systems. We test our approach on CIFAR-10 and ImageNet datasets and show that the produced saliency maps are easily interpretable, sharp, and free of artifacts. We suggest a new metric for saliency and test our method on the ImageNet object localisation task. We achieve results outperforming other weakly supervised methods.},
archivePrefix = {arXiv},
arxivId = {1705.07857},
author = {Dabkowski, Piotr and Gal, Yarin},
eprint = {1705.07857},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dabkowski, Gal - 2017 - Real time image saliency for black box classifiers.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {Nips},
pages = {6968--6977},
title = {{Real time image saliency for black box classifiers}},
volume = {2017-Decem},
year = {2017}
}
@article{Bramhall2020,
author = {Bramhall, Steven and Horn, Hayley and Tieu, Michael and Lohia, Nibhrat},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bramhall et al. - 2020 - QLIME—A quadratic local interpretable model-agnostic explanation approach.pdf:pdf},
journal = {SMU Data Science Review},
number = {1},
pages = {73--88},
title = {{QLIME—A quadratic local interpretable model-agnostic explanation approach}},
volume = {3},
year = {2020}
}
@article{Liu2020,
abstract = {Building explainable systems is a critical problem in the field of Natural Language Processing (NLP), since most machine learning models provide no explanations for the predictions. Existing approaches for explainable machine learning systems tend to focus on interpreting the outputs or the connections between inputs and outputs. However, the fine-grained information (e.g. textual explanations for the labels) is often ignored, and the systems do not explicitly generate the human-readable explanations. To solve this problem, we propose a novel generative explanation framework that learns to make classification decisions and generate fine-grained explanations at the same time. More specifically, we introduce the explainable factor and the minimum risk training approach that learn to generate more reasonable explanations. We construct two new datasets that contain summaries, rating scores, and fine-grained reasons. We conduct experiments on both datasets, comparing with several strong neural network baseline systems. Experimental results show that our method surpasses all baselines on both datasets, and is able to generate concise explanations at the same time.},
archivePrefix = {arXiv},
arxivId = {1811.00196},
author = {Liu, Hui and Yin, Qingyu and Wang, William Yang},
doi = {10.18653/v1/p19-1560},
eprint = {1811.00196},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu, Yin, Wang - 2020 - Towards explainable NLP A generative explanation framework for text classification(2).pdf:pdf},
isbn = {9781950737482},
journal = {ACL 2019 - 57th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference},
pages = {5570--5581},
title = {{Towards explainable NLP: A generative explanation framework for text classification}},
year = {2020}
}
@article{Wang2020,
abstract = {Recently, increasing attention has been drawn to the internal mechanisms of convolutional neural networks, and the reason why the network makes specific decisions. In this paper, we develop a novel post-hoc visual explanation method called Score-CAM based on class activation mapping. Unlike previous class activation mapping based approaches, Score-CAM gets rid of the dependence on gradients by obtaining the weight of each activation map through its forward passing score on target class, the final result is obtained by a linear combination of weights and activation maps. We demonstrate that Score-CAM achieves better visual performance and fairness for interpreting the decision making process. Our approach outperforms previous methods on both recognition and localization tasks, it also passes the sanity check. We also indicate its application as debugging tools. The implementation is available1.},
archivePrefix = {arXiv},
arxivId = {1910.01279},
author = {Wang, Haofan and Wang, Zifan and Du, Mengnan and Yang, Fan and Zhang, Zijian and Ding, Sirui and Mardziel, Piotr and Hu, Xia},
doi = {10.1109/CVPRW50498.2020.00020},
eprint = {1910.01279},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2020 - Score-CAM Score-weighted visual explanations for convolutional neural networks.pdf:pdf},
isbn = {9781728193601},
issn = {21607516},
journal = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
pages = {111--119},
title = {{Score-CAM: Score-weighted visual explanations for convolutional neural networks}},
volume = {2020-June},
year = {2020}
}
@article{GethsiyalAugasta2012,
abstract = {Artificial neural networks often achieve high classification accuracy rates, but they are considered as black boxes due to their lack of explanation capability. This paper proposes the new rule extraction algorithm RxREN to overcome this drawback. In pedagogical approach the proposed algorithm extracts the rules from trained neural networks for datasets with mixed mode attributes. The algorithm relies on reverse engineering technique to prune the insignificant input neurons and to discover the technological principles of each significant input neuron of neural network in classification. The novelty of this algorithm lies in the simplicity of the extracted rules and conditions in rule are involving both discrete and continuous mode of attributes. Experimentation using six different real datasets namely iris, wbc, hepatitis, pid, ionosphere and creditg show that the proposed algorithm is quite efficient in extracting smallest set of rules with high classification accuracy than those generated by other neural network rule extraction methods. {\textcopyright} Springer Science+Business Media, LLC. 2011.},
author = {{Gethsiyal Augasta}, M. and Kathirvalavakumar, T.},
doi = {10.1007/s11063-011-9207-8},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gethsiyal Augasta, Kathirvalavakumar - 2012 - Reverse engineering the neural networks for rule extraction in classification problems.pdf:pdf},
issn = {13704621},
journal = {Neural Processing Letters},
keywords = {Classification,Neural networks,Pedagogical,Pruning,Reverse engineering,Rule extraction},
number = {2},
pages = {131--150},
title = {{Reverse engineering the neural networks for rule extraction in classification problems}},
volume = {35},
year = {2012}
}
@article{Hase2020,
abstract = {Algorithmic approaches to interpreting machine learning models have proliferated in recent years. We carry out human subject tests that are the first of their kind to isolate the effect of algorithmic explanations on a key aspect of model interpretability, simulatability, while avoiding important confounding experimental factors. A model is simulatable when a person can predict its behavior on new inputs. Through two kinds of simulation tests involving text and tabular data, we evaluate five explanations methods: (1) LIME, (2) Anchor, (3) Decision Boundary, (4) a Prototype model, and (5) a Composite approach that combines explanations from each method. Clear evidence of method effectiveness is found in very few cases: LIME improves simulatability in tabular classification, and our Prototype method is effective in counterfactual simulation tests. We also collect subjective ratings of explanations, but we do not find that ratings are predictive of how helpful explanations are. Our results provide the first reliable and comprehensive estimates of how explanations influence simulatability across a variety of explanation methods and data domains. We show that (1) we need to be careful about the metrics we use to evaluate explanation methods, and (2) there is significant room for improvement in current methods.1},
archivePrefix = {arXiv},
arxivId = {2005.01831},
author = {Hase, Peter and Bansal, Mohit},
doi = {10.18653/v1/2020.acl-main.491},
eprint = {2005.01831},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hase, Bansal - 2020 - Evaluating Explainable AI Which Algorithmic Explanations Help Users Predict Model Behavior.pdf:pdf},
issn = {23318422},
journal = {arXiv},
number = {3},
title = {{Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?}},
year = {2020}
}
@article{Zhou2019,
abstract = {The success of recent deep convolutional neural networks (CNNs) depends on learning hidden representations that can summarize the important factors of variation behind the data. In this work, we describe Network Dissection, a method that interprets networks by providing meaningful labels to their individual units. The proposed method quantifies the interpretability of CNN representations by evaluating the alignment between individual hidden units and visual semantic concepts. By identifying the best alignments, units are given interpretable labels ranging from colors, materials, textures, parts, objects and scenes. The method reveals that deep representations are more transparent and interpretable than they would be under a random equivalently powerful basis. We apply our approach to interpret and compare the latent representations of several network architectures trained to solve a wide range of supervised and self-supervised tasks. We then examine factors affecting the network interpretability such as the number of the training iterations, regularizations, different initialization parameters, as well as networks depth and width. Finally we show that the interpreted units can be used to provide explicit explanations of a given CNN prediction for an image. Our results highlight that interpretability is an important property of deep neural networks that provides new insights into what hierarchical structures can learn.},
archivePrefix = {arXiv},
arxivId = {1711.05611},
author = {Zhou, Bolei and Bau, David and Oliva, Aude and Torralba, Antonio},
doi = {10.1109/TPAMI.2018.2858759},
eprint = {1711.05611},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhou et al. - 2019 - Interpreting Deep Visual Representations via Network Dissection.pdf:pdf},
issn = {19393539},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Convolutional neural networks,interpretable machine learning,network interpretability,visual recognition},
number = {9},
pages = {2131--2145},
pmid = {30040625},
title = {{Interpreting Deep Visual Representations via Network Dissection}},
volume = {41},
year = {2019}
}
@article{IsmailFawaz2019,
abstract = {Purpose: Manual feedback from senior surgeons observing less experienced trainees is a laborious task that is very expensive, time-consuming and prone to subjectivity. With the number of surgical procedures increasing annually, there is an unprecedented need to provide an accurate, objective and automatic evaluation of trainees' surgical skills in order to improve surgical practice. Methods: In this paper, we designed a convolutional neural network (CNN) to classify surgical skills by extracting latent patterns in the trainees' motions performed during robotic surgery. The method is validated on the JIGSAWS dataset for two surgical skills evaluation tasks: classification and regression. Results: Our results show that deep neural networks constitute robust machine learning models that are able to reach new competitive state-of-the-art performance on the JIGSAWS dataset. While we leveraged from CNNs' efficiency, we were able to minimize its black-box effect using the class activation map technique. Conclusions: This characteristic allowed our method to automatically pinpoint which parts of the surgery influenced the skill evaluation the most, thus allowing us to explain a surgical skill classification and provide surgeons with a novel personalized feedback technique. We believe this type of interpretable machine learning model could integrate within “Operation Room 2.0” and support novice surgeons in improving their skills to eventually become experts.},
archivePrefix = {arXiv},
arxivId = {1908.07319},
author = {{Ismail Fawaz}, Hassan and Forestier, Germain and Weber, Jonathan and Idoumghar, Lhassane and Muller, Pierre Alain},
doi = {10.1007/s11548-019-02039-4},
eprint = {1908.07319},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ismail Fawaz et al. - 2019 - Accurate and interpretable evaluation of surgical skills from kinematic data using fully convolutional neur.pdf:pdf},
issn = {18616429},
journal = {International Journal of Computer Assisted Radiology and Surgery},
keywords = {Deep learning,Interpretable machine learning,Kinematic data,Surgical education,Time-series classification},
number = {9},
pages = {1611--1617},
pmid = {31363983},
publisher = {Springer International Publishing},
title = {{Accurate and interpretable evaluation of surgical skills from kinematic data using fully convolutional neural networks}},
url = {https://doi.org/10.1007/s11548-019-02039-4},
volume = {14},
year = {2019}
}
@article{Delaney2020,
abstract = {In recent years there has been a cascade of research in attempting to make AI systems more interpretable by providing explanations; so-called Explainable AI (XAI). Most of this research has dealt with the challenges that arise in explaining black-box deep learning systems in classification and regression tasks, with a focus on tabular and image data; for example, there is a rich seam of work on post-hoc counterfactual explanations for a variety of black-box classifiers (e.g., when a user is refused a loan, the counterfactual explanation tells the user about the conditions under which they would get the loan). However, less attention has been paid to the parallel interpretability challenges arising in AI systems dealing with time series data. This paper advances a novel technique, called Native-Guide, for the generation of proximal and plausible counterfactual explanations for instance-based time series classification tasks (e.g., where users are provided with alternative time series to explain how a classification might change). The Native-Guide method retrieves and uses native in-sample counterfactuals that already exist in the training data as "guides" for perturbation in time series counterfactual generation. This method can be coupled with both Euclidean and Dynamic Time Warping (DTW) distance measures. After illustrating the technique on a case study involving a climate classification task, we reported on a comprehensive series of experiments on both real-world and synthetic data sets from the UCR archive. These experiments provide computational evidence of the quality of the counterfactual explanations generated.},
archivePrefix = {arXiv},
arxivId = {2009.13211},
author = {Delaney, Eoin and Greene, Derek and Keane, Mark T.},
eprint = {2009.13211},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Delaney, Greene, Keane - 2020 - Instance-Based Counterfactual Explanations for Time Series Classification.pdf:pdf},
title = {{Instance-Based Counterfactual Explanations for Time Series Classification}},
url = {http://arxiv.org/abs/2009.13211},
year = {2020}
}
@article{Grath2018,
abstract = {We predict credit applications with off-the-shelf, interchangeable black-box classifiers and we explain single predictions with counterfactual explanations. Counterfactual explanations expose the minimal changes required on the input data to obtain a different result e.g., approved vs rejected application. Despite their effectiveness, counterfactuals are mainly designed for changing an undesired outcome of a prediction i.e. loan rejected. Counterfactuals, however, can be difficult to interpret, especially when a high number of features are involved in the explanation. Our contribution is two-fold: i) we propose positive counterfactuals, i.e. we adapt counterfactual explanations to also explain accepted loan applications, and ii) we propose two weighting strategies to generate more interpretable counterfactuals. Experiments on the HELOC loan applications dataset show that our contribution outperforms the baseline counterfactual generation strategy, by leading to smaller and hence more interpretable counterfactuals.},
archivePrefix = {arXiv},
arxivId = {1811.05245},
author = {Grath, Rory Mc and Costabello, Luca and Van, Chan Le and Sweeney, Paul and Kamiab, Farbod and Shen, Zhao and Lecue, Freddy},
eprint = {1811.05245},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Grath et al. - 2018 - Interpretable Credit Application Predictions With Counterfactual Explanations(2).pdf:pdf},
pages = {1--9},
title = {{Interpretable Credit Application Predictions With Counterfactual Explanations}},
url = {http://arxiv.org/abs/1811.05245},
year = {2018}
}
@article{Yang2019a,
abstract = {Interpretability is an important area of research for safe deployment of machine learning systems. One particular type of interpretability method attributes model decisions to input features. Despite active development, quantitative evaluation of feature attribution methods remains difficult due to the lack of ground truth: we do not know which input features are in fact important to a model. In this work, we propose a framework for Benchmarking Attribution Methods (BAM) with a priori knowledge of relative feature importance. BAM includes 1) a carefully crafted dataset and models trained with known relative feature importance and 2) three complementary metrics to quantitatively evaluate attribution methods by comparing feature attributions between pairs of models and pairs of inputs. Our evaluation on several widely-used attribution methods suggests that certain methods are more likely to produce false positive explanations---features that are incorrectly attributed as more important to model prediction. We open source our dataset, models, and metrics.},
archivePrefix = {arXiv},
arxivId = {1907.09701},
author = {Yang, Mengjiao and Kim, Been},
eprint = {1907.09701},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang, Kim - 2019 - Benchmarking Attribution Methods with Relative Feature Importance.pdf:pdf},
number = {2018},
title = {{Benchmarking Attribution Methods with Relative Feature Importance}},
url = {http://arxiv.org/abs/1907.09701},
year = {2019}
}
@article{Doshi-Velez2017,
abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
archivePrefix = {arXiv},
arxivId = {1702.08608},
author = {Doshi-Velez, Finale and Kim, Been},
eprint = {1702.08608},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Doshi-Velez, Kim - 2017 - Towards A Rigorous Science of Interpretable Machine Learning(2).pdf:pdf},
number = {Ml},
pages = {1--13},
title = {{Towards A Rigorous Science of Interpretable Machine Learning}},
url = {http://arxiv.org/abs/1702.08608},
year = {2017}
}
@article{Manuscript2020,
abstract = {Cladding austenitic stainless steels are trendy these days in pressure vessels to enhance surface qualities. In this work, austenitic stainless steel clad layers deposited by flux cored arc welding process on structural steel plates used in boiler construction are investigated. To facilitate this, composite clad layers are produced with 316L stainless steel deposits on low carbon structural steel plates based on the design of experiments. Results exhibit an incremental trend of thermal conductivity with respect to percentage of weld dilution. A linear mathematical relationship is established between the percentage of weld dilution and post-weld thermal conductivity of clad layers for the prediction of thermal conductivity of clad layer deposits. These results could be supportive in the fabrication industries for producing energy efficient thermal equipment},
author = {Manuscript, Accepted},
title = {{Ac ce us}},
year = {2020}
}
@article{Zhang2018d,
abstract = {This paper reviews recent studies in understanding neural-network representations and learning neural networks with interpretable/disentangled middle-layer representations. Although deep neural networks have exhibited superior performance in various tasks, interpretability is always Achilles' heel of deep neural networks. At present, deep neural networks obtain high discrimination power at the cost of a low interpretability of their black-box representations. We believe that high model interpretability may help people break several bottlenecks of deep learning, e.g., learning from a few annotations, learning via human–computer communications at the semantic level, and semantically debugging network representations. We focus on convolutional neural networks (CNNs), and revisit the visualization of CNN representations, methods of diagnosing representations of pre-trained CNNs, approaches for disentangling pre-trained CNN representations, learning of CNNs with disentangled representations, and middle-to-end learning based on model interpretability. Finally, we discuss prospective trends in explainable artificial intelligence.},
archivePrefix = {arXiv},
arxivId = {1802.00614},
author = {shi Zhang, Quan and chun Zhu, Song},
doi = {10.1631/FITEE.1700808},
eprint = {1802.00614},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Zhu - 2018 - Visual interpretability for deep learning a survey(2).pdf:pdf},
issn = {20959230},
journal = {Frontiers of Information Technology and Electronic Engineering},
keywords = {Artificial intelligence,Deep learning,Interpretable model},
number = {1},
pages = {27--39},
title = {{Visual interpretability for deep learning: a survey}},
volume = {19},
year = {2018}
}
@article{Jang2018,
abstract = {In this paper, we aimed to understand and analyze the outputs of a convolutional neural network model that classifies the laterality of fundus images. Our model not only automatizes the classification process, which results in reducing the labors of clinicians, but also highlights the key regions in the image and evaluates the uncertainty for the decision with proper analytic tools. Our model was trained and tested with 25,911 fundus images (43.4% of macula-centered images and 28.3% each of superior and nasal retinal fundus images). Also, activation maps were generated to mark important regions in the image for the classification. Then, uncertainties were quantified to support explanations as to why certain images were incorrectly classified under the proposed model. Our model achieved a mean training accuracy of 99%, which is comparable to the performance of clinicians. Strong activations were detected at the location of optic disc and retinal blood vessels around the disc, which matches to the regions that clinicians attend when deciding the laterality. Uncertainty analysis discovered that misclassified images tend to accompany with high prediction uncertainties and are likely ungradable. We believe that visualization of informative regions and the estimation of uncertainty, along with presentation of the prediction result, would enhance the interpretability of neural network models in a way that clinicians can be benefitted from using the automatic classification system.},
author = {Jang, Yeonwoo and Son, Jaemin and Park, Kyu Hyung and Park, Sang Jun and Jung, Kyu Hwan},
doi = {10.1007/s10278-018-0099-2},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jang et al. - 2018 - Laterality Classification of Fundus Images Using Interpretable Deep Neural Network.pdf:pdf},
issn = {1618727X},
journal = {Journal of Digital Imaging},
keywords = {Deep learning,Deep neural network,Fundus images,Interpretability,Laterality classification},
number = {6},
pages = {923--928},
pmid = {29948436},
publisher = {Journal of Digital Imaging},
title = {{Laterality Classification of Fundus Images Using Interpretable Deep Neural Network}},
volume = {31},
year = {2018}
}
@article{Yeh2018,
abstract = {We propose to explain the predictions of a deep neural network, by pointing to the set of what we call representer points in the training set, for a given test point prediction. Specifically, we show that we can decompose the pre-activation prediction of a neural network into a linear combination of activations of training points, with the weights corresponding to what we call representer values, which thus capture the importance of that training point on the learned parameters of the network. But it provides a deeper understanding of the network than simply training point influence: with positive representer values corresponding to excitatory training points, and negative values corresponding to inhibitory points, which as we show provides considerably more insight. Our method is also much more scalable, allowing for real-time feedback in a manner not feasible with influence functions.},
archivePrefix = {arXiv},
arxivId = {1811.09720},
author = {Yeh, Chih Kuan and Kim, Joon Sik and Yen, Ian E.H. and Ravikumar, Pradeep},
eprint = {1811.09720},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yeh et al. - 2018 - Representer point selection for explaining deep neural networks.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {Nips},
pages = {9291--9301},
title = {{Representer point selection for explaining deep neural networks}},
volume = {2018-Decem},
year = {2018}
}
@misc{Roth1988,
abstract = {In the finite theory of von Neumann and Morgenstern difficulty in evaluation persists for the essential games, and for only those. In this paper a value is deduced for the essential case and a number of its elementary properties are examined. A set of three axioms, having simple intuitive interpretations, which suffice to determine the value uniquely serves as a starting point.},
author = {Roth, Alvin E.},
booktitle = {The Shapley Value},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Roth - 1988 - A value for n-person games.pdf:pdf},
isbn = {0-521-36177-X},
keywords = {*GAME THEORY,FUNCTIONS(MATHEMATICS),MATHEMATICAL ANALYSIS,SET THEORY,THEOREMS},
pages = {31--41},
title = {{A value for n-person games}},
year = {1988}
}
@article{Dosilovic2018,
abstract = {In the last decade, with availability of large datasets and more computing power, machine learning systems have achieved (super)human performance in a wide variety of tasks. Examples of this rapid development can be seen in image recognition, speech analysis, strategic game planning and many more. The problem with many state-of-the-art models is a lack of transparency and interpretability. The lack of thereof is a major drawback in many applications, e.g. healthcare and finance, where rationale for model's decision is a requirement for trust. In the light of these issues, explainable artificial intelligence (XAI) has become an area of interest in research community. This paper summarizes recent developments in XAI in supervised learning, starts a discussion on its connection with artificial general intelligence, and gives proposals for further research directions.},
author = {Dosilovic, Filip Karlo and Brcic, Mario and Hlupic, Nikica},
doi = {10.23919/MIPRO.2018.8400040},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dosilovic, Brcic, Hlupic - 2018 - Explainable artificial intelligence A survey.pdf:pdf},
isbn = {9789532330977},
journal = {2018 41st International Convention on Information and Communication Technology, Electronics and Microelectronics, MIPRO 2018 - Proceedings},
keywords = {comprehensibility,explainability,explainable artificial intelligence,interpretability},
pages = {210--215},
publisher = {Croatian Society MIPRO},
title = {{Explainable artificial intelligence: A survey}},
year = {2018}
}
@article{Kang2020,
abstract = {To understand the black-box characteristics of deep networks, counterfactual explanation that deduces not only the important features of an input space but also how those features should be modified to classify input as a target class has gained an increasing interest. The patterns that deep networks have learned from a training dataset can be grasped by observing the feature variation among various classes. However, current approaches perform the feature modification to increase the classification probability for the target class irrespective of the internal characteristics of deep networks. This often leads to unclear explanations that deviate from real-world data distributions. To address this problem, we propose a counterfactual explanation method that exploits the statistics learned from a training dataset. Especially, we gradually construct an explanation by iterating over masking and composition steps. The masking step aims to select an important feature from the input data to be classified as a target class. Meanwhile, the composition step aims to optimize the previously selected feature by ensuring that its output score is close to the logit space of the training data that are classified as the target class. Experimental results show that our method produces human-friendly interpretations on various classification datasets and verify that such interpretations can be achieved with fewer feature modification.},
archivePrefix = {arXiv},
arxivId = {2008.01897},
author = {Kang, Sin-Han and Jung, Hong-Gyu and Won, Dong-Ok and Lee, Seong-Whan},
eprint = {2008.01897},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kang et al. - 2020 - Counterfactual Explanation Based on Gradual Construction for Deep Networks(2).pdf:pdf},
pages = {1--26},
title = {{Counterfactual Explanation Based on Gradual Construction for Deep Networks}},
url = {http://arxiv.org/abs/2008.01897},
year = {2020}
}
@article{Elton2020,
abstract = {The ability to explain decisions made by AI systems is highly sought after, especially in domains where human lives are at stake such as medicine or autonomous vehicles. While it is often possible to approximate the input-output relations of deep neural networks with a few human-understandable rules, the discovery of the double descent phenomena suggests that such approximations do not accurately capture the mechanism by which deep neural networks work. Double descent indicates that deep neural networks typically operate by smoothly interpolating between data points rather than by extracting a few high level rules. As a result, neural networks trained on complex real world data are inherently hard to interpret and prone to failure if asked to extrapolate. To show how we might be able to trust AI despite these problems we introduce the concept of self-explaining AI. Self-explaining AIs are capable of providing a human-understandable explanation of each decision along with confidence levels for both the decision and explanation. Some difficulties with this approach along with possible solutions are sketched. Finally, we argue it is important that deep learning based systems include a “warning light” based on techniques from applicability domain analysis to warn the user if a model is asked to extrapolate outside its training distribution.},
archivePrefix = {arXiv},
arxivId = {2002.05149},
author = {Elton, Daniel C.},
doi = {10.1007/978-3-030-52152-3_10},
eprint = {2002.05149},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Elton - 2020 - Self-explaining ai as an alternative to interpretable ai(2).pdf:pdf},
isbn = {9783030521516},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Deep learning,Explainability,Explainable artificial intelligence,Interpretability,Trust,XAI},
pages = {95--106},
title = {{Self-explaining ai as an alternative to interpretable ai}},
volume = {12177 LNAI},
year = {2020}
}
@article{Keane2020,
archivePrefix = {arXiv},
arxivId = {2007.05684},
author = {Keane, Mark T. and Smyth, Barry},
eprint = {2007.05684},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Keane, Smyth - 2020 - Good Counterfactuals and Where to Find Them A Case-Based Technique for Generating Counterfactuals for Explainab(2).pdf:pdf},
journal = {28th International Conference on Case Based Reasoning (ICCBR2020)},
keywords = {cbr,contrastive,counterfactuals,explanation,xai},
title = {{Good Counterfactuals and Where to Find Them: A Case-Based Technique for Generating Counterfactuals for Explainable AI (XAI)}},
url = {http://arxiv.org/abs/2007.05684},
year = {2020}
}
@article{Ribeiro2018,
abstract = {We introduce a novel model-agnostic system that explains the behavior of complex models with high-precision rules called anchors, representing local, “sufficient” conditions for predictions. We propose an algorithm to efficiently compute these explanations for any black-box model with high-probability guarantees. We demonstrate the flexibility of anchors by explaining a myriad of different models for different domains and tasks. In a user study, we show that anchors enable users to predict how a model would behave on unseen instances with less effort and higher precision, as compared to existing linear explanations or no explanations.},
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ribeiro, Singh, Guestrin - 2018 - Anchors High-precision model-agnostic explanations.pdf:pdf},
isbn = {9781577358008},
journal = {32nd AAAI Conference on Artificial Intelligence, AAAI 2018},
pages = {1527--1535},
title = {{Anchors: High-precision model-agnostic explanations}},
year = {2018}
}
@article{Thiagarajan2016,
abstract = {With the advent of highly predictive but opaque deep learning models, it has become more important than ever to understand and explain the predictions of such models. Existing approaches define interpretability as the inverse of complexity and achieve interpretability at the cost of accuracy. This introduces a risk of producing interpretable but misleading explanations. As humans, we are prone to engage in this kind of behavior \cite{mythos}. In this paper, we take a step in the direction of tackling the problem of interpretability without compromising the model accuracy. We propose to build a Treeview representation of the complex model via hierarchical partitioning of the feature space, which reveals the iterative rejection of unlikely class labels until the correct association is predicted.},
archivePrefix = {arXiv},
arxivId = {1611.07429},
author = {Thiagarajan, Jayaraman J. and Kailkhura, Bhavya and Sattigeri, Prasanna and Ramamurthy, Karthikeyan Natesan},
eprint = {1611.07429},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Thiagarajan et al. - 2016 - TreeView Peeking into Deep Neural Networks Via Feature-Space Partitioning.pdf:pdf},
number = {Nips},
title = {{TreeView: Peeking into Deep Neural Networks Via Feature-Space Partitioning}},
url = {http://arxiv.org/abs/1611.07429},
year = {2016}
}
@article{Laugel2018,
abstract = {In the context of post-hoc interpretability, this paper addresses the task of explaining the prediction of a classifier, considering the case where no information is available, neither on the classifier itself, nor on the processed data (neither the training nor the test data). It proposes an inverse classification approach whose principle consists in determining the minimal changes needed to alter a prediction: in an instance-based framework, given a data point whose classification must be explained, the proposed method consists in identifying a close neighbor classified differently, where the closeness definition integrates a sparsity constraint. This principle is implemented using observation generation in the Growing Spheres algorithm. Experimental results on two datasets illustrate the relevance of the proposed approach that can be used to gain knowledge about the classifier.},
archivePrefix = {arXiv},
arxivId = {arXiv:1712.08443v1},
author = {Laugel, Thibault and Lesot, Marie Jeanne and Marsala, Christophe and Renard, Xavier and Detyniecki, Marcin},
doi = {10.1007/978-3-319-91473-2_9},
eprint = {arXiv:1712.08443v1},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Laugel et al. - 2018 - Comparison-based inverse classification for interpretability in machine learning(3).pdf:pdf},
isbn = {9783319914725},
issn = {18650929},
journal = {Communications in Computer and Information Science},
keywords = {Comparison-based,Inverse classification,Local explanation,Post-hoc interpretability},
pages = {100--111},
title = {{Comparison-based inverse classification for interpretability in machine learning}},
volume = {853},
year = {2018}
}
@article{Ghorbani2019,
abstract = {Interpretability has become an important topic of research as more machine learning (ML) models are deployed and widely used to make important decisions. Most of the current explanation methods provide explanations through feature importance scores, which identify features that are important for each individual input. However, how to systematically summarize and interpret such per sample feature importance scores itself is challenging. In this work, we propose principles and desiderata for concept based explanation, which goes beyond per-sample features to identify higher level human-understandable concepts that apply across the entire dataset. We develop a new algorithm, ACE, to automatically extract visual concepts. Our systematic experiments demonstrate that ACE discovers concepts that are human-meaningful, coherent and important for the neural network's predictions.},
archivePrefix = {arXiv},
arxivId = {1902.03129},
author = {Ghorbani, Amirata and Wexler, James and Zou, James and Kim, Been},
eprint = {1902.03129},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ghorbani et al. - 2019 - Towards automatic concept-based explanations.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
title = {{Towards automatic concept-based explanations}},
volume = {32},
year = {2019}
}
@article{Capra2020,
abstract = {Deep Neural Networks (DNNs) are nowadays a common practice in most of the Artificial Intelligence (AI) applications. Their ability to go beyond human precision has made these networks a milestone in the history of AI. However, while on the one hand they present cutting edge performance, on the other hand they require enormous computing power. For this reason, numerous optimization techniques at the hardware and software level, and specialized architectures, have been developed to process these models with high performance and power/energy efficiency without affecting their accuracy. In the past, multiple surveys have been reported to provide an overview of different architectures and optimization techniques for efficient execution of Deep Learning (DL) algorithms. This work aims at providing an up-to-date survey, especially covering the prominent works from the last 3 years of the hardware architectures research for DNNs. In this paper, the reader will first understand what a hardware accelerator is, and what are its main components, followed by the latest techniques in the field of dataflow, reconfigurability, variable bit-width, and sparsity.},
author = {Capra, Maurizio and Bussolino, Beatrice and Marchisio, Alberto and Shafique, Muhammad and Masera, Guido and Martina, Maurizio},
doi = {10.3390/fi12070113},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Capra et al. - 2020 - An Updated Survey of Efficient Hardware Architectures for Accelerating Deep Convolutional Neural Networks.pdf:pdf},
issn = {1999-5903},
journal = {Future Internet},
keywords = {ai,area,artificial intelligence,cnns,computer architecture,convolutional neural networks,data flow,deep learning,deep neural networks,dnns,efficiency,energy,hardware accelerator,latency,machine learning,optimization,performance,power consumption,vlsi},
number = {7},
pages = {113},
title = {{An Updated Survey of Efficient Hardware Architectures for Accelerating Deep Convolutional Neural Networks}},
volume = {12},
year = {2020}
}
@article{Fellous2019,
abstract = {The use of Artificial Intelligence and machine learning in basic research and clinical neuroscience is increasing. AI methods enable the interpretation of large multimodal datasets that can provide unbiased insights into the fundamental principles of brain function, potentially paving the way for earlier and more accurate detection of brain disorders and better informed intervention protocols. Despite AI's ability to create accurate predictions and classifications, in most cases it lacks the ability to provide a mechanistic understanding of how inputs and outputs relate to each other. Explainable Artificial Intelligence (XAI) is a new set of techniques that attempts to provide such an understanding, here we report on some of these practical approaches. We discuss the potential value of XAI to the field of neurostimulation for both basic scientific inquiry and therapeutic purposes, as well as, outstanding questions and obstacles to the success of the XAI approach.},
author = {Fellous, Jean Marc and Sapiro, Guillermo and Rossi, Andrew and Mayberg, Helen and Ferrante, Michele},
doi = {10.3389/fnins.2019.01346},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fellous et al. - 2019 - Explainable Artificial Intelligence for Neuroscience Behavioral Neurostimulation(2).pdf:pdf},
issn = {1662453X},
journal = {Frontiers in Neuroscience},
keywords = {behavioral paradigms,closed-loop neurostimulation,computational psychiatry,data-driven discoveries of brain circuit theories,explain AI, closed-loop neurostimulation, computat,machine learning,neuro-behavioral decisions systems},
number = {December},
pages = {1--14},
title = {{Explainable Artificial Intelligence for Neuroscience: Behavioral Neurostimulation}},
volume = {13},
year = {2019}
}
@article{Godi2019,
abstract = {In deep learning, visualization techniques extract the salient patterns exploited by deep networks for image classification, focusing on single images; no effort has been spent in investigating whether these patterns are systematically related to precise semantic entities over multiple images belonging to a same class, thus failing to capture the very understanding of the image class the network has realized. This paper goes in this direction, presenting a visualization framework which produces a group of clusters or summaries, each one formed by crisp salient image regions focusing on a particular part that the network has exploited with high regularity to decide for a given class. The approach is based on a sparse optimization step providing sharp image saliency masks that are clustered together by means of a semantic flow similarity measure. The summaries communicate clearly what a network has exploited of a particular image class, and this is proved through automatic image tagging and with a user study. Beyond the deep network understanding, summaries are also useful for many quantitative reasons: their number is correlated with ability of a network to classify (more summaries, better performances), and they can be used to improve the classification accuracy of a network through summary-driven specializations.},
archivePrefix = {arXiv},
arxivId = {1801.09103},
author = {Godi, Marco and Carletti, Marco and Aghaei, Maedeh and Giuliari, Francesco and Cristani, Marco},
eprint = {1801.09103},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Godi et al. - 2019 - Understanding deep architectures by visual summaries(2).pdf:pdf},
journal = {British Machine Vision Conference 2018, BMVC 2018},
pages = {1--12},
title = {{Understanding deep architectures by visual summaries}},
year = {2019}
}
@article{Gu2019,
abstract = {A number of backpropagation-based approaches such as DeConvNets, vanilla Gradient Visualization and Guided Backpropagation have been proposed to better understand individual decisions of deep convolutional neural networks. The saliency maps produced by them are proven to be non-discriminative. Recently, the Layer-wise Relevance Propagation (LRP) approach was proposed to explain the classification decisions of rectifier neural networks. In this work, we evaluate the discriminativeness of the generated explanations and analyze the theoretical foundation of LRP, i.e. Deep Taylor Decomposition. The experiments and analysis conclude that the explanations generated by LRP are not class-discriminative. Based on LRP, we propose Contrastive Layer-wise Relevance Propagation (CLRP), which is capable of producing instance-specific, class-discriminative, pixel-wise explanations. In the experiments, we use the CLRP to explain the decisions and understand the difference between neurons in individual classification decisions. We also evaluate the explanations quantitatively with a Pointing Game and an ablation study. Both qualitative and quantitative evaluations show that the CLRP generates better explanations than the LRP.},
archivePrefix = {arXiv},
arxivId = {1812.02100},
author = {Gu, Jindong and Yang, Yinchong and Tresp, Volker},
doi = {10.1007/978-3-030-20893-6_8},
eprint = {1812.02100},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gu, Yang, Tresp - 2019 - Understanding Individual Decisions of CNNs via Contrastive Backpropagation(2).pdf:pdf},
isbn = {9783030208929},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Discriminative saliency maps,Explainable deep learning,LRP},
pages = {119--134},
title = {{Understanding Individual Decisions of CNNs via Contrastive Backpropagation}},
volume = {11363 LNCS},
year = {2019}
}
@article{Zhang2018b,
abstract = {In this paper, we study the task of detecting semantic parts of an object, e.g., a wheel of a car, under partial occlusion. We propose that all models should be trained without seeing occlusions while being able to transfer the learned knowledge to deal with occlusions. This setting alleviates the difficulty in collecting an exponentially large dataset to cover occlusion patterns and is more essential. In this scenario, the proposal-based deep networks, like RCNN-series, often produce unsatisfactory results, because both the proposal extraction and classification stages may be confused by the irrelevant occluders. To address this, [25] proposed a voting mechanism that combines multiple local visual cues to detect semantic parts. The semantic parts can still be detected even though some visual cues are missing due to occlusions. However, this method is manually-designed, thus is hard to be optimized in an end-to-end manner. In this paper, we present DeepVoting, which incorporates the robustness shown by [25] into a deep network, so that the whole pipeline can be jointly optimized. Specifically, it adds two layers after the intermediate features of a deep network, e.g., the pool-4 layer of VGGNet. The first layer extracts the evidence of local visual cues, and the second layer performs a voting mechanism by utilizing the spatial relationship between visual cues and semantic parts. We also propose an improved version DeepVoting+ by learning visual cues from context outside objects. In experiments, DeepVoting achieves significantly better performance than several baseline methods, including Faster-RCNN, for semantic part detection under occlusion. In addition, DeepVoting enjoys explainability as the detection results can be diagnosed via looking up the voting cues.},
archivePrefix = {arXiv},
arxivId = {1709.04577},
author = {Zhang, Zhishuai and Xie, Cihang and Wang, Jianyu and Xie, Lingxi and Yuille, Alan L.},
doi = {10.1109/CVPR.2018.00149},
eprint = {1709.04577},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2018 - DeepVoting A Robust and Explainable Deep Network for Semantic Part Detection under Partial Occlusion(2).pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {1372--1380},
title = {{DeepVoting: A Robust and Explainable Deep Network for Semantic Part Detection under Partial Occlusion}},
volume = {1},
year = {2018}
}
@article{Bang2019,
abstract = {Interpretable machine learning has gained much attention recently. Briefness and comprehensiveness are necessary in order to provide a large amount of information concisely when explaining a black-box decision system. However, existing interpretable machine learning methods fail to consider briefness and comprehensiveness simultaneously, leading to redundant explanations. We propose the variational information bottleneck for interpretation, VIBI, a system-agnostic interpretable method that provides a brief but comprehensive explanation. VIBI adopts an information theoretic principle, information bottleneck principle, as a criterion for finding such explanations. For each instance, VIBI selects key features that are maximally compressed about an input (briefness), and informative about a decision made by a black-box system on that input (comprehensive). We evaluate VIBI on three datasets and compare with state-of-the-art interpretable machine learning methods in terms of both interpretability and fidelity evaluated by human and quantitative metrics},
archivePrefix = {arXiv},
arxivId = {1902.06918},
author = {Bang, Seojin and Xie, Pengtao and Lee, Heewook and Wu, Wei and Xing, Eric},
eprint = {1902.06918},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bang et al. - 2019 - Explaining a black-box using Deep Variational Information Bottleneck Approach.pdf:pdf},
number = {Mi},
title = {{Explaining a black-box using Deep Variational Information Bottleneck Approach}},
url = {http://arxiv.org/abs/1902.06918},
year = {2019}
}
@article{Manuscript2020,
abstract = {Cladding austenitic stainless steels are trendy these days in pressure vessels to enhance surface qualities. In this work, austenitic stainless steel clad layers deposited by flux cored arc welding process on structural steel plates used in boiler construction are investigated. To facilitate this, composite clad layers are produced with 316L stainless steel deposits on low carbon structural steel plates based on the design of experiments. Results exhibit an incremental trend of thermal conductivity with respect to percentage of weld dilution. A linear mathematical relationship is established between the percentage of weld dilution and post-weld thermal conductivity of clad layers for the prediction of thermal conductivity of clad layer deposits. These results could be supportive in the fabrication industries for producing energy efficient thermal equipment},
author = {Manuscript, Accepted},
title = {{Ac ce us}},
year = {2020}
}
@article{Robnik-Sikonja2008,
abstract = {We present a method for explaining predictions for individual instances. The presented approach is general and can be used with all classification models that output probabilities. It is based on the decomposition of a model's predictions on individual contributions of each attribute. Our method works for the so-called black box models such as support vector machines, neural networks, and nearest neighbor algorithms, as well as for ensemble methods such as boosting and random forests. We demonstrate that the generated explanations closely follow the learned models and present a visualization technique that shows the utility of our approach and enables the comparison of different prediction methods. {\textcopyright} 2007 IEEE.},
author = {Robnik-{\v{S}}ikonja, Marko and Kononenko, Igor},
doi = {10.1109/TKDE.2007.190734},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Robnik-{\v{S}}ikonja, Kononenko - 2008 - Explaining classifications for individual instances.pdf:pdf},
issn = {10414347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Classification,Decision support,Decision visualization,Information visualization,Knowledge modeling,Machine learning,Model comprehensibility,Model explanation,Nearest eighbor,Neural nets,Prediction models,Support vector machines},
number = {5},
pages = {589--600},
title = {{Explaining classifications for individual instances}},
volume = {20},
year = {2008}
}
@article{Selvaraju2016,
abstract = {We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing input regions that are 'important' for predictions -- or visual explanations. Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses class-specific gradient information to localize important regions. These localizations are combined with existing pixel-space visualizations to create a novel high-resolution and class-discriminative visualization called Guided Grad-CAM. These methods help better understand CNN-based models, including image captioning and visual question answering (VQA) models. We evaluate our visual explanations by measuring their ability to discriminate between classes, to inspire trust in humans, and their correlation with occlusion maps. Grad-CAM provides a new way to understand CNN-based models. We have released code, an online demo hosted on CloudCV, and a full version of this extended abstract.},
archivePrefix = {arXiv},
arxivId = {1611.07450},
author = {Selvaraju, Ramprasaath R and Das, Abhishek and Vedantam, Ramakrishna and Cogswell, Michael and Parikh, Devi and Batra, Dhruv},
eprint = {1611.07450},
pages = {1--4},
title = {{Grad-CAM: Why did you say that?}},
url = {http://arxiv.org/abs/1611.07450},
year = {2016}
}
@article{Atakishiyev2020,
abstract = {The rapid growth of research in explainable artificial intelligence (XAI) follows on two substantial developments. First, the enormous application success of modern machine learning methods, especially deep and reinforcement learning, which have created high expectations for industrial, commercial and social value. Second, the emergence of concern for creating trusted AI systems, including the creation of regulatory principles to ensure transparency and trust of AI systems. These two threads have created a kind of “perfect storm” of research activity, all eager to create and deliver any set of tools and techniques to address the XAI demand. As some surveys of current XAI suggest, there is yet to appear a principled framework that respects the literature of explainability in the history of science, and which provides a basis for the development of a framework for transparent XAI. Here we intend to provide a strategic inventory of XAI requirements, demonstrate their connection to a history of XAI ideas, and synthesize those ideas into a simple framework to calibrate five successive levels of XAI.},
archivePrefix = {arXiv},
arxivId = {2005.01908},
author = {Atakishiyev, S. and Babiker, H. and Farruque, N. and Goebel, R. and Kim, M. Y. and Motallebi, M. H. and Rabelo, J. and Syed, T. and Za{\"{i}}ane, O. R.},
eprint = {2005.01908},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Atakishiyev et al. - 2020 - A multi-component framework for the analysis and design of explainable artificial intelligence.pdf:pdf},
issn = {23318422},
journal = {arXiv},
keywords = {Interpretation explanation reproducibility causal},
pages = {1--39},
title = {{A multi-component framework for the analysis and design of explainable artificial intelligence}},
year = {2020}
}
@article{Mahendran2016,
abstract = {Image representations, from SIFT and bag of visual words to convolutional neural networks (CNNs) are a crucial component of almost all computer vision systems. However, our understanding of them remains limited. In this paper we study several landmark representations, both shallow and deep, by a number of complementary visualization techniques. These visualizations are based on the concept of “natural pre-image”, namely a natural-looking image whose representation has some notable property. We study in particular three such visualizations: inversion, in which the aim is to reconstruct an image from its representation, activation maximization, in which we search for patterns that maximally stimulate a representation component, and caricaturization, in which the visual patterns that a representation detects in an image are exaggerated. We pose these as a regularized energy-minimization framework and demonstrate its generality and effectiveness. In particular, we show that this method can invert representations such as HOG more accurately than recent alternatives while being applicable to CNNs too. Among our findings, we show that several layers in CNNs retain photographically accurate information about the image, with different degrees of geometric and photometric invariance.},
archivePrefix = {arXiv},
arxivId = {1512.02017},
author = {Mahendran, Aravindh and Vedaldi, Andrea},
doi = {10.1007/s11263-016-0911-8},
eprint = {1512.02017},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mahendran, Vedaldi - 2016 - Visualizing Deep Convolutional Neural Networks Using Natural Pre-images.pdf:pdf},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Convolutional neural networks,Pre-image problem,Visualization},
number = {3},
pages = {233--255},
publisher = {Springer US},
title = {{Visualizing Deep Convolutional Neural Networks Using Natural Pre-images}},
volume = {120},
year = {2016}
}
@article{Zeiler2011,
abstract = {We present a hierarchical model that learns image decompositions via alternating layers of convolutional sparse coding and max pooling. When trained on natural images, the layers of our model capture image information in a variety of forms: low-level edges, mid-level edge junctions, high-level object parts and complete objects. To build our model we rely on a novel inference scheme that ensures each layer reconstructs the input, rather than just the output of the layer directly beneath, as is common with existing hierarchical approaches. This makes it possible to learn multiple layers of representation and we show models with 4 layers, trained on images from the Caltech-101 and 256 datasets. When combined with a standard classifier, features extracted from these models outperform SIFT, as well as representations from other feature learning methods. {\textcopyright} 2011 IEEE.},
author = {Zeiler, Matthew D. and Taylor, Graham W. and Fergus, Rob},
doi = {10.1109/ICCV.2011.6126474},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zeiler, Taylor, Fergus - 2011 - Adaptive deconvolutional networks for mid and high level feature learning.pdf:pdf},
isbn = {9781457711015},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {2018--2025},
title = {{Adaptive deconvolutional networks for mid and high level feature learning}},
year = {2011}
}
@article{Montavon2018,
abstract = {This paper provides an entry point to the problem of interpreting a deep neural network model and explaining its predictions. It is based on a tutorial given at ICASSP 2017. As a tutorial paper, the set of methods covered here is not exhaustive, but sufficiently representative to discuss a number of questions in interpretability, technical challenges, and possible applications. The second part of the tutorial focuses on the recently proposed layer-wise relevance propagation (LRP) technique, for which we provide theory, recommendations, and tricks, to make most efficient use of it on real data.},
archivePrefix = {arXiv},
arxivId = {1706.07979},
author = {Montavon, Gr{\'{e}}goire and Samek, Wojciech and M{\"{u}}ller, Klaus Robert},
doi = {10.1016/j.dsp.2017.10.011},
eprint = {1706.07979},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Montavon, Samek, M{\"{u}}ller - 2018 - Methods for interpreting and understanding deep neural networks.pdf:pdf},
issn = {10512004},
journal = {Digital Signal Processing: A Review Journal},
keywords = {Activation maximization,Deep neural networks,Layer-wise relevance propagation,Sensitivity analysis,Taylor decomposition},
pages = {1--15},
publisher = {Elsevier Inc.},
title = {{Methods for interpreting and understanding deep neural networks}},
url = {https://doi.org/10.1016/j.dsp.2017.10.011},
volume = {73},
year = {2018}
}
@article{Yeh2019,
abstract = {Deep neural networks (DNNs) build high-level intelligence on low-level raw features. Under- standing of this high-level intelligence can be enabled by deciphering the concepts they base their decisions on, as human-level thinking. In this paper, we study concept-based explainability for DNNs in a systematic framework. First, we define the notion of completeness, which quantifies how sufficient a particular set of concepts is in explaining a model's prediction behavior. Based on performance and variability motivations, we propose two definitions to quantify completeness. We show that under degenerate conditions, our method is equivalent to Principal Component Analysis. Next, we propose a concept discovery method that considers two additional constraints to encourage the interpretability of the discovered concepts. We use game-theoretic notions to aggregate over sets to define an importance score for each discovered concept, which we call ConceptSHAP. On specifically-designed synthetic datasets and real-world text and image datasets, we validate the effectiveness of our framework in finding concepts that are complete in explaining the decision, and interpretable.},
archivePrefix = {arXiv},
arxivId = {1910.07969},
author = {Yeh, Chih-Kuan and Kim, Been and Arik, Sercan O. and Li, Chun-Liang and Pfister, Tomas and Ravikumar, Pradeep},
eprint = {1910.07969},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yeh et al. - 2019 - On Concept-Based Explanations in Deep Neural Networks.pdf:pdf},
title = {{On Concept-Based Explanations in Deep Neural Networks}},
url = {http://arxiv.org/abs/1910.07969},
year = {2019}
}
@article{Yang2017,
abstract = {Modern machine learning methods are increasingly powerful and opaque. This opaqueness is a concern across a variety of domains in which algorithms are making important decisions that should be scrutable. The explainabilty of machine learning systems is therefore of increasing interest. We propose an explanation-byexamples approach that builds on our recent research in Bayesian teaching in which we aim to select a small subset of the data that would lead the learner to similar conclusions as the entire dataset. We discuss this approach, explicating several key advantages. First, the ability to cover any model with a probabilistic interpretation including supervised, unsupervised, and reinforcement learning (including deep learning). Second, we discuss the empirical foundations of this approach in the cognitive science of learning from other agents. Third, we outline challenges to full realization of the promise of this approach. We conclude by discussing implications for machine learning and applications to real-world problems.},
author = {Yang, Scott Cheng-Hsin and Shafto, Patrick},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang, Shafto - 2017 - Explainable Artificial Intelligence via Bayesian Teaching(2).pdf:pdf},
journal = {Neural Information Processing Systems Workshop: Teaching Machines, Robots, and Humans},
number = {Nips},
title = {{Explainable Artificial Intelligence via Bayesian Teaching}},
year = {2017}
}
@article{Erhan2010,
abstract = {Deep architectures have demonstrated state-of-the-art performance in a variety of settings, especially with vision datasets. Deep learning algorithms are based on learn- ing several levels of representation of the input. Beyond test-set performance, there is a need for qualitative comparisons of the solutions learned by various deep archi- tectures, focused on those learned representations. One of the goals of our research is to improve tools for finding good qualitative interpretations of high level features learned by such models. We also seek to gain insight into the invariances learned by deep networks. To this end, we contrast and compare several techniques for finding such interpretations. We applied our techniques on Stacked Denoising Auto-Encoders and Deep Belief Networks, trained on several vision datasets. We show that consistent filter-like interpretation is possible and simple to accomplish at the unit level. The tools developed make it possible to analyze deep models in more depth and accomplish the tracing of invariance manifolds for each of the hidden units. We hope that such tech- niques will allow researchers in deep architectures to understandmore of how and why deep architectures work.},
archivePrefix = {arXiv},
arxivId = {arXiv:1206.5538v1},
author = {Erhan, Dumitru and Courville, Aaron and Bengio, Yoshua},
eprint = {arXiv:1206.5538v1},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Erhan, Courville, Bengio - 2010 - Understanding Representations Learned in Deep Architectures.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Network},
number = {October 2014},
pages = {1--25},
title = {{Understanding Representations Learned in Deep Architectures}},
year = {2010}
}
@article{Haghighi2019,
abstract = {Autoencoders are a common building block of Deep Learning architectures, where they are mainly used for representation learning. They have also been successfully used in Collaborative Filtering (CF) recommender systems to predict missing ratings. Unfortunately, like all black box machine learning models, they are unable to explain their outputs. Hence, while predictions from an Autoencoder-based recommender system might be accurate, it might not be clear to the user why a recommendation was generated. In this work, we design an explainable recommendation system using an Autoencoder model whose predictions can be explained using the neighborhood based explanation style. Our preliminary work can be considered to be the first step towards an explainable deep learning architecture based on Autoencoders.},
archivePrefix = {arXiv},
arxivId = {2001.04344},
author = {Haghighi, Pegah Sagheb and Seton, Olurotimi and Nasraoui, Olfa},
eprint = {2001.04344},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Haghighi, Seton, Nasraoui - 2019 - An Explainable Autoencoder For Collaborative Filtering Recommendation(2).pdf:pdf},
keywords = {autoen-,coder,deep learning,explainability,neural networks,recommender systems},
title = {{An Explainable Autoencoder For Collaborative Filtering Recommendation}},
url = {http://arxiv.org/abs/2001.04344},
year = {2019}
}
@article{Li2021,
abstract = {Recent years, many researches attempt to open the black box of deep neural networks and propose a various of theories to understand it. Among them, information bottleneck (IB) theory claims that there are two distinct phases consisting of fitting phase and compression phase in the course of training. This statement attracts many attentions since its success in explaining the inner behavior of feedforward neural networks. In this paper, we employ IB theory to understand the dynamic behavior of convolutional neural networks (CNNs) and investigate how the fundamental features such as convolutional layer width, kernel size, network depth, pooling layers and multi-fully connected layer have impact on the performance of CNNs. In particular, through a series of experimental analysis on benchmark of MNIST and Fashion-MNIST, we demonstrate that the compression phase is not observed in all these cases. This shows us the CNNs have a rather complicated behavior than feedforward neural networks.},
archivePrefix = {arXiv},
arxivId = {1911.03722},
author = {Li, Junjie and Liu, Ding},
doi = {10.1007/s11063-021-10445-6},
eprint = {1911.03722},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Liu - 2021 - Information Bottleneck Theory on Convolutional Neural Networks.pdf:pdf},
issn = {1573773X},
journal = {Neural Processing Letters},
keywords = {Convolutional neural networks,Deep learning,Information bottleneck,Representation learning},
number = {2},
pages = {1385--1400},
title = {{Information Bottleneck Theory on Convolutional Neural Networks}},
volume = {53},
year = {2021}
}
@article{Hsieh2020,
abstract = {Feature based explanations, that provide importance of each feature towards the model prediction, is arguably one of the most intuitive ways to explain a model. In this paper, we establish a novel set of evaluation criteria for such feature based explanations by robustness analysis. In contrast to existing evaluations which require us to specify some way to "remove" features that could inevitably introduces biases and artifacts, we make use of the subtler notion of smaller adversarial perturbations. By optimizing towards our proposed evaluation criteria, we obtain new explanations that are loosely necessary and sufficient for a prediction. We further extend the explanation to extract the set of features that would move the current prediction to a target class by adopting targeted adversarial attack for the robustness analysis. Through experiments across multiple domains and a user study, we validate the usefulness of our evaluation criteria and our derived explanations.},
archivePrefix = {arXiv},
arxivId = {2006.00442},
author = {Hsieh, Cheng-Yu and Yeh, Chih-Kuan and Liu, Xuanqing and Ravikumar, Pradeep and Kim, Seungyeon and Kumar, Sanjiv and Hsieh, Cho-Jui},
eprint = {2006.00442},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hsieh et al. - 2020 - Evaluations and Methods for Explanation through Robustness Analysis(2).pdf:pdf},
pages = {1--21},
title = {{Evaluations and Methods for Explanation through Robustness Analysis}},
url = {http://arxiv.org/abs/2006.00442},
year = {2020}
}
@article{Yuan2020,
abstract = {Deep learning methods are achieving ever-increasing performance on many artificial intelligence tasks. A major limitation of deep models is that they are not amenable to interpretability. This limitation can be circumvented by developing post hoc techniques to explain the predictions, giving rise to the area of explainability. Recently, explainability of deep models on images and texts has achieved significant progress. In the area of graph data, graph neural networks (GNNs) and their explainability are experiencing rapid developments. However, there is neither a unified treatment of GNN explainability methods, nor a standard benchmark and testbed for evaluations. In this survey, we provide a unified and taxonomic view of current GNN explainability methods. Our unified and taxonomic treatments of this subject shed lights on the commonalities and differences of existing methods and set the stage for further methodological developments. To facilitate evaluations, we generate a set of benchmark graph datasets specifically for GNN explainability. We summarize current datasets and metrics for evaluating GNN explainability. Altogether, this work provides a unified methodological treatment of GNN explainability and a standardized testbed for evaluations.},
archivePrefix = {arXiv},
arxivId = {2012.15445},
author = {Yuan, Hao and Yu, Haiyang and Gui, Shurui and Ji, Shuiwang},
eprint = {2012.15445},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yuan et al. - 2020 - Explainability in Graph Neural Networks A Taxonomic Survey.pdf:pdf},
pages = {1--14},
title = {{Explainability in Graph Neural Networks: A Taxonomic Survey}},
url = {http://arxiv.org/abs/2012.15445},
year = {2020}
}
@article{Goyal2019a,
abstract = {In this work, we develop a technique to produce counterfactual visual explanations. Given a 'query' image I for which a vision system predicts class c, a counterfactual visual explanation identifies how I could change such that the system would output a different specified class c'. To do this, we select a 'distractor' image I' that the system predicts as class c' and identify spatial regions in I and I' such that replacing the identified region in I with the identified region in I' would push the system towards classifying I as d. We apply our approach to multiple image classification datasets generating qualitative results showcasing the interpretability and discriminativeness of our counterfactual explanations. To explore the effectiveness of our explanations in teaching humans, we present machine teaching experiments for the task of fine-grained bird classification. We find that users trained to distinguish bird species fare better when given access to counterfactual explanations in addition to training examples.},
archivePrefix = {arXiv},
arxivId = {1904.07451},
author = {Goyal, Yash and Wu, Ziyan and Ernst, Jan and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
eprint = {1904.07451},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Goyal et al. - 2019 - Counterfactual visual explanations.pdf:pdf},
isbn = {9781510886988},
journal = {36th International Conference on Machine Learning, ICML 2019},
pages = {4254--4262},
title = {{Counterfactual visual explanations}},
volume = {2019-June},
year = {2019}
}
@article{Hashemi2020,
archivePrefix = {arXiv},
arxivId = {arXiv:2008.10138v2},
author = {Hashemi, Masoud and Toronto, Bay St},
eprint = {arXiv:2008.10138v2},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hashemi, Toronto - Unknown - PermuteAttack Counterfactual Explanation of Machine Learning Credit Scorecards(2).pdf:pdf},
title = {{PermuteAttack : Counterfactual Explanation of Machine Learning Credit Scorecards}}
}
@article{Das2020,
abstract = {Nowadays, deep neural networks are widely used in mission critical systems such as healthcare, self-driving vehicles, and military which have direct impact on human lives. However, the black-box nature of deep neural networks challenges its use in mission critical applications, raising ethical and judicial concerns inducing lack of trust. Explainable Artificial Intelligence (XAI) is a field of Artificial Intelligence (AI) that promotes a set of tools, techniques, and algorithms that can generate high-quality interpretable, intuitive, human-understandable explanations of AI decisions. In addition to providing a holistic view of the current XAI landscape in deep learning, this paper provides mathematical summaries of seminal work. We start by proposing a taxonomy and categorizing the XAI techniques based on their scope of explanations, methodology behind the algorithms, and explanation level or usage which helps build trustworthy, interpretable, and self-explanatory deep learning models. We then describe the main principles used in XAI research and present the historical timeline for landmark studies in XAI from 2007 to 2020. After explaining each category of algorithms and approaches in detail, we then evaluate the explanation maps generated by eight XAI algorithms on image data, discuss the limitations of this approach, and provide potential future directions to improve XAI evaluation.},
archivePrefix = {arXiv},
arxivId = {2006.11371},
author = {Das, Arun and Rad, Paul},
eprint = {2006.11371},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Das, Rad - 2020 - Opportunities and Challenges in Explainable Artificial Intelligence (XAI) A Survey.pdf:pdf},
issn = {23318422},
journal = {arXiv},
keywords = {Computer vision,Explainable ai,Interpretable deep learning,Machine learning,Neural network,Xai},
pages = {1--24},
title = {{Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey}},
year = {2020}
}
@article{Holzinger2019,
abstract = {Explainable artificial intelligence (AI) is attracting much interest in medicine. Technically, the problem of explainability is as old as AI itself and classic AI represented comprehensible retraceable approaches. However, their weakness was in dealing with uncertainties of the real world. Through the introduction of probabilistic learning, applications became increasingly successful, but increasingly opaque. Explainable AI deals with the implementation of transparency and traceability of statistical black-box machine learning methods, particularly deep learning (DL). We argue that there is a need to go beyond explainable AI. To reach a level of explainable medicine we need causability. In the same way that usability encompasses measurements for the quality of use, causability encompasses measurements for the quality of explanations. In this article, we provide some necessary definitions to discriminate between explainability and causability as well as a use-case of DL interpretation and of human explanation in histopathology. The main contribution of this article is the notion of causability, which is differentiated from explainability in that causability is a property of a person, while explainability is a property of a system. This article is categorized under: Fundamental Concepts of Data and Knowledge > Human Centricity and User Interaction.},
author = {Holzinger, Andreas and Langs, Georg and Denk, Helmut and Zatloukal, Kurt and M{\"{u}}ller, Heimo},
doi = {10.1002/widm.1312},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Holzinger et al. - 2019 - Causability and explainability of artificial intelligence in medicine.pdf:pdf},
issn = {19424795},
journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
keywords = {artificial intelligence,causability,explainability,explainable AI,histopathology,medicine},
number = {4},
pages = {1--13},
title = {{Causability and explainability of artificial intelligence in medicine}},
volume = {9},
year = {2019}
}
@article{Kindermans2018,
abstract = {DeConvNet, Guided BackProp, LRP, were invented to better understand deep neural networks. We show that these methods do not produce the theoretically correct explanation for a linear model. Yet they are used on multi-layer networks with millions of parameters. This is a cause for concern since linear models are simple neural networks. We argue that explanation methods for neural nets should work reliably in the limit of simplicity, the linear models. Based on our analysis of linear models we propose a generalization that yields two explanation techniques (PatternNet and PatternAttribution) that are theoretically sound for linear models and produce improved explanations for deep networks.},
archivePrefix = {arXiv},
arxivId = {1705.05598},
author = {Kindermans, Pieter Jan and Sch{\"{u}}tt, Kristof T. and Alber, Maximilian and M{\"{u}}ller, Klaus Robert and Erhan, Dumitru and Kim, Been and D{\"{a}}hne, Sven},
eprint = {1705.05598},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kindermans et al. - 2018 - Learning how to explain neural networks Patternnet and Patternattribution.pdf:pdf},
journal = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
pages = {1--12},
title = {{Learning how to explain neural networks: Patternnet and Patternattribution}},
year = {2018}
}
@inproceedings{Zeiler2014,
abstract = {Large Convolutional Network models have recently demon- strated impressive classification performance on the ImageNet bench- mark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the oper- ation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al. on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of- the-art results on Caltech-101 and Caltech-256 datasets.},
author = {Zeiler, Matthew D. and Rob, Fergus},
booktitle = {European Conference on Computer Vision},
doi = {10.1016/j.ancr.2017.02.001},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zeiler, Rob - 2014 - Visualizing and Understanding Convolutional Networks.pdf:pdf},
issn = {22141812},
pages = {818--833},
title = {{Visualizing and Understanding Convolutional Networks}},
year = {2014}
}
@article{Singh2020,
abstract = {Deep learning methods have been very effective for a variety of medical diagnostic tasks and have even outperformed human experts on some of those. However, the black-box nature of the algorithms has restricted their clinical use. Recent explainability studies aim to show the features that influence the decision of a model the most. The majority of literature reviews of this area have focused on taxonomy, ethics, and the need for explanations. A review of the current applications of explainable deep learning for different medical imaging tasks is presented here. The various approaches, challenges for clinical deployment, and the areas requiring further research are discussed here from a practical standpoint of a deep learning researcher designing a system for the clinical end-users.},
author = {Singh, Amitojdeep and Sengupta, Sourya and Lakshminarayanan, Vasudevan},
doi = {10.3390/JIMAGING6060052},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Singh, Sengupta, Lakshminarayanan - 2020 - Explainable deep learning models in medical image analysis.pdf:pdf},
issn = {2313433X},
journal = {Journal of Imaging},
keywords = {Deep learning,Diagnosis,Explainability,Explainable AI,Medical imaging,XAI},
number = {6},
pages = {1--19},
title = {{Explainable deep learning models in medical image analysis}},
volume = {6},
year = {2020}
}
@article{Zhang2018c,
abstract = {We present a new algorithm to generate minimal, stable, and symbolic corrections to an input that will cause a neural network with ReLU activations to change its output. We argue that such a correction is a useful way to provide feedback to a user when the network's output is different from a desired output. Our algorithm generates such a correction by solving a series of linear constraint satisfaction problems. The technique is evaluated on three neural network models: one predicting whether an applicant will pay a mortgage, one predicting whether a first-order theorem can be proved efficiently by a solver using certain heuristics, and the final one judging whether a drawing is an accurate rendition of a canonical drawing of a cat.},
archivePrefix = {arXiv},
arxivId = {1802.07384},
author = {Zhang, Xin and Solar-Lezama, Armando and Singh, Rishabh},
eprint = {1802.07384},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Solar-Lezama, Singh - 2018 - Interpreting neural network judgments via minimal, stable, and symbolic corrections(2).pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {4874--4885},
title = {{Interpreting neural network judgments via minimal, stable, and symbolic corrections}},
volume = {2018-Decem},
year = {2018}
}
@article{Hooker2019,
abstract = {We propose an empirical measure of the approximate accuracy of feature importance estimates in deep neural networks. Our results across several large-scale image classification datasets show that many popular interpretability methods produce estimates of feature importance that are not better than a random designation of feature importance. Only certain ensemble based approaches-VarGrad and SmoothGrad-Squared-outperform such a random assignment of importance. The manner of ensembling remains critical, we show that some approaches do no better then the underlying method but carry a far higher computational burden.},
archivePrefix = {arXiv},
arxivId = {1806.10758},
author = {Hooker, Sara and Erhan, Dumitru and Kindermans, Pieter Jan and Kim, Been},
eprint = {1806.10758},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hooker et al. - 2019 - A benchmark for interpretability methods in deep neural networks.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {NeurIPS},
title = {{A benchmark for interpretability methods in deep neural networks}},
volume = {32},
year = {2019}
}
@article{Sattarzadeh2020,
abstract = {As an emerging field in Machine Learning, Explainable AI (XAI) has been offering remarkable performance in interpreting the decisions made by Convolutional Neural Networks (CNNs). To achieve visual explanations for CNNs, methods based on class activation mapping and randomized input sampling have gained great popularity. However, the attribution methods based on these techniques provide lower-resolution and blurry explanation maps that limit their explanation power. To circumvent this issue, visualization based on various layers is sought. In this work, we collect visualization maps from multiple layers of the model based on an attribution-based input sampling technique and aggregate them to reach a fine-grained and complete explanation. We also propose a layer selection strategy that applies to the whole family of CNN-based models, based on which our extraction framework is applied to visualize the last layers of each convolutional block of the model. Moreover, we perform an empirical analysis of the efficacy of derived lower-level information to enhance the represented attributions. Comprehensive experiments conducted on shallow and deep models trained on natural and industrial datasets, using both ground-truth and model-truth based evaluation metrics validate our proposed algorithm by meeting or outperforming the state-of-the-art methods in terms of explanation ability and visual quality, demonstrating that our method shows stability regardless of the size of objects or instances to be explained.},
archivePrefix = {arXiv},
arxivId = {2010.00672},
author = {Sattarzadeh, Sam and Sudhakar, Mahesh and Lem, Anthony and Mehryar, Shervin and Plataniotis, K. N. and Jang, Jongseong and Kim, Hyunwoo and Jeong, Yeonjeong and Lee, Sangmin and Bae, Kyunghoon},
eprint = {2010.00672},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sattarzadeh et al. - 2020 - Explaining convolutional neural networks through attribution-based input sampling and block-wise feature agg.pdf:pdf},
issn = {23318422},
journal = {arXiv},
title = {{Explaining convolutional neural networks through attribution-based input sampling and block-wise feature aggregation}},
year = {2020}
}
@article{Lundberg2017,
abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
archivePrefix = {arXiv},
arxivId = {1705.07874},
author = {Lundberg, Scott M. and Lee, Su In},
eprint = {1705.07874},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lundberg, Lee - 2017 - A unified approach to interpreting model predictions.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {Section 2},
pages = {4766--4775},
title = {{A unified approach to interpreting model predictions}},
volume = {2017-Decem},
year = {2017}
}
@article{Hiley2020,
abstract = {A small subset of explainability techniques developed initially for image recognition models has recently been applied for interpretability of 3D Convolutional Neural Network models in activity recognition tasks. Much like the models themselves, the techniques require little or no modification to be compatible with 3D inputs. However, these explanation techniques regard spatial and temporal information jointly. Therefore, using such explanation techniques, a user cannot explicitly distinguish the role of motion in a 3D model's decision. In fact, it has been shown that these models do not appropriately factor motion information into their decision. We propose a selective relevance method for adapting the 2D explanation techniques to provide motion-specific explanations, better aligning them with the human understanding of motion as conceptually separate from static spatial features. We demonstrate the utility of our method in conjunction with several widely-used 2D explanation methods, and show that it improves explanation selectivity for motion. Our results show that the selective relevance method can not only provide insight on the role played by motion in the model's decision -- in effect, revealing and quantifying the model's spatial bias -- but the method also simplifies the resulting explanations for human consumption.},
archivePrefix = {arXiv},
arxivId = {2003.14285},
author = {Hiley, Liam and Preece, Alun and Hicks, Yulia and Chakraborty, Supriyo and Gurram, Prudhvi and Tomsett, Richard},
eprint = {2003.14285},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hiley et al. - 2020 - Explaining Motion Relevance for Activity Recognition in Video Deep Learning Models.pdf:pdf},
pages = {1--9},
title = {{Explaining Motion Relevance for Activity Recognition in Video Deep Learning Models}},
url = {http://arxiv.org/abs/2003.14285},
year = {2020}
}
@article{Pillai2019,
abstract = {Given the widespread deployment of black box deep neural networks in computer vision applications, the interpretability aspect of these black box systems has recently gained traction. Various methods have been proposed to explain the results of such deep neural networks. However, some recent works have shown that such explanation methods are biased and do not produce consistent interpretations. Hence, rather than introducing a novel explanation method, we learn models that are encouraged to be interpretable given an explanation method. We use Grad-CAM as the explanation algorithm and encourage the network to learn consistent interpretations along with maximizing the log-likelihood of the correct class. We show that our method outperforms the baseline on the pointing game evaluation on ImageNet and MS-COCO datasets respectively. We also introduce new evaluation met-rics that penalize the saliency map if it lies outside the ground truth bounding box or segmentation mask, and show that our method outperforms the baseline on these metrics as well. Moreover, our model trained with interpretation consistency generalizes to other explanation algorithms on all the evaluation metrics.},
author = {Pillai, Vipin and Pirsiavash, Hamed},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pillai, Pirsiavash - 2019 - Explainable Models with Consistent Interpretations.pdf:pdf},
title = {{Explainable Models with Consistent Interpretations}},
year = {2019}
}
@article{Ramon2019,
abstract = {We study the interpretability of predictive systems that use high-dimensonal behavioral and textual data. Examples include predicting product interest based on online browsing data and detecting spam emails or objectionable web content. Recently, counterfactual explanations have been proposed for generating insight into model predictions, which focus on what is relevant to a particular instance. Conducting a complete search to compute counterfactuals is very time-consuming because of the huge dimensionality. To our knowledge, for behavioral and text data, only one model-agnostic heuristic algorithm (SEDC) for finding counterfactual explanations has been proposed in the literature. However, there may be better algorithms for finding counterfactuals quickly. This study aligns the recently proposed Linear Interpretable Model-agnostic Explainer (LIME) and Shapley Additive Explanations (SHAP) with the notion of counterfactual explanations, and empirically benchmarks their effectiveness and efficiency against SEDC using a collection of 13 data sets. Results show that LIME-Counterfactual (LIME-C) and SHAP-Counterfactual (SHAP-C) have low and stable computation times, but mostly, they are less efficient than SEDC. However, for certain instances on certain data sets, SEDC's run time is comparably large. With regard to effectiveness, LIME-C and SHAP-C find reasonable, if not always optimal, counterfactual explanations. SHAP-C, however, seems to have difficulties with highly unbalanced data. Because of its good overall performance, LIME-C seems to be a favorable alternative to SEDC, which failed for some nonlinear models to find counterfactuals because of the particular heuristic search algorithm it uses. A main upshot of this paper is that there is a good deal of room for further research. For example, we propose algorithmic adjustments that are direct upshots of the paper's findings.},
archivePrefix = {arXiv},
arxivId = {1912.01819},
author = {Ramon, Yanou and Martens, David and Provost, Foster and Evgeniou, Theodoros},
eprint = {1912.01819},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ramon et al. - 2019 - Counterfactual Explanation Algorithms for Behavioral and Textual Data.pdf:pdf},
title = {{Counterfactual Explanation Algorithms for Behavioral and Textual Data}},
url = {http://arxiv.org/abs/1912.01819},
year = {2019}
}
@article{Murdoch2019,
abstract = {Machine-learning models have demonstrated great success in learning complex patterns that enable them to make predictions about unobserved data. In addition to using models for prediction, the ability to interpret what a model has learned is receiving an increasing amount of attention. However, this increased focus has led to considerable confusion about the notion of interpretability. In particular, it is unclear how the wide array of proposed interpretation methods are related and what common concepts can be used to evaluate them. We aim to address these concerns by defining interpretability in the context of machine learning and introducing the predictive, descriptive, relevant (PDR) framework for discussing interpretations. The PDR framework provides 3 overarching desiderata for evaluation: predictive accuracy, descriptive accuracy, and relevancy, with relevancy judged relative to a human audience. Moreover, to help manage the deluge of interpretation methods, we introduce a categorization of existing techniques into model-based and post hoc categories, with subgroups including sparsity, modularity, and simulatability. To demonstrate how practitioners can use the PDR framework to evaluate and understand interpretations, we provide numerous real-world examples. These examples highlight the often underappreciated role played by human audiences in discussions of interpretability. Finally, based on our framework, we discuss limitations of existing methods and directions for future work. We hope that this work will provide a common vocabulary that will make it easier for both practitioners and researchers to discuss and choose from the full range of interpretation methods.},
author = {Murdoch, W. James and Singh, Chandan and Kumbier, Karl and Abbasi-Asl, Reza and Yu, Bin},
doi = {10.1073/pnas.1900654116},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Murdoch et al. - 2019 - Definitions, methods, and applications in interpretable machine learning(2).pdf:pdf},
issn = {10916490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Explainability,Interpretability,Machine learning,Relevancy},
number = {44},
pages = {22071--22080},
pmid = {31619572},
title = {{Definitions, methods, and applications in interpretable machine learning}},
volume = {116},
year = {2019}
}
@article{Bhatt2020,
abstract = {A feature-based model explanation denotes how much each input feature contributes to a model's output for a given data point. As the number of proposed explanation functions grows, we lack quantitative evaluation criteria to help practitioners know when to use which explanation function. This paper proposes quantitative evaluation criteria for feature-based explanations: low sensitivity, high faithfulness, and low complexity. We devise a framework for aggregating explanation functions. We develop a procedure for learning an aggregate explanation function with lower complexity and then derive a new aggregate Shapley value explanation function that minimizes sensitivity.},
archivePrefix = {arXiv},
arxivId = {2005.00631},
author = {Bhatt, Umang and Weller, Adrian and Moura, Jos{\'{e}} M.F.},
doi = {10.24963/ijcai.2020/417},
eprint = {2005.00631},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bhatt, Weller, Moura - 2020 - Evaluating and aggregating feature-based model explanations.pdf:pdf},
isbn = {9780999241165},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
pages = {3016--3022},
title = {{Evaluating and aggregating feature-based model explanations}},
volume = {2021-Janua},
year = {2020}
}
@article{Adebayo2018,
abstract = {Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual assessment can be misleading. Through extensive experiments we show that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. We interpret our findings through an analogy with edge detection in images, a technique that requires neither training data nor model. Theory in the case of a linear model and a single-layer convolutional neural network supports our experimental findings2},
archivePrefix = {arXiv},
arxivId = {1810.03292},
author = {Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
eprint = {1810.03292},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Adebayo et al. - 2018 - Sanity checks for saliency maps.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {NeurIPS},
pages = {9505--9515},
title = {{Sanity checks for saliency maps}},
volume = {2018-Decem},
year = {2018}
}
@article{Nam2020,
abstract = {As Deep Neural Networks (DNNs) have demonstrated superhuman performance in a variety of fields, there is an increasing interest in understanding the complex internal mechanisms of DNNs. In this paper, we propose Relative Attributing Propagation (RAP), which decomposes the output predictions of DNNs with a new perspective of separating the relevant (positive) and irrelevant (negative) attributions according to the relative influence between the layers. The relevance of each neuron is identified with respect to its degree of contribution, separated into positive and negative, while preserving the conservation rule. Considering the relevance assigned to neurons in terms of relative priority, RAP allows each neuron to be assigned with a bi-polar importance score concerning the output: from highly relevant to highly irrelevant. Therefore, our method makes it possible to interpret DNNs with much clearer and attentive visualizations of the separated attributions than the conventional explaining methods. To verify that the attributions propagated by RAP correctly account for each meaning, we utilize the evaluation metrics: (i) Outside-inside relevance ratio, (ii) Segmentation mIOU and (iii) Region perturbation. In all experiments and metrics, we present a sizable gap in comparison to the existing literature.},
archivePrefix = {arXiv},
arxivId = {1904.00605},
author = {Nam, Woo Jeoung and Gur, Shir and Choi, Jaesik and Wolf, Lior and Lee, Seong Whan},
doi = {10.1609/aaai.v34i03.5632},
eprint = {1904.00605},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nam et al. - 2020 - Relative attributing propagation Interpreting the comparative contributions of individual units in deep neural ne(2).pdf:pdf},
isbn = {9781577358350},
issn = {2159-5399},
journal = {AAAI 2020 - 34th AAAI Conference on Artificial Intelligence},
pages = {2501--2508},
title = {{Relative attributing propagation: Interpreting the comparative contributions of individual units in deep neural networks}},
year = {2020}
}
@article{Elenberg2017,
abstract = {In many machine learning applications, it is important to explain the predictions of a black-box classifier. For example, why does a deep neural network assign an image to a particular class? We cast interpretability of black-box classifiers as a combinatorial maximization problem and propose an efficient streaming algorithm to solve it subject to cardinality constraints. By extending ideas from Badanidiyuru et al. [2014], we provide a constant factor approximation guarantee for our algorithm in the case of random stream order and a weakly submodular objective function. This is the first such theoretical guarantee for this general class of functions, and we also show that no such algorithm exists for a worst case stream order. Our algorithm obtains similar explanations of Inception V3 predictions 10 times faster than the state-of-the-art LIME framework of Ribeiro et al. [2016].},
archivePrefix = {arXiv},
arxivId = {1703.02647},
author = {Elenberg, Ethan R. and Dimakis, Alexandros G. and Feldman, Moran and Karbasi, Amin},
eprint = {1703.02647},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Elenberg et al. - 2017 - Streaming weak submodularity Interpreting neural networks on the fly.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {4045--4055},
title = {{Streaming weak submodularity: Interpreting neural networks on the fly}},
volume = {2017-Decem},
year = {2017}
}
@article{Manuscript2020,
abstract = {Cladding austenitic stainless steels are trendy these days in pressure vessels to enhance surface qualities. In this work, austenitic stainless steel clad layers deposited by flux cored arc welding process on structural steel plates used in boiler construction are investigated. To facilitate this, composite clad layers are produced with 316L stainless steel deposits on low carbon structural steel plates based on the design of experiments. Results exhibit an incremental trend of thermal conductivity with respect to percentage of weld dilution. A linear mathematical relationship is established between the percentage of weld dilution and post-weld thermal conductivity of clad layers for the prediction of thermal conductivity of clad layer deposits. These results could be supportive in the fabrication industries for producing energy efficient thermal equipment},
author = {Manuscript, Accepted},
title = {{Ac ce us}},
year = {2020}
}
@article{Du2018a,
abstract = {While deep neural networks (DNN) have become an effective computational tool, the prediction results are often criticized by the lack of interpretability, which is essential in many real-world applications such as health informatics. Existing attempts based on local interpretations aim to identify relevant features contributing the most to the prediction of DNN by monitoring the neighborhood of a given input. They usually simply ignore the intermediate layers of the DNN that might contain rich information for interpretation. To bridge the gap, in this paper, we propose to investigate a guided feature inversion framework for taking advantage of the deep architectures towards effective interpretation. The proposed framework not only determines the contribution of each feature in the input but also provides insights into the decision-making process of DNN models. By further interacting with the neuron of the target category at the output layer of the DNN, we enforce the interpretation result to be class-discriminative. We apply the proposed interpretation model to different CNN architectures to provide explanations for image data and conduct extensive experiments on ImageNet and PASCAL VOC07 datasets. The interpretation results demonstrate the effectiveness of our proposed framework in providing class-discriminative interpretation for DNN-based prediction.},
archivePrefix = {arXiv},
arxivId = {1804.00506},
author = {Du, Mengnan and Liu, Ninghao and Song, Qingquan and Hu, Xia},
doi = {10.1145/3219819.3220099},
eprint = {1804.00506},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Du et al. - 2018 - Towards explanation of DNN-based prediction with guided feature inversion.pdf:pdf},
isbn = {9781450355520},
journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
keywords = {Deep learning,Guided feature inversion,Intermediate layers,Machine learning interpretation},
pages = {1358--1367},
title = {{Towards explanation of DNN-based prediction with guided feature inversion}},
year = {2018}
}
@article{Tjoa2019,
abstract = {Recently, artificial intelligence, especially machine learning has demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning. Along with research progress, machine learning has encroached into many different fields and disciplines. Some of them, such as the medical field, require high level of accountability, and thus transparency, which means we need to be able to explain machine decisions, predictions and justify their reliability. This requires greater interpretability, which often means we need to understand the mechanism underlying the algorithms. Unfortunately, the black-box nature of the deep learning is still unresolved, and many machine decisions are still poorly understood. We provide a review on interpretabilities suggested by different research works and categorize them, with the intention of providing alternative perspective that is hopefully more tractable for future adoption of interpretability standard. We explore further into interpretability in the medical field, illustrating the complexity of interpretability issue.},
archivePrefix = {arXiv},
arxivId = {1907.07374},
author = {Tjoa, Erico and Fellow, Cuntai Guan},
doi = {10.1109/tnnls.2020.3027314},
eprint = {1907.07374},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tjoa, Fellow - 2019 - A Survey on Explainable Artificial Intelligence (XAI) Towards Medical XAI.pdf:pdf},
issn = {23318422},
journal = {arXiv},
pages = {1--21},
title = {{A Survey on Explainable Artificial Intelligence (XAI): Towards Medical XAI}},
year = {2019}
}
@article{Zhou2018,
abstract = {Weakly supervised instance segmentation with image-level labels, instead of expensive pixel-level masks, remains unexplored. In this paper, we tackle this challenging problem by exploiting class peak responses to enable a classification network for instance mask extraction. With image labels supervision only, CNN classifiers in a fully convolutional manner can produce class response maps, which specify classification confidence at each image location. We observed that local maximums, i.e., peaks, in a class response map typically correspond to strong visual cues residing inside each instance. Motivated by this, we first design a process to stimulate peaks to emerge from a class response map. The emerged peaks are then back-propagated and effectively mapped to highly informative regions of each object instance, such as instance boundaries. We refer to the above maps generated from class peak responses as Peak Response Maps (PRMs). PRMs provide a fine-detailed instance-level representation, which allows instance masks to be extracted even with some off-the-shelf methods. To the best of our knowledge, we for the first time report results for the challenging image-level supervised instance segmentation task. Extensive experiments show that our method also boosts weakly supervised pointwise localization as well as semantic segmentation performance, and reports state-of-the-art results on popular benchmarks, including PASCAL VOC 2012 and MS COCO.},
archivePrefix = {arXiv},
arxivId = {1804.00880},
author = {Zhou, Yanzhao and Zhu, Yi and Ye, Qixiang and Qiu, Qiang and Jiao, Jianbin},
doi = {10.1109/CVPR.2018.00399},
eprint = {1804.00880},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhou et al. - 2018 - Weakly Supervised Instance Segmentation Using Class Peak Response.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {3791--3800},
title = {{Weakly Supervised Instance Segmentation Using Class Peak Response}},
year = {2018}
}
@article{YingkaiGao2018,
abstract = {The identification of drug-target interactions (DTIs) is a key task in drug discovery, where drugs are chemical compounds and targets are proteins. Traditional DTI prediction methods are either time consuming (simulation-based methods) or heavily dependent on domain expertise (similarity-based and feature-based methods). In this work, we propose an end-to-end neural network model that predicts DTIs directly from low level representations. In addition to making predictions, our model provides biological interpretation using two-way attention mechanism. Instead of using simplified settings where a dataset is evaluated as a whole, we designed an evaluation dataset from BindingDB following more realistic settings where predictions of unobserved examples (proteins and drugs) have to be made. We experimentally compared our model with matrix factorization, similarity-based methods, and a previous deep learning approach. Overall, the results show that our model outperforms other approaches without requiring domain knowledge and feature engineering. In a case study, we illustrated the ability of our approach to provide biological insights to interpret the predictions.},
author = {{Yingkai Gao}, Kyle and Fokoue, Achille and Luo, Heng and Iyengar, Arun and Dey, Sanjoy and Zhang, Ping},
doi = {10.24963/ijcai.2018/468},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yingkai Gao et al. - 2018 - Interpretable drug target prediction using deep neural representation.pdf:pdf},
isbn = {9780999241127},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Machine Learning Applications: Applications of Sup,Machine Learning Applications: Bio,Machine Learning: Deep Learning,Machine Learning: Experimental Methodology,Machine Learning: Neural Networks,Medicine,Multidisciplinary Topics and Applications: Biology,Replicability},
pages = {3371--3377},
title = {{Interpretable drug target prediction using deep neural representation}},
volume = {2018-July},
year = {2018}
}
@article{Boz2002,
abstract = {Neural Networks are successful in acquiring hidden knowledge in datasets. Their biggest weakness is that the knowledge they acquire is represented in a form not understandable to humans. Researchers tried to address this problem by extracting rules from trained Neural Networks. Most of the proposed rule extraction methods required specialized type of Neural Networks; some required binary inputs and some were computationally expensive. Craven proposed extracting MofN type Decision Trees from Neural Networks. We believe MofN type Decision Trees are only good for MofN type problems and trees created for regular high dimensional real world problems may be very complex. In this paper, we introduced a new method for extracting regular C4.5 like Decision Trees from trained Neural Networks. We showed that the new method (DecText) is effective in extracting high fidelity trees from trained networks. We also introduced a new discretization technique to make DecText be able to handle continuous features and a new pruning technique for finding simplest tree with the highest fidelity.},
author = {Boz, Olcay},
doi = {10.1145/775107.775113},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Boz - 2002 - Extracting decision trees from trained neural networks.pdf:pdf},
isbn = {158113567X},
journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {456--461},
title = {{Extracting decision trees from trained neural networks}},
year = {2002}
}
@article{Baehrens2010,
abstract = {After building a classifier with modern tools of machine learning we typically have a black box at hand that is able to predict well for unseen data. Thus, we get an answer to the question what is the most likely label of a given unseen data point. However, most methods will provide no answer why the model predicted a particular label for a single instance and what features were most influential for that particular instance. The only method that is currently able to provide such explanations are decision trees. This paper proposes a procedure which (based on a set of assumptions) allows to explain the decisions of any classification method. {\textcopyright} 2010 David Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe, Katja Hansen and Klaus-Robert M{\"{u}}ller.},
archivePrefix = {arXiv},
arxivId = {0912.1128},
author = {Baehrens, David and Schroeter, Timon and Harmeling, Stefan and Kawanabe, Motoaki and Hansen, Katja and M{\"{u}}ller, Klaus Robert},
eprint = {0912.1128},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baehrens et al. - 2010 - How to explain individual classification decisions.pdf:pdf},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {Ames mutagenicity,Black box model,Explaining,Kernel methods,Nonlinear},
pages = {1803--1831},
title = {{How to explain individual classification decisions}},
volume = {11},
year = {2010}
}
@article{Laugel2018b,
abstract = {In the context of post-hoc interpretability, this paper addresses the task of explaining the prediction of a classifier, considering the case where no information is available, neither on the classifier itself, nor on the processed data (neither the training nor the test data). It proposes an inverse classification approach whose principle consists in determining the minimal changes needed to alter a prediction: in an instance-based framework, given a data point whose classification must be explained, the proposed method consists in identifying a close neighbor classified differently, where the closeness definition integrates a sparsity constraint. This principle is implemented using observation generation in the Growing Spheres algorithm. Experimental results on two datasets illustrate the relevance of the proposed approach that can be used to gain knowledge about the classifier.},
author = {Laugel, Thibault and Lesot, Marie Jeanne and Marsala, Christophe and Renard, Xavier and Detyniecki, Marcin},
doi = {10.1007/978-3-319-91473-2_9},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Laugel et al. - 2018 - Comparison-based inverse classification for interpretability in machine learning(4).pdf:pdf},
isbn = {9783319914725},
issn = {18650929},
journal = {Communications in Computer and Information Science},
keywords = {Comparison-based,Inverse classification,Local explanation,Post-hoc interpretability},
pages = {100--111},
title = {{Comparison-based inverse classification for interpretability in machine learning}},
volume = {853},
year = {2018}
}
@article{Bahdanau2015a,
abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder–decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
archivePrefix = {arXiv},
arxivId = {1409.0473},
author = {Bahdanau, Dzmitry and Cho, Kyung Hyun and Bengio, Yoshua},
eprint = {1409.0473},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bahdanau, Cho, Bengio - 2015 - Neural machine translation by jointly learning to align and translate.pdf:pdf},
journal = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
pages = {1--15},
title = {{Neural machine translation by jointly learning to align and translate}},
year = {2015}
}
@article{Saini2018,
abstract = {Despite their increasing popularity and success in a variety of supervised learning problems, deep neural networks are extremely hard to interpret and debug: Given and already trained Deep Neural Net, and a set of test inputs, how can we gain insight into how those inputs interact with different layers of the neural network? Furthermore, can we characterize a given deep neural network based on it's observed behavior on different inputs? In this paper we propose a novel factorization based approach on understanding how different deep neural networks operate. In our preliminary results, we identify fascinating patterns that link the factorization rank (typically used as a measure of interestingness in unsupervised data analysis) with how well or poorly the deep network has been trained. Finally, our proposed approach can help provide visual insights on how high-level. interpretable patterns of the network's input behave inside the hidden layers of the deep network.},
archivePrefix = {arXiv},
arxivId = {1806.02012},
author = {Saini, Uday Singh and Papalexakis, Evangelos E.},
eprint = {1806.02012},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Saini, Papalexakis - 2018 - A Peek Into the Hidden Layers of a Convolutional Neural Network Through a Factorization Lens.pdf:pdf},
isbn = {1234567245},
title = {{A Peek Into the Hidden Layers of a Convolutional Neural Network Through a Factorization Lens}},
url = {http://arxiv.org/abs/1806.02012},
year = {2018}
}
@article{Xu2020,
abstract = {We study the attribution problem [28] for deep networks applied to perception tasks. For vision tasks, attribution techniques attribute the prediction of a network to the pixels of the input image. We propose a new technique called Blur Integrated Gradients (BlurIG). This technique has several advantages over other methods. First, it can tell at what scale a network recognizes an object. It produces scores in the scale/frequency dimension, that we find captures interesting phenomena. Second, it satisfies the scale-space axioms [14], which imply that it employs perturbations that are free of artifact. We therefore produce explanations that are cleaner and consistent with the operation of deep networks. Third, it eliminates the need for a 'baseline' parameter for Integrated Gradients [31] for perception tasks. This is desirable because the choice of baseline has a significant effect on the explanations. We compare the proposed technique against previous techniques and demonstrate application on three tasks: ImageNet object recognition, Diabetic Retinopathy prediction, and AudioSet audio event identification. Code and examples are on github1},
archivePrefix = {arXiv},
arxivId = {2004.03383},
author = {Xu, Shawn and Venugopalan, Subhashini and Sundararajan, Mukund},
doi = {10.1109/CVPR42600.2020.00970},
eprint = {2004.03383},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu, Venugopalan, Sundararajan - 2020 - Attribution in scale and space(2).pdf:pdf},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {9677--9686},
title = {{Attribution in scale and space}},
year = {2020}
}
@article{Yeom2021,
abstract = {The success of convolutional neural networks (CNNs) in various applications is accompanied by a significant increase in computation and parameter storage costs. Recent efforts to reduce these overheads involve pruning and compressing the weights of various layers while at the same time aiming to not sacrifice performance. In this paper, we propose a novel criterion for CNN pruning inspired by neural network interpretability: The most relevant units, i.e. weights or filters, are automatically found using their relevance scores obtained from concepts of explainable AI (XAI). By exploring this idea, we connect the lines of interpretability and model compression research. We show that our proposed method can efficiently prune CNN models in transfer-learning setups in which networks pre-trained on large corpora are adapted to specialized tasks. The method is evaluated on a broad range of computer vision datasets. Notably, our novel criterion is not only competitive or better compared to state-of-the-art pruning criteria when successive retraining is performed, but clearly outperforms these previous criteria in the resource-constrained application scenario in which the data of the task to be transferred to is very scarce and one chooses to refrain from fine-tuning. Our method is able to compress the model iteratively while maintaining or even improving accuracy. At the same time, it has a computational cost in the order of gradient computation and is comparatively simple to apply without the need for tuning hyperparameters for pruning.},
archivePrefix = {arXiv},
arxivId = {1912.08881},
author = {Yeom, Seul Ki and Seegerer, Philipp and Lapuschkin, Sebastian and Binder, Alexander and Wiedemann, Simon and M{\"{u}}ller, Klaus Robert and Samek, Wojciech},
doi = {10.1016/j.patcog.2021.107899},
eprint = {1912.08881},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yeom et al. - 2021 - Pruning by explaining A novel criterion for deep neural network pruning(2).pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Convolutional neural network (CNN),Explainable AI (XAI),Interpretation of models,Layer-wise relevance propagation (LRP),Pruning},
title = {{Pruning by explaining: A novel criterion for deep neural network pruning}},
volume = {115},
year = {2021}
}
@article{Manuscript2020,
abstract = {Cladding austenitic stainless steels are trendy these days in pressure vessels to enhance surface qualities. In this work, austenitic stainless steel clad layers deposited by flux cored arc welding process on structural steel plates used in boiler construction are investigated. To facilitate this, composite clad layers are produced with 316L stainless steel deposits on low carbon structural steel plates based on the design of experiments. Results exhibit an incremental trend of thermal conductivity with respect to percentage of weld dilution. A linear mathematical relationship is established between the percentage of weld dilution and post-weld thermal conductivity of clad layers for the prediction of thermal conductivity of clad layer deposits. These results could be supportive in the fabrication industries for producing energy efficient thermal equipment},
author = {Manuscript, Accepted},
title = {{Ac ce us}},
year = {2020}
}
@article{Zee2019,
abstract = {The purpose of this work is to determine if the ability to interpret a convolutional neural network (CNN) architecture can enhance human performance, pertaining to face recognition. We are interested in distinguishing between the faces of two similar-looking actresses of Indian origin, who have only a few discriminating features. This recognition task proved challenging for humans who were not previously familiar with the actresses (novices) as they performed only just better than random. When asked to perform the same task, humans who were more familiar with the actresses (experts) performed significantly better. We attempted the same task with a Siamese CNN which performed as well as the experts. We therefore became interested in applying any new knowledge obtained from the CNN to aid in improving the distinguishing abilities of other novices. This was accomplished by generating activation maps from the CNN. The maps showed what parts of the input face images created the highest activations in the last convolutional layer of the network. Using 'fooling'' techniques, we also investigated what spatial locations on the face were most responsible for confusing one actress for the other. Empirically, the cheekbones and foreheads were determined to be the strongest differentiating features between the actresses. By providing this information verbally to a new set of novices, we successfully raised the human recognition rates by 11%. For this work, we therefore successfully increased human understanding pertaining to facial recognition via post-hoc interpretability of a CNN.},
author = {Zee, Timothy and Gali, Geeta and Nwogu, Ifeoma},
doi = {10.1109/ICCVW.2019.00064},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zee, Gali, Nwogu - 2019 - Enhancing human face recognition with an interpretable neural network.pdf:pdf},
isbn = {9781728150239},
journal = {Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019},
keywords = {Adversarial examples,CNN,Face recognition,Interpretability,Siamese networks},
pages = {514--522},
title = {{Enhancing human face recognition with an interpretable neural network}},
year = {2019}
}
@article{Joshi2021,
abstract = {Artificial Intelligence techniques powered by deep neural nets have achieved much success in several application domains, most significantly and notably in the Computer Vision applications and Natural Language Processing tasks. Surpassing human-level performance propelled the research in the applications where different modalities amongst language, vision, sensory, text play an important role in accurate predictions and identification. Several multimodal fusion methods employing deep learning models are proposed in the literature. Despite their outstanding performance, the complex, opaque and black-box nature of the deep neural nets limits their social acceptance and usability. This has given rise to the quest for model interpretability and explainability, more so in the complex tasks involving multimodal AI methods. This paper extensively reviews the present literature to present a comprehensive survey and commentary on the explainability in multimodal deep neural nets, especially for the vision and language tasks. Several topics on multimodal AI and its applications for generic domains have been covered in this paper, including the significance, datasets, fundamental building blocks of the methods and techniques, challenges, applications, and future trends in this domain.},
author = {Joshi, Gargi and Walambe, Rahee and Kotecha, Ketan},
doi = {10.1109/ACCESS.2021.3070212},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Joshi, Walambe, Kotecha - 2021 - A Review on Explainability in Multimodal Deep Neural Nets.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Artificial intelligence,Biomedical imaging,Data models,Deep learning,Neural networks,Task analysis,Visualization,XAI,deep multimodal learning,explainable AI,interpretability,survey,trends,vision and language research},
title = {{A Review on Explainability in Multimodal Deep Neural Nets}},
volume = {XX},
year = {2021}
}
@article{Selvaraju2016,
abstract = {We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing input regions that are 'important' for predictions -- or visual explanations. Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses class-specific gradient information to localize important regions. These localizations are combined with existing pixel-space visualizations to create a novel high-resolution and class-discriminative visualization called Guided Grad-CAM. These methods help better understand CNN-based models, including image captioning and visual question answering (VQA) models. We evaluate our visual explanations by measuring their ability to discriminate between classes, to inspire trust in humans, and their correlation with occlusion maps. Grad-CAM provides a new way to understand CNN-based models. We have released code, an online demo hosted on CloudCV, and a full version of this extended abstract.},
archivePrefix = {arXiv},
arxivId = {1611.07450},
author = {Selvaraju, Ramprasaath R and Das, Abhishek and Vedantam, Ramakrishna and Cogswell, Michael and Parikh, Devi and Batra, Dhruv},
eprint = {1611.07450},
pages = {1--4},
title = {{Grad-CAM: Why did you say that?}},
url = {http://arxiv.org/abs/1611.07450},
year = {2016}
}
@article{Adadi2018,
abstract = {At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.},
author = {Adadi, Amina and Berrada, Mohammed},
doi = {10.1109/ACCESS.2018.2870052},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Adadi, Berrada - 2018 - Peeking Inside the Black-Box A Survey on Explainable Artificial Intelligence (XAI).pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Explainable artificial intelligence,black-box models,interpretable machine learning},
pages = {52138--52160},
publisher = {IEEE},
title = {{Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)}},
volume = {6},
year = {2018}
}
@article{Frosst2018,
abstract = {Deep neural networks have proved to be a very e ective way to perform classification tasks. They excel when the input data is high dimensional, the relationship between the input and the output is complicated, and the number of labeled training examples is large [Szegedy et al., 2015, Wu et al., 2016, Jozefowicz et al., 2016, Graves et al., 2013]. But it is hard to explain why a learned network makes a particular classification decision on a particular test case. This is due to their reliance on distributed hierarchical representations. If we could take the knowledge acquired by the neural net and express the same knowledge in a model that relies on hierarchical decisions instead, explaining a particular decision would be much easier. We describe a way of using a trained neural net to create a type of soft decision tree that generalizes better than one learned directly from the training data.},
archivePrefix = {arXiv},
arxivId = {1711.09784},
author = {Frosst, Nicholas and rey Hinton, Geo},
eprint = {1711.09784},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Frosst, Hinton - 2018 - Distilling a neural network into a soft decision tree.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
title = {{Distilling a neural network into a soft decision tree}},
volume = {2071},
year = {2018}
}
@article{Samek2021,
abstract = {With the broader and highly successful usage of machine learning (ML) in industry and the sciences, there has been a growing demand for explainable artificial intelligence (XAI). Interpretability and explanation methods for gaining a better understanding of the problem-solving abilities and strategies of nonlinear ML, in particular, deep neural networks, are, therefore, receiving increased attention. In this work, we aim to: 1) provide a timely overview of this active emerging field, with a focus on 'post hoc' explanations, and explain its theoretical foundations; 2) put interpretability algorithms to a test both from a theory and comparative evaluation perspective using extensive simulations; 3) outline best practice aspects, i.e., how to best include interpretation methods into the standard usage of ML; and 4) demonstrate successful usage of XAI in a representative selection of application scenarios. Finally, we discuss challenges and possible future directions of this exciting foundational field of ML.},
archivePrefix = {arXiv},
arxivId = {2003.07631},
author = {Samek, Wojciech and Montavon, Gr{\'{e}}goire and Lapuschkin, Sebastian and Anders, Christopher J. and M{\"{u}}ller, Klaus Robert},
doi = {10.1109/JPROC.2021.3060483},
eprint = {2003.07631},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Samek et al. - 2021 - Toward Interpretable Machine Learning Transparent Deep Neural Networks and Beyond.pdf:pdf},
issn = {15582256},
journal = {Proceedings of the IEEE},
keywords = {Black-box models,Interpretability,deep learning,explainable artificial intelligence (XAI),model transparency,neural networks},
number = {3},
pages = {247--278},
title = {{Toward Interpretable Machine Learning Transparent Deep Neural Networks and Beyond}},
volume = {109},
year = {2021}
}
@article{Lin2019,
abstract = {Affective computing is an emerging research area which provides insights on human's mental state through human-machine interaction. During the interaction process, bio-signal analysis is essential to detect human affective changes. Currently, machine learning methods to analyse bio-signals are the state of the art to detect the affective states, but most empirical works mainly deploy traditional machine learning methods rather than deep learning models due to the need for explainability. In this paper, we propose a deep learning model to process multimodal-multisensory bio-signals for affect recognition. It supports batch training for different sampling rate signals at the same time, and our results show significant improvement compared to the state of the art. Furthermore, the results are interpreted at the sensor- and signal- level to improve the explainaibility of our deep learning model.},
author = {Lin, Jionghao and Pan, Shirui and Lee, Cheng Siong and Oviatt, Sharon},
doi = {10.1145/3357384.3358160},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lin et al. - 2019 - An explainable deep fusion network for affect recognition using physiological signals.pdf:pdf},
isbn = {9781450369763},
journal = {International Conference on Information and Knowledge Management, Proceedings},
keywords = {Affect recognition,Deep learning,Explainability,Multimodal fusion},
number = {1},
pages = {2069--2072},
title = {{An explainable deep fusion network for affect recognition using physiological signals}},
year = {2019}
}
@article{Omeiza2019,
abstract = {Gaining insight into how deep convolutional neural network models perform image classification and how to explain their outputs have been a concern to computer vision researchers and decision makers. These deep models are often referred to as black box due to low comprehension of their internal workings. As an effort to developing explainable deep learning models, several methods have been proposed such as finding gradients of class output with respect to input image (sensitivity maps), class activation map (CAM), and Gradient based Class Activation Maps (Grad-CAM). These methods under perform when localizing multiple occurrences of the same class and do not work for all CNNs. In addition, Grad-CAM does not capture the entire object in completeness when used on single object images, this affect performance on recognition tasks. With the intention to create an enhanced visual explanation in terms of visual sharpness, object localization and explaining multiple occurrences of objects in a single image, we present Smooth Grad-CAM++ 1, a technique that combines methods from two other recent techniques-SMOOTHGRAD and Grad-CAM++. Our Smooth Grad-CAM++ technique provides the capability of either visualizing a layer, subset of feature maps, or subset of neurons within a feature map at each instance at the inference level (model prediction process). After experimenting with few images, Smooth Grad-CAM++ produced more visually sharp maps with better localization of objects in the given input images when compared with other methods.},
archivePrefix = {arXiv},
arxivId = {1908.01224},
author = {Omeiza, Daniel and Speakman, Skyler and Cintas, Celia and Weldemariam, Komminist},
eprint = {1908.01224},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Omeiza et al. - 2019 - Smooth Grad-CAM An enhanced inference level visualization technique for deep convolutional neural network models.pdf:pdf},
issn = {23318422},
journal = {arXiv},
keywords = {Class Activation Maps,Computer Vision,Convolutional Neural Network},
pages = {1--10},
title = {{Smooth Grad-CAM++: An enhanced inference level visualization technique for deep convolutional neural network models}},
year = {2019}
}
@article{Nam2020a,
abstract = {The clear transparency of Deep Neural Networks (DNNs) is hampered by complex internal structures and nonlinear transformations along deep hierarchies. In this paper, we propose a new attribution method, Relative Sectional Propagation (RSP), for fully decomposing the output predictions with the characteristics of class-discriminative attributions and clear objectness. We carefully revisit some shortcomings of backpropagation-based attribution methods, which are trade-off relations in decomposing DNNs. We define hostile factor as an element that interferes with finding the attributions of the target and propagate it in a distinguishable way to overcome the non-suppressed nature of activated neurons. As a result, it is possible to assign the bi-polar relevance scores of the target (positive) and hostile (negative) attributions while maintaining each attribution aligned with the importance. We also present the purging techniques to prevent the decrement of the gap between the relevance scores of the target and hostile attributions during backward propagation by eliminating the conflicting units to channel attribution map. Therefore, our method makes it possible to decompose the predictions of DNNs with clearer class-discriminativeness and detailed elucidations of activation neurons compared to the conventional attribution methods. In a verified experimental environment, we report the results of the assessments: (i) Pointing Game, (ii) mIoU, and (iii) Model Sensitivity with PASCAL VOC 2007, MS COCO 2014, and ImageNet datasets. The results demonstrate that our method outperforms existing backward decomposition methods, including distinctive and intuitive visualizations.},
archivePrefix = {arXiv},
arxivId = {2012.03434},
author = {Nam, Woo-Jeoung and Choi, Jaesik and Lee, Seong-Whan},
eprint = {2012.03434},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nam, Choi, Lee - 2020 - Interpreting Deep Neural Networks with Relative Sectional Propagation by Analyzing Comparative Gradients and Hos.pdf:pdf},
title = {{Interpreting Deep Neural Networks with Relative Sectional Propagation by Analyzing Comparative Gradients and Hostile Activations}},
url = {http://arxiv.org/abs/2012.03434},
year = {2020}
}
@article{Xie2020,
abstract = {The deep neural network (DNN) is an indispensable machine learning tool for achieving human-level performance on many learning tasks. Yet, due to its black-box nature, it is inherently difficult to understand which aspects of the input data drive the decisions of the network. There are various real-world scenarios in which humans need to make actionable decisions based on the output of a decision support system that makes use of DNNs. These decision support systems can be found in critical domains, such as legislation, law enforcement, and healthcare. It is important that the humans making high-level decisions can be sure that the DNN decisions are driven by combinations of data features that are appropriate in the context of the deployment of the decision support system and that the decisions made are legally or ethically defensible. Due to the incredible pace at which DNN technology is being developed and adopted, the development of new methods and studies on explaining the decision-making process of DNNs has blossomed into an active research field. A practitioner beginning to study explainable deep learning may be intimidated by the plethora of orthogonal directions the field is taking. This complexity is further exacerbated by the general confusion that exists in defining what it means to be able to explain the actions of a deep learning system and to evaluate a system's “ability to explain”. To alleviate this problem, this article offers a “field guide” to deep learning explainability for those uninitiated in the field. The field guide: i) Discusses the traits of a deep learning system that researchers enhance in explainability research, ii) places explainability in the context of other related deep learning research areas, and iii) introduces three simple dimensions defining the space of foundational methods that contribute to explainable deep learning. The guide is designed as an easy-to-digest starting point for those just embarking in the field.},
archivePrefix = {arXiv},
arxivId = {2004.14545},
author = {Xie, Ning and Ras, Gabri{\"{e}}lle and van Gerven, Marcel and Doran, Derek},
eprint = {2004.14545},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xie et al. - 2020 - Explainable deep learning A field guide for the uninitiated.pdf:pdf},
issn = {23318422},
journal = {arXiv},
title = {{Explainable deep learning: A field guide for the uninitiated}},
year = {2020}
}
@article{Ying2019,
abstract = {Graph Neural Networks (GNNs) are a powerful tool for machine learning on graphs. GNNs combine node feature information with the graph structure by recursively passing neural messages along edges of the input graph. However, incorporating both graph structure and feature information leads to complex models and explaining predictions made by GNNs remains unsolved. Here we propose GNNEXPLAINER, the first general, model-agnostic approach for providing interpretable explanations for predictions of any GNN-based model on any graph-based machine learning task. Given an instance, GNNEXPLAINER identifies a compact subgraph structure and a small subset of node features that have a crucial role in GNN's prediction. Further, GNNEXPLAINER can generate consistent and concise explanations for an entire class of instances. We formulate GNNEXPLAINER as an optimization task that maximizes the mutual information between a GNN's prediction and distribution of possible subgraph structures. Experiments on synthetic and real-world graphs show that our approach can identify important graph structures as well as node features, and outperforms alternative baseline approaches by up to 43.0% in explanation accuracy. GNNEXPLAINER provides a variety of benefits, from the ability to visualize semantically relevant structures to interpretability, to giving insights into errors of faulty GNNs.},
archivePrefix = {arXiv},
arxivId = {1903.03894},
author = {Ying, Rex and Bourgeois, Dylan and You, Jiaxuan and Zitnik, Marinka and Leskovec, Jure},
eprint = {1903.03894},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ying et al. - 2019 - GNNExplainer Generating explanations for graph neural networks.pdf:pdf},
issn = {23318422},
journal = {arXiv},
number = {iii},
title = {{GNNExplainer: Generating explanations for graph neural networks}},
year = {2019}
}
@article{Selvaraju2016,
abstract = {We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing input regions that are 'important' for predictions -- or visual explanations. Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses class-specific gradient information to localize important regions. These localizations are combined with existing pixel-space visualizations to create a novel high-resolution and class-discriminative visualization called Guided Grad-CAM. These methods help better understand CNN-based models, including image captioning and visual question answering (VQA) models. We evaluate our visual explanations by measuring their ability to discriminate between classes, to inspire trust in humans, and their correlation with occlusion maps. Grad-CAM provides a new way to understand CNN-based models. We have released code, an online demo hosted on CloudCV, and a full version of this extended abstract.},
archivePrefix = {arXiv},
arxivId = {1611.07450},
author = {Selvaraju, Ramprasaath R and Das, Abhishek and Vedantam, Ramakrishna and Cogswell, Michael and Parikh, Devi and Batra, Dhruv},
eprint = {1611.07450},
pages = {1--4},
title = {{Grad-CAM: Why did you say that?}},
url = {http://arxiv.org/abs/1611.07450},
year = {2016}
}
@article{Fong2018,
abstract = {In an effort to understand the meaning of the intermediate representations captured by deep networks, recent papers have tried to associate specific semantic concepts to individual neural network filter responses, where interesting correlations are often found, largely by focusing on extremal filter responses. In this paper, we show that this approach can favor easy-to-interpret cases that are not necessarily representative of the average behavior of a representation. A more realistic but harder-to-study hypothesis is that semantic representations are distributed, and thus filters must be studied in conjunction. In order to investigate this idea while enabling systematic visualization and quantification of multiple filter responses, we introduce the Net2Vec framework, in which semantic concepts are mapped to vectorial embeddings based on corresponding filter responses. By studying such embeddings, we are able to show that 1., in most cases, multiple filters are required to code for a concept, that 2., often filters are not concept specific and help encode multiple concepts, and that 3., compared to single filter activations, filter embeddings are able to better characterize the meaning of a representation and its relationship to other concepts.},
archivePrefix = {arXiv},
arxivId = {1801.03454},
author = {Fong, Ruth and Vedaldi, Andrea},
doi = {10.1109/CVPR.2018.00910},
eprint = {1801.03454},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fong, Vedaldi - 2018 - Net2Vec Quantifying and Explaining How Concepts are Encoded by Filters in Deep Neural Networks.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {8730--8738},
publisher = {IEEE},
title = {{Net2Vec: Quantifying and Explaining How Concepts are Encoded by Filters in Deep Neural Networks}},
year = {2018}
}
@misc{ImperialCollegeLondon1992,
author = {{Imperial College London}},
doi = {10.1080/0260293920170307},
file = {:home/anna/Desktop/Professional Development Programme _ Study _ Imper.html:html},
issn = {0260-2938},
title = {{Professional Development Programme}},
url = {https://www.imperial.ac.uk/study/pg/graduate-school/students/doctoral/professional-development/}
}
@article{Manuscript2020,
abstract = {Cladding austenitic stainless steels are trendy these days in pressure vessels to enhance surface qualities. In this work, austenitic stainless steel clad layers deposited by flux cored arc welding process on structural steel plates used in boiler construction are investigated. To facilitate this, composite clad layers are produced with 316L stainless steel deposits on low carbon structural steel plates based on the design of experiments. Results exhibit an incremental trend of thermal conductivity with respect to percentage of weld dilution. A linear mathematical relationship is established between the percentage of weld dilution and post-weld thermal conductivity of clad layers for the prediction of thermal conductivity of clad layer deposits. These results could be supportive in the fabrication industries for producing energy efficient thermal equipment},
author = {Manuscript, Accepted},
title = {{Ac ce us}},
year = {2020}
}
@article{Dandl2020,
abstract = {Counterfactual explanations are one of the most popular methods to make predictions of black box machine learning models interpretable by providing explanations in the form of ‘what-if scenarios'. Most current approaches optimize a collapsed, weighted sum of multiple objectives, which are naturally difficult to balance a-priori. We propose the Multi-Objective Counterfactuals (MOC) method, which translates the counterfactual search into a multi-objective optimization problem. Our approach not only returns a diverse set of counterfactuals with different trade-offs between the proposed objectives, but also maintains diversity in feature space. This enables a more detailed post-hoc analysis to facilitate better understanding and also more options for actionable user responses to change the predicted outcome. Our approach is also model-agnostic and works for numerical and categorical input features. We show the usefulness of MOC in concrete cases and compare our approach with state-of-the-art methods for counterfactual explanations.},
archivePrefix = {arXiv},
arxivId = {2004.11165},
author = {Dandl, Susanne and Molnar, Christoph and Binder, Martin and Bischl, Bernd},
doi = {10.1007/978-3-030-58112-1_31},
eprint = {2004.11165},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dandl et al. - 2020 - Multi-objective counterfactual explanations(2).pdf:pdf},
isbn = {9783030581114},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Counterfactual explanations,Interpretability,Interpretable machine learning,Multi-objective optimization,NSGA-II},
number = {01},
pages = {448--469},
title = {{Multi-objective counterfactual explanations}},
volume = {12269 LNCS},
year = {2020}
}
@article{Craven1996,
abstract = {A significant limitation of neural networks is that the representations they learn are usually incomprehensible to humans. We present a novel algorithm, Trepan, for extracting comprehensible, symbolic representations from trained neural networks. Our algorithm uses queries to induce a decision tree that approximates the concept represented by a given network. Our experiments demonstrate that Trepan is able to produce decision trees that maintain a high level of fidelity to their respective...},
author = {Craven, Mark W and Shavlik, Jude W},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Craven, Shavlik - 1996 - Extracting tree-structured representations of trained neural networks.pdf:pdf},
isbn = {9780262201070},
journal = {Advances in Neural Information Processing Systems},
pages = {24--30},
title = {{Extracting tree-structured representations of trained neural networks}},
url = {citeseer.ist.psu.edu/craven96extracting.html},
volume = {8},
year = {1996}
}
@article{Vilone2020,
abstract = {Explainable Artificial Intelligence (XAI) has experienced a significant growth over the last few years. This is due to the widespread application of machine learning, particularly deep learning, that has led to the development of highly accurate models but lack explainability and interpretability. A plethora of methods to tackle this problem have been proposed, developed and tested. This systematic review contributes to the body of knowledge by clustering these methods with a hierarchical classification system with four main clusters: review articles, theories and notions, methods and their evaluation. It also summarises the state-of-the-art in XAI and recommends future research directions.},
archivePrefix = {arXiv},
arxivId = {2006.00093},
author = {Vilone, Giulia and Longo, Luca},
eprint = {2006.00093},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vilone, Longo - 2020 - Explainable Artificial Intelligence a Systematic Review.pdf:pdf},
issn = {23318422},
journal = {arXiv},
keywords = {Explainable artificial intelligence,Method classification,Survey,Systematic literature review},
number = {Dl},
title = {{Explainable Artificial Intelligence: a Systematic Review}},
year = {2020}
}
@article{Wieczorek2020,
abstract = {Combining the information bottleneck model with deep learning by replacing mutual information terms with deep neural nets has proven successful in areas ranging from generative modelling to interpreting deep neural networks. In this paper, we revisit the deep variational information bottleneck and the assumptions needed for its derivation. The two assumed properties of the data, X and Y, and their latent representation T, take the form of two Markov chains T-X-Y and X-T-Y. Requiring both to hold during the optimisation process can be limiting for the set of potential joint distributions P(X, Y, T). We, therefore, show how to circumvent this limitation by optimising a lower bound for the mutual information between T and Y: I(T;Y), for which only the latter Markov chain has to be satisfied. The mutual information I(T;Y) can be split into two non-negative parts. The first part is the lower bound for I(T;Y), which is optimised in deep variational information bottleneck (DVIB) and cognate models in practice. The second part consists of two terms that measure how much the former requirement T-X-Y is violated. Finally, we propose interpreting the family of information bottleneck models as directed graphical models, and show that in this framework, the original and deep information bottlenecks are special cases of a fundamental IB model.},
archivePrefix = {arXiv},
arxivId = {1912.13480},
author = {Wieczorek, Aleksander and Roth, Volker},
doi = {10.3390/e22020131},
eprint = {1912.13480},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wieczorek, Roth - 2020 - On the difference between the information bottleneck and the deep information bottleneck.pdf:pdf},
issn = {10994300},
journal = {Entropy},
keywords = {Conditional independence,Deep variational information bottleneck,Information bottleneck,Markov assumption,Markov chain,Mutual information},
number = {2},
title = {{On the difference between the information bottleneck and the deep information bottleneck}},
volume = {22},
year = {2020}
}
@article{Arik2020,
abstract = {We propose a novel inherently interpretable machine learning method that bases decisions on few relevant examples that we call prototypes. Our method, ProtoAttend, can be integrated into a wide range of neural network architectures including pre-trained models. It utilizes an attention mechanism that relates the encoded representations to samples in order to determine prototypes. Protoattend yields superior results in three high impact problems without sacrificing accuracy of the original model: (1) it enables high-quality interpretability that outputs samples most relevant to the decision-making (i.e. a samplebased interpretability method); (2) it achieves state of the art confidence estimation by quantifying the mismatch across prototype labels; and (3) it obtains state of the art in distribution mismatch detection. All these can be achieved with minimal additional test time and a practically viable training time computational cost.},
archivePrefix = {arXiv},
arxivId = {1902.06292},
author = {Arik, Sercan O. and Pfister, Tomas},
eprint = {1902.06292},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Arik, Pfister - 2020 - Protoattend Attention-based prototypical learning(2).pdf:pdf},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Attention,Confidence,Explainable deep learning,Prototypical,Sample-based interpretability},
number = {2},
pages = {1--23},
title = {{Protoattend: Attention-based prototypical learning}},
volume = {21},
year = {2020}
}
@article{Cai2019,
author = {Cai, Han and Zhu, Ligeng and Han, Song},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cai, Zhu, Han - 2019 - Proxyless Nas Direct Neural Architecture Search on Target Task and Hardware.pdf:pdf},
journal = {International Conference on Learning Representations ICRL 2019},
pages = {1--13},
title = {{Proxyless Nas: Direct Neural Architecture Search on Target Task and Hardware}},
year = {2019}
}
@article{Arrieta2021,
abstract = {Since their inception, learning techniques under the Reservoir Computing paradigm have shown a great modeling capability for recurrent systems without the computing overheads required for other approaches. Among them, different flavors of echo state networks have attracted many stares through time, mainly due to the simplicity and computational efficiency of their learning algorithm. However, these advantages do not compensate for the fact that echo state networks remain as black-box models whose decisions cannot be easily explained to the general audience. This work addresses this issue by conducting an explainability study of Echo State Networks when applied to learning tasks with time series, image and video data. Specifically, the study proposes three different techniques capable of eliciting understandable information about the knowledge grasped by these recurrent models, namely, potential memory, temporal patterns and pixel absence effect. Potential memory addresses questions related to the effect of the reservoir size in the capability of the model to store temporal information, whereas temporal patterns unveils the recurrent relationships captured by the model over time. Finally, pixel absence effect attempts at evaluating the effect of the absence of a given pixel when the echo state network model is used for image and video classification. We showcase the benefits of our proposed suite of techniques over three different domains of applicability: time series modeling, image and, for the first time in the related literature, video classification. Our results reveal that the proposed techniques not only allow for a informed understanding of the way these models work, but also serve as diagnostic tools capable of detecting issues inherited from data (e.g. presence of hidden bias).},
archivePrefix = {arXiv},
arxivId = {2102.08634},
author = {Arrieta, Alejandro Barredo and Gil-Lopez, Sergio and La{\~{n}}a, Ibai and Bilbao, Miren Nekane and {Del Ser}, Javier},
eprint = {2102.08634},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Arrieta et al. - 2021 - On the Post-hoc Explainability of Deep Echo State Networks for Time Series Forecasting, Image and Video Class(2).pdf:pdf},
keywords = {computing,echo state networks,explainable artificial intelligence,randomization based machine learning,reservoir},
title = {{On the Post-hoc Explainability of Deep Echo State Networks for Time Series Forecasting, Image and Video Classification}},
url = {http://arxiv.org/abs/2102.08634},
year = {2021}
}
@article{Tang2019,
abstract = {Neuropathologists assess vast brain areas to identify diverse and subtly-differentiated morphologies. Standard semi-quantitative scoring approaches, however, are coarse-grained and lack precise neuroanatomic localization. We report a proof-of-concept deep learning pipeline that identifies specific neuropathologies—amyloid plaques and cerebral amyloid angiopathy—in immunohistochemically-stained archival slides. Using automated segmentation of stained objects and a cloud-based interface, we annotate > 70,000 plaque candidates from 43 whole slide images (WSIs) to train and evaluate convolutional neural networks. Networks achieve strong plaque classification on a 10-WSI hold-out set (0.993 and 0.743 areas under the receiver operating characteristic and precision recall curve, respectively). Prediction confidence maps visualize morphology distributions at high resolution. Resulting network-derived amyloid beta (A$\beta$)-burden scores correlate well with established semi-quantitative scores on a 30-WSI blinded hold-out. Finally, saliency mapping demonstrates that networks learn patterns agreeing with accepted pathologic features. This scalable means to augment a neuropathologist's ability suggests a route to neuropathologic deep phenotyping.},
author = {Tang, Ziqi and Chuang, Kangway V. and DeCarli, Charles and Jin, Lee Way and Beckett, Laurel and Keiser, Michael J. and Dugger, Brittany N.},
doi = {10.1038/s41467-019-10212-1},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tang et al. - 2019 - Interpretable classification of Alzheimer's disease pathologies with a convolutional neural network pipeline.pdf:pdf},
isbn = {4146701910},
issn = {20411723},
journal = {Nature Communications},
number = {1},
pages = {1--14},
pmid = {31092819},
title = {{Interpretable classification of Alzheimer's disease pathologies with a convolutional neural network pipeline}},
volume = {10},
year = {2019}
}
@article{Wang2020a,
abstract = {The problem of counterfactual visual explanations is considered. A new family of discriminant explanations is introduced. These produce heatmaps that attribute high scores to image regions informative of a classifier prediction but not of a counter class. They connect attributive explanations, which are based on a single heat map, to counterfactual explanations, which account for both predicted class and counter class. The latter are shown to be computable by combination of two discriminant explanations, with reversed class pairs. It is argued that self-awareness, namely the ability to produce classification confidence scores, is important for the computation of discriminant explanations, which seek to identify regions where it is easy to discriminate between prediction and counter class. This suggests the computation of discriminant explanations by the combination of three attribution maps. The resulting counterfactual explanations are optimization free and thus much faster than previous methods. To address the difficulty of their evaluation, a proxy task and set of quantitative metrics are also proposed. Experiments under this protocol show that the proposed counterfactual explanations outperform the state of the art while achieving much higher speeds, for popular networks. In a human-learning machine teaching experiment, they are also shown to improve mean student accuracy from chance level to 95%.},
archivePrefix = {arXiv},
arxivId = {2004.07769},
author = {Wang, Pei and Vasconcelos, Nuno},
doi = {10.1109/CVPR42600.2020.00900},
eprint = {2004.07769},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Vasconcelos - 2020 - Scout Self-aware discriminant counterfactual explanations.pdf:pdf},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {8978--8987},
title = {{Scout: Self-aware discriminant counterfactual explanations}},
year = {2020}
}
@article{Sabih2020,
abstract = {For many applications, utilizing DNNs (Deep Neural Networks) requires their implementation on a target architecture in an optimized manner concerning energy consumption, memory requirement, throughput, etc. DNN compression is used to reduce the memory footprint and complexity of a DNN before its deployment on hardware. Recent efforts to understand and explain AI (Artificial Intelligence) methods have led to a new research area, termed as explainable AI. Explainable AI methods allow us to understand better the inner working of DNNs, such as the importance of different neurons and features. The concepts from explainable AI provide an opportunity to improve DNN compression methods such as quantization and pruning in several ways that have not been sufficiently explored so far. In this paper, we utilize explainable AI methods: mainly DeepLIFT method. We use these methods for (1) pruning of DNNs; this includes structured and unstructured pruning of \ac{CNN} filters pruning as well as pruning weights of fully connected layers, (2) non-uniform quantization of DNN weights using clustering algorithm; this is also referred to as Weight Sharing, and (3) integer-based mixed-precision quantization; this is where each layer of a DNN may use a different number of integer bits. We use typical image classification datasets with common deep learning image classification models for evaluation. In all these three cases, we demonstrate significant improvements as well as new insights and opportunities from the use of explainable AI in DNN compression.},
archivePrefix = {arXiv},
arxivId = {2008.09072},
author = {Sabih, Muhammad and Hannig, Frank and Teich, Juergen},
eprint = {2008.09072},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sabih, Hannig, Teich - 2020 - Utilizing Explainable AI for Quantization and Pruning of Deep Neural Networks.pdf:pdf},
title = {{Utilizing Explainable AI for Quantization and Pruning of Deep Neural Networks}},
url = {http://arxiv.org/abs/2008.09072},
year = {2020}
}
@article{Manuscript2020,
abstract = {Cladding austenitic stainless steels are trendy these days in pressure vessels to enhance surface qualities. In this work, austenitic stainless steel clad layers deposited by flux cored arc welding process on structural steel plates used in boiler construction are investigated. To facilitate this, composite clad layers are produced with 316L stainless steel deposits on low carbon structural steel plates based on the design of experiments. Results exhibit an incremental trend of thermal conductivity with respect to percentage of weld dilution. A linear mathematical relationship is established between the percentage of weld dilution and post-weld thermal conductivity of clad layers for the prediction of thermal conductivity of clad layer deposits. These results could be supportive in the fabrication industries for producing energy efficient thermal equipment},
author = {Manuscript, Accepted},
title = {{Ac ce us}},
year = {2020}
}
@article{Lu2020,
abstract = {This paper solves the fake news detection problem under a more realistic scenario on social media. Given the source short-text tweet and the corresponding sequence of retweet users without text comments, we aim at predicting whether the source tweet is fake or not, and generating explanation by highlighting the evidences on suspicious retweeters and the words they concern. We develop a novel neural network-based model, Graph-aware Co-Attention Networks (GCAN), to achieve the goal. Extensive experiments conducted on real tweet datasets exhibit that GCAN can significantly outperform state-of-the-art methods by 16% in accuracy on average. In addition, the case studies also show that GCAN can produce reasonable explanations.},
archivePrefix = {arXiv},
arxivId = {2004.11648},
author = {Lu, Yi-Ju and Li, Cheng-Te},
doi = {10.18653/v1/2020.acl-main.48},
eprint = {2004.11648},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lu, Li - 2020 - GCAN Graph-aware Co-Attention Networks for Explainable Fake News Detection on Social Media.pdf:pdf},
pages = {505--514},
title = {{GCAN: Graph-aware Co-Attention Networks for Explainable Fake News Detection on Social Media}},
year = {2020}
}
@article{Selvaraju2016,
abstract = {We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing input regions that are 'important' for predictions -- or visual explanations. Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses class-specific gradient information to localize important regions. These localizations are combined with existing pixel-space visualizations to create a novel high-resolution and class-discriminative visualization called Guided Grad-CAM. These methods help better understand CNN-based models, including image captioning and visual question answering (VQA) models. We evaluate our visual explanations by measuring their ability to discriminate between classes, to inspire trust in humans, and their correlation with occlusion maps. Grad-CAM provides a new way to understand CNN-based models. We have released code, an online demo hosted on CloudCV, and a full version of this extended abstract.},
archivePrefix = {arXiv},
arxivId = {1611.07450},
author = {Selvaraju, Ramprasaath R and Das, Abhishek and Vedantam, Ramakrishna and Cogswell, Michael and Parikh, Devi and Batra, Dhruv},
eprint = {1611.07450},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Selvaraju et al. - 2016 - Grad-CAM Why did you say that.pdf:pdf},
pages = {1--4},
title = {{Grad-CAM: Why did you say that?}},
url = {http://arxiv.org/abs/1611.07450},
year = {2016}
}
@article{Puiutta2020,
abstract = {Explainable Artificial Intelligence (XAI), i.e., the development of more transparent and interpretable AI models, has gained increased traction over the last few years. This is due to the fact that, in conjunction with their growth into powerful and ubiquitous tools, AI models exhibit one detrimental characteristic: a performance-transparency trade-off. This describes the fact that the more complex a model's inner workings, the less clear it is how its predictions or decisions were achieved. But, especially considering Machine Learning (ML) methods like Reinforcement Learning (RL) where the system learns autonomously, the necessity to understand the underlying reasoning for their decisions becomes apparent. Since, to the best of our knowledge, there exists no single work offering an overview of Explainable Reinforcement Learning (XRL) methods, this survey attempts to address this gap. We give a short summary of the problem, a definition of important terms, and offer a classification and assessment of current XRL methods. We found that a) the majority of XRL methods function by mimicking and simplifying a complex model instead of designing an inherently simple one, and b) XRL (and XAI) methods often neglect to consider the human side of the equation, not taking into account research from related fields like psychology or philosophy. Thus, an interdisciplinary effort is needed to adapt the generated explanations to a (non-expert) human user in order to effectively progress in the field of XRL and XAI in general.},
archivePrefix = {arXiv},
arxivId = {2005.06247},
author = {Puiutta, Erika and Veith, Eric M.S.P.},
doi = {10.1007/978-3-030-57321-8_5},
eprint = {2005.06247},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Puiutta, Veith - 2020 - Explainable Reinforcement Learning A Survey.pdf:pdf},
isbn = {9783030573201},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Explainable,Human-computer interaction,Interpretable,Machine learning,Reinforcement Learning},
pages = {77--95},
title = {{Explainable Reinforcement Learning: A Survey}},
volume = {12279 LNCS},
year = {2020}
}
@article{Bau2017,
abstract = {We propose a general framework called Network Dissection for quantifying the interpretability of latent representations of CNNs by evaluating the alignment between individual hidden units and a set of semantic concepts. Given any CNN model, the proposed method draws on a broad data set of visual concepts to score the semantics of hidden units at each intermediate convolutional layer. The units with semantics are given labels across a range of objects, parts, scenes, textures, materials, and colors. We use the proposed method to test the hypothesis that interpretability of units is equivalent to random linear combinations of units, then we apply our method to compare the latent representations of various networks when trained to solve different supervised and self-supervised training tasks. We further analyze the effect of training iterations, compare networks trained with different initializations, examine the impact of network depth and width, and measure the effect of dropout and batch normalization on the interpretability of deep visual representations. We demonstrate that the proposed method can shed light on characteristics of CNN models and training methods that go beyond measurements of their discriminative power.},
archivePrefix = {arXiv},
arxivId = {1704.05796},
author = {Bau, David and Zhou, Bolei and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},
doi = {10.1109/CVPR.2017.354},
eprint = {1704.05796},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bau et al. - 2017 - Network dissection Quantifying interpretability of deep visual representations.pdf:pdf},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {3319--3327},
title = {{Network dissection: Quantifying interpretability of deep visual representations}},
volume = {2017-Janua},
year = {2017}
}
@article{Srinivas2019,
abstract = {We introduce a new tool for interpreting neural net responses, namely full-gradients, which decomposes the neural net response into input sensitivity and per-neuron sensitivity components. This is the first proposed representation which satisfies two key properties: completeness and weak dependence, which provably cannot be satisfied by any saliency map-based interpretability method. For convolutional nets, we also propose an approximate saliency map representation, called FullGrad, obtained by aggregating the full-gradient components. We experimentally evaluate the usefulness of FullGrad in explaining model behaviour with two quantitative tests: pixel perturbation and remove-and-retrain. Our experiments reveal that our method explains model behavior correctly, and more comprehensively, than other methods in the literature. Visual inspection also reveals that our saliency maps are sharper and more tightly confined to object regions than other methods.},
archivePrefix = {arXiv},
arxivId = {1905.00780},
author = {Srinivas, Suraj and Fleuret, Fran{\c{c}}ois},
eprint = {1905.00780},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Srinivas, Fleuret - 2019 - Full-gradient representation for neural network visualization(2).pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {NeurIPS},
pages = {1--10},
title = {{Full-gradient representation for neural network visualization}},
volume = {32},
year = {2019}
}
@article{Bansal2020,
abstract = {Attribution methods can provide powerful insights into the reasons for a classifier's decision. We argue that a key desideratum of an explanation method is its robustness to input hyperparameters which are often randomly set or empirically tuned. High sensitivity to arbitrary hyperparameter choices does not only impede reproducibility but also questions the correctness of an explanation and impairs the trust of end-users. In this paper, we provide a thorough empirical study on the sensitivity of existing attribution methods. We found an alarming trend that many methods are highly sensitive to changes in their common hyperparameters e.g. even changing a random seed can yield a different explanation! Interestingly, such sensitivity is not reflected in the average explanation accuracy scores over the dataset as commonly reported in the literature. In addition, explanations generated for robust classifiers (i.e. which are trained to be invariant to pixel-wise perturbations) are surprisingly more robust than those generated for regular classifiers.},
author = {Bansal, Naman and Agarwal, Chirag and Nguyen, Anh},
doi = {10.1109/CVPRW50498.2020.00009},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bansal, Agarwal, Nguyen - 2020 - SAM The sensitivity of attribution methods to hyperparameters.pdf:pdf},
isbn = {9781728193601},
issn = {21607516},
journal = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
pages = {11--21},
title = {{SAM: The sensitivity of attribution methods to hyperparameters}},
volume = {2020-June},
year = {2020}
}
@article{Yang2020a,
abstract = {The lack of interpretability and transparency are preventing economists from using advanced tools like neural networks in their empirical research. In this paper, we propose a class of interpretable neural network models that can achieve both high prediction accuracy and interpretability. The model can be written as a simple function of a regularized number of interpretable features, which are outcomes of interpretable functions encoded in the neural network. Researchers can design different forms of interpretable functions based on the nature of their tasks. In particular, we encode a class of interpretable functions named persistent change filters in the neural network to study time series cross-sectional data. We apply the model to predicting individual's monthly employment status using high-dimensional administrative data. We achieve an accuracy of 94.5% in the test set, which is comparable to the best performed conventional machine learning methods. Furthermore, the interpretability of the model allows us to understand the mechanism that underlies the prediction: an individual's employment status is closely related to whether she pays different types of insurances. Our work is a useful step towards overcoming the black-box problem of neural networks, and provide a new tool for economists to study administrative and proprietary big data.},
archivePrefix = {arXiv},
arxivId = {2010.05311},
author = {Yang, Yucheng and Zheng, Zhong and E, Weinan},
doi = {10.2139/ssrn.3708445},
eprint = {2010.05311},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang, Zheng, E - 2020 - Interpretable Neural Networks for Panel Data Analysis in Economics.pdf:pdf},
journal = {SSRN Electronic Journal},
title = {{Interpretable Neural Networks for Panel Data Analysis in Economics}},
year = {2020}
}
@article{Rawal2020,
abstract = {As predictive models are increasingly being deployed in high-stakes decision-making, there has been a lot of interest in developing algorithms which can provide recourses to affected individuals. While developing such tools is important, it is even more critical to analyse and interpret a predictive model, and vet it thoroughly to ensure that the recourses it offers are meaningful and non-discriminatory before it is deployed in the real world. To this end, we propose a novel model agnostic framework called Actionable Recourse Summaries (AReS) to construct global counterfactual explanations which provide an interpretable and accurate summary of recourses for the entire population. We formulate a novel objective which simultaneously optimizes for correctness of the recourses and interpretability of the explanations, while minimizing overall recourse costs across the entire population. More specifically, our objective enables us to learn, with optimality guarantees on recourse correctness, a small number of compact rule sets each of which capture recourses for well defined subpopulations within the data. We also demonstrate theoretically that several of the prior approaches proposed to generate recourses for individuals are special cases of our framework. Experimental evaluation with real world datasets and user studies demonstrate that our framework can provide decision makers with a comprehensive overview of recourses corresponding to any black box model, and consequently help detect undesirable model biases and discrimination.},
archivePrefix = {arXiv},
arxivId = {2009.07165},
author = {Rawal, Kaivalya and Lakkaraju, Himabindu},
eprint = {2009.07165},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rawal, Lakkaraju - 2020 - Beyond Individualized Recourse Interpretable and Interactive Summaries of Actionable Recourses(2).pdf:pdf},
number = {NeurIPS},
title = {{Beyond Individualized Recourse: Interpretable and Interactive Summaries of Actionable Recourses}},
url = {http://arxiv.org/abs/2009.07165},
year = {2020}
}
@article{Spinner2020,
abstract = {We propose a framework for interactive and explainable machine learning that enables users to (1) understand machine learning models; (2) diagnose model limitations using different explainable AI methods; as well as (3) refine and optimize the models. Our framework combines an iterative XAI pipeline with eight global monitoring and steering mechanisms, including quality monitoring, provenance tracking, model comparison, and trust building. To operationalize the framework, we present explAIner, a visual analytics system for interactive and explainable machine learning that instantiates all phases of the suggested pipeline within the commonly used TensorBoard environment. We performed a user-study with nine participants across different expertise levels to examine their perception of our workflow and to collect suggestions to fill the gap between our system and framework. The evaluation confirms that our tightly integrated system leads to an informed machine learning process while disclosing opportunities for further extensions.},
archivePrefix = {arXiv},
arxivId = {1908.00087},
author = {Spinner, Thilo and Schlegel, Udo and Sch{\"{a}}fer, Hanna and El-Assady, Mennatallah},
doi = {10.1109/TVCG.2019.2934629},
eprint = {1908.00087},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Spinner et al. - 2020 - ExplAIner A Visual Analytics Framework for Interactive and Explainable Machine Learning.pdf:pdf},
issn = {19410506},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Deep Learning,Explainability,Explainable AI,Interactive Machine Learning,Interpretability,Visual Analytics},
number = {1},
pages = {1064--1074},
pmid = {31442998},
title = {{ExplAIner: A Visual Analytics Framework for Interactive and Explainable Machine Learning}},
volume = {26},
year = {2020}
}
@article{Burns2020,
abstract = {In science and medicine, model interpretations may be reported as discoveries of natural phenomena or used to guide patient treatments. In such high-stakes tasks, false discoveries may lead investigators astray. These applications would therefore benefit from control over the finite-sample error rate of interpretations. We reframe black box model interpretability as a multiple hypothesis testing problem. The task is to discover "important"features by testing whether the model prediction is significantly different from what would be expected if the features were replaced with uninformative counterfactuals. We propose two testing methods: one that provably controls the false discovery rate but which is not yet feasible for large-scale applications, and an approximate testing method which can be applied to real-world data sets. In simulation, both tests have high power relative to existing interpretability methods. When applied to state-of-the-art vision and language models, the framework selects features that intuitively explain model predictions. The resulting explanations have the additional advantage that they are themselves easy to interpret.},
archivePrefix = {arXiv},
arxivId = {1904.00045},
author = {Burns, Collin and Thomason, Jesse and Tansey, Wesley},
doi = {10.1145/3412815.3416889},
eprint = {1904.00045},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Burns, Thomason, Tansey - 2020 - Interpreting Black Box Models via Hypothesis Testing(2).pdf:pdf},
isbn = {9781450381031},
journal = {FODS 2020 - Proceedings of the 2020 ACM-IMS Foundations of Data Science Conference},
keywords = {black box,fdr control,hypothesis testing,interpretability,transparency},
pages = {47--57},
title = {{Interpreting Black Box Models via Hypothesis Testing}},
year = {2020}
}
@article{LeonSixt2021,
abstract = {Current state of the art computer vision applications rely on highly complex models. Their interpretability is mostly limited to post-hoc methods which are not guaranteed to be faithful to the model. To elucidate a model's decision, we present a novel interpretable model based on an invertible deep convolutional network. Our model generates meaningful, faithful, and ideal counterfactuals. Using PCA on the classifier's input, we can also create "isofactuals"-image interpolations with the same outcome but visually meaningful different features. Counter-and isofactuals can be used to identify positive and negative evidence in an image. This can also be visualized with heatmaps. We evaluate our approach against gradient-based at-tribution methods, which we find to produce meaningless adversarial perturbations. Using our method, we reveal biases in three different datasets. In a human subject experiment, we test whether non-experts find our method useful to spot spurious correlations learned by a model. Our work is a step towards more trustworthy explanations for computer vision. For code and datasets see 1 .},
author = {{Leon Sixt} and $\sim$Leon_Sixt1 and {, Martin Schuessler, Philipp Wei{\ss}}, Tim Landgraf},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Leon Sixt, $\sim$Leon_Sixt1, , Martin Schuessler, Philipp Wei{\ss} - 2021 - Interpretability Through Invertibility A Deep Convolutional Netwo(2).pdf:pdf},
journal = {Not accepted - ICLR},
number = {2018},
pages = {1--20},
title = {{Interpretability Through Invertibility: A Deep Convolutional Network With Ideal Counterfactuals And Isosurfaces}},
url = {https://paperswithcode.com/paper/interpretability-through-invertibility-a-deep},
year = {2021}
}
@article{Bastani2017a,
abstract = {Interpretability has become incredibly important as machine learning is increasingly used to inform consequential decisions. We propose to construct global explanations of complex, blackbox models in the form of a decision tree approximating the original model---as long as the decision tree is a good approximation, then it mirrors the computation performed by the blackbox model. We devise a novel algorithm for extracting decision tree explanations that actively samples new training points to avoid overfitting. We evaluate our algorithm on a random forest to predict diabetes risk and a learned controller for cart-pole. Compared to several baselines, our decision trees are both substantially more accurate and equally or more interpretable based on a user study. Finally, we describe several insights provided by our interpretations, including a causal issue validated by a physician.},
archivePrefix = {arXiv},
arxivId = {1705.08504},
author = {Bastani, Osbert and Kim, Carolyn and Bastani, Hamsa},
eprint = {1705.08504},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bastani, Kim, Bastani - 2017 - Interpreting Blackbox Models via Model Extraction.pdf:pdf},
title = {{Interpreting Blackbox Models via Model Extraction}},
url = {http://arxiv.org/abs/1705.08504},
year = {2017}
}
@article{Liu2019,
abstract = {In this work, we propose an introspection technique for deep neural networks that relies on a generative model to instigate salient editing of the input image for model interpretation. Such modification provides the fundamental interventional operation that allows us to obtain answers to counterfactual inquiries, i.e., what meaningful change can be made to the input image in order to alter the prediction. We demonstrate how to reveal interesting properties of the given classifiers by utilizing the proposed introspection approach on both the MNIST and the CelebA dataset.},
archivePrefix = {arXiv},
arxivId = {1907.03077},
author = {Liu, Shusen and Kailkhura, Bhavya and Loveland, Donald and Han, Yong},
doi = {10.1109/GlobalSIP45357.2019.8969491},
eprint = {1907.03077},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2019 - Generative counterfactual introspection for explainable deep learning(2).pdf:pdf},
isbn = {9781728127231},
journal = {GlobalSIP 2019 - 7th IEEE Global Conference on Signal and Information Processing, Proceedings},
keywords = {Counterfactual Reasoning,Explainable Deep Learning,Generative Adversarial Network,Model Introspection},
title = {{Generative counterfactual introspection for explainable deep learning}},
year = {2019}
}
@article{Jonsson2020,
author = {J{\'{o}}nsson, Hlynur and Cherubini, Giovanni and Eleftheriou, Evangelos},
doi = {10.3390/e22070727},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/J{\'{o}}nsson, Cherubini, Eleftheriou - 2020 - Convergence Behavior of DNNs with Mutual-Information-Based Regularization Hlynur.pdf:pdf},
keywords = {deep neural networks,information bottleneck,regularization methods},
title = {{Convergence Behavior of DNNs with Mutual-Information-Based Regularization Hlynur}},
year = {2020}
}
@article{Zhang2019,
abstract = {This paper aims to quantitatively explain the rationales of each prediction that is made by a pre-trained convolutional neural network (CNN). We propose to learn a decision tree, which clarifies the specific reason for each prediction made by the CNN at the semantic level. I.e., the decision tree decomposes feature representations in high conv-layers of the CNN into elementary concepts of object parts. In this way, the decision tree tells people which object parts activate which filters for the prediction and how much each object part contributes to the prediction score. Such semantic and quantitative explanations for CNN predictions have specific values beyond the traditional pixel-level analysis of CNNs. More specifically, our method mines all potential decision modes of the CNN, where each mode represents a typical case of how the CNN uses object parts for prediction. The decision tree organizes all potential decision modes in a coarse-to-fine manner to explain CNN predictions at different fine-grained levels. Experiments have demonstrated the effectiveness of the proposed method.},
archivePrefix = {arXiv},
arxivId = {1802.00121},
author = {Zhang, Quanshi and Yang, Yu and Ma, Haotian and Wu, Ying Nian},
doi = {10.1109/CVPR.2019.00642},
eprint = {1802.00121},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2019 - Interpreting cnns via decision trees.pdf:pdf},
isbn = {9781728132938},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {Deep Learning,Grouping and Shape,Others,Representation Learning,Segmentation,Statistical Learn,Visual Reasoning},
pages = {6254--6263},
title = {{Interpreting cnns via decision trees}},
volume = {2019-June},
year = {2019}
}
@article{Kanehira2019b,
abstract = {This study addresses generating counterfactual explanations with multimodal information. Our goal is not only to classify a video into a specific category, but also to provide explanations on why it is not categorized to a specific class with combinations of visual-linguistic information. Requirements that the expected output should satisfy are referred to as counterfactuality in this paper: (1) Compatibility of visual-linguistic explanations, and (2) Positiveness/negativeness for the specific positive/negative class. Exploiting a spatio-temporal region (tube) and an attribute as visual and linguistic explanations respectively, the explanation model is trained to predict the counterfactuality for possible combinations of multimodal information in a post-hoc manner. The optimization problem, which appears during training/inference, can be efficiently solved by inserting a novel neural network layer, namely the maximum subpath layer. We demonstrated the effectiveness of this method by comparison with a baseline of the action recognition datasets extended for this task. Moreover, we provide information-theoretical insight into the proposed method.},
archivePrefix = {arXiv},
arxivId = {1812.01263},
author = {Kanehira, Atsushi and Takemoto, Kentaro and Inayoshi, Sho and Harada, Tatsuya},
doi = {10.1109/CVPR.2019.00879},
eprint = {1812.01263},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kanehira et al. - 2019 - Multimodal explanations by predicting counterfactuality in videos(2).pdf:pdf},
isbn = {9781728132938},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {Video Analytics,Vision + Language,Vision Applications and Systems},
pages = {8586--8594},
title = {{Multimodal explanations by predicting counterfactuality in videos}},
volume = {2019-June},
year = {2019}
}
@article{Cortez2011,
abstract = {There are several supervised learning Data Mining (DM) methods, such as Neural Networks (NN), Support Vector Machines (SVM) and ensembles, that often attain high quality predictions, although the obtained models are difficult to interpret by humans. In this paper, we open these black box DM models by using a novel visualization approach that is based on a Sensitivity Analysis (SA) method. In particular, we propose a Global SA (GSA), which extends the applicability of previous SA methods (e.g. to classification tasks), and several visualization techniques (e.g. variable effect characteristic curve), for assessing input relevance and effects on the model's responses. We show the GSA capabilities by conducting several experiments, using a NN ensemble and SVM model, in both synthetic and real-world datasets. {\textcopyright} 2011 IEEE.},
author = {Cortez, Paulo and Embrechts, Mark J.},
doi = {10.1109/CIDM.2011.5949423},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cortez, Embrechts - 2011 - Opening black box Data Mining models using Sensitivity Analysis.pdf:pdf},
isbn = {9781424499274},
journal = {IEEE Symposium on Computational Intelligence and Data Mining},
pages = {341--348},
title = {{Opening black box Data Mining models using Sensitivity Analysis}},
year = {2011}
}
@article{Vilamala2017,
abstract = {Sleep studies are important for diagnosing sleep disorders such as insomnia, narcolepsy or sleep apnea. They rely on manual scoring of sleep stages from raw polisomnography signals, which is a tedious visual task requiring the workload of highly trained professionals. Consequently, research efforts to purse for an automatic stage scoring based on machine learning techniques have been carried out over the last years. In this work, we resort to multitaper spectral analysis to create visually interpretable images of sleep patterns from EEG signals as inputs to a deep convolutional network trained to solve visual recognition tasks. As a working example of transfer learning, a system able to accurately classify sleep stages in new unseen patients is presented. Evaluations in a widely-used publicly available dataset favourably compare to state-of-the-art results, while providing a framework for visual interpretation of outcomes.},
archivePrefix = {arXiv},
arxivId = {1710.00633},
author = {Vilamala, Albert and Madsen, Kristoffer H. and Hansen, Lars K.},
doi = {10.1109/MLSP.2017.8168133},
eprint = {1710.00633},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vilamala, Madsen, Hansen - 2017 - Deep convolutional neural networks for interpretable analysis of EEG sleep stage scoring.pdf:pdf},
isbn = {9781509063413},
issn = {21610371},
journal = {IEEE International Workshop on Machine Learning for Signal Processing, MLSP},
keywords = {Convolutional Neural Networks,Multitaper Spectral Analysis,Sleep Stage Scoring,Transfer Learning},
number = {October},
pages = {1--6},
title = {{Deep convolutional neural networks for interpretable analysis of EEG sleep stage scoring}},
volume = {2017-Septe},
year = {2017}
}
@article{Kumar2017,
abstract = {In this work, we propose CLass-Enhanced Attentive Response (CLEAR): an approach to visualize and understand the decisions made by deep neural networks (DNNs) given a specific input. CLEAR facilitates the visualization of attentive regions and levels of interest of DNNs during the decision-making process. It also enables the visualization of the most dominant classes associated with these attentive regions of interest. As such, CLEAR can mitigate some of the shortcomings of heatmap-based methods associated with decision ambiguity, and allows for better insights into the decision-making process of DNNs. Quantitative and qualitative experiments across three different datasets demonstrate the efficacy of CLEAR for gaining a better understanding of the inner workings of DNNs during the decision-making process.},
archivePrefix = {arXiv},
arxivId = {1704.04133},
author = {Kumar, Devinder and Wong, Alexander and Taylor, Graham W.},
doi = {10.1109/CVPRW.2017.215},
eprint = {1704.04133},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kumar, Wong, Taylor - 2017 - Explaining the Unexplained A CLass-Enhanced Attentive Response (CLEAR) Approach to Understanding Deep Neura.pdf:pdf},
isbn = {9781538607336},
issn = {21607516},
journal = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
pages = {1686--1694},
title = {{Explaining the Unexplained: A CLass-Enhanced Attentive Response (CLEAR) Approach to Understanding Deep Neural Networks}},
volume = {2017-July},
year = {2017}
}
@article{Barratt2017,
abstract = {Humans are able to explain their reasoning. On the contrary, deep neural networks are not. This paper attempts to bridge this gap by introducing a new way to design interpretable neural networks for classification, inspired by physiological evidence of the human visual system's inner-workings. This paper proposes a neural network design paradigm, termed InterpNET, which can be combined with any existing classification architecture to generate natural language explanations of the classifications. The success of the module relies on the assumption that the network's computation and reasoning is represented in its internal layer activations. While in principle InterpNET could be applied to any existing classification architecture, it is evaluated via an image classification and explanation task. Experiments on a CUB bird classification and explanation dataset show qualitatively and quantitatively that the model is able to generate high-quality explanations. While the current state-of-the-art METEOR score on this dataset is 29.2, InterpNET achieves a much higher METEOR score of 37.9.},
archivePrefix = {arXiv},
arxivId = {1710.09511},
author = {Barratt, Shane},
eprint = {1710.09511},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Barratt - 2017 - InterpNET Neural Introspection for Interpretable Deep Learning(2).pdf:pdf},
number = {Nips},
title = {{InterpNET: Neural Introspection for Interpretable Deep Learning}},
url = {http://arxiv.org/abs/1710.09511},
year = {2017}
}
@article{Yosinski2015,
abstract = {Recent years have produced great advances in training large, deep neural networks (DNNs), including notable successes in training convolutional neural networks (convnets) to recognize natural images. However, our understanding of how these models work, especially what computations they perform at intermediate layers, has lagged behind. Progress in the field will be further accelerated by the development of better tools for visualizing and interpreting neural nets. We introduce two such tools here. The first is a tool that visualizes the activations produced on each layer of a trained convnet as it processes an image or video (e.g. a live webcam stream). We have found that looking at live activations that change in response to user input helps build valuable intuitions about how convnets work. The second tool enables visualizing features at each layer of a DNN via regularized optimization in image space. Because previous versions of this idea produced less recognizable images, here we introduce several new regularization methods that combine to produce qualitatively clearer, more interpretable visualizations. Both tools are open source and work on a pre-trained convnet with minimal setup.},
archivePrefix = {arXiv},
arxivId = {1506.06579},
author = {Yosinski, Jason and Clune, Jeff and Nguyen, Anh and Fuchs, Thomas and Lipson, Hod},
eprint = {1506.06579},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yosinski et al. - 2015 - Understanding Neural Networks Through Deep Visualization.pdf:pdf},
title = {{Understanding Neural Networks Through Deep Visualization}},
url = {http://arxiv.org/abs/1506.06579},
year = {2015}
}
@article{Bhatt2020a,
abstract = {Explainable machine learning offers the potential to provide stakeholders with insights into model behavior by using various methods such as feature importance scores, counterfactual explanations, or influential training data. Yet there is little understanding of how organizations use these methods in practice. This study explores how organizations view and use explainability for stakeholder consumption. We find that, currently, the majority of deployments are not for end users affected by the model but rather for machine learning engineers, who use explainability to debug the model itself. There is thus a gap between explainability in practice and the goal of transparency, since explanations primarily serve internal stakeholders rather than external ones. Our study synthesizes the limitations of current explainability techniques that hamper their use for end users. To facilitate end user interaction, we develop a framework for establishing clear goals for explainability. We end by discussing concerns raised regarding explainability.},
archivePrefix = {arXiv},
arxivId = {1909.06342},
author = {Bhatt, Umang and Xiang, Alice and Sharma, Shubham and Weller, Adrian and Taly, Ankur and Jia, Yunhan and Ghosh, Joydeep and Puri, Ruchir and Moura, Jos{\'{e}} M.F. and Eckersley, Peter},
doi = {10.1145/3351095.3375624},
eprint = {1909.06342},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bhatt et al. - 2020 - Explainable machine learning in deployment.pdf:pdf},
isbn = {9781450369367},
journal = {FAT* 2020 - Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
keywords = {Deployed systems,Explainability,Machine learning,Qualitative study,Transparency},
pages = {648--657},
title = {{Explainable machine learning in deployment}},
year = {2020}
}
@article{Geiger2020,
abstract = {In this short note, we relate the variational bounds proposed in Alemi et al. (2017) and Fischer (2020) for the information bottleneck (IB) and the conditional entropy bottleneck (CEB) functional, respectively. Although the two functionals were shown to be equivalent, it was empirically observed that optimizing bounds on the CEB functional achieves better generalization performance and adversarial robustness than optimizing those on the IB functional. This work tries to shed light on this issue by showing that, in the most general setting, no ordering can be established between these variational bounds, while such an ordering can be enforced by restricting the feasible sets over which the optimizations take place. The absence of such an ordering in the general setup suggests that the variational bound on the CEB functional is either more amenable to optimization or a relevant cost function for optimization in its own regard, i.e., without justification from the IB or CEB functionals.},
author = {Geiger, Bernhard C. and Fischer, Ian S.},
doi = {10.3390/e22111229},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Geiger, Fischer - 2020 - A comparison of variational bounds for the information bottleneck functional.pdf:pdf},
issn = {10994300},
journal = {Entropy},
keywords = {Deep learning,Information bottleneck,Neural networks},
number = {11},
pages = {1--12},
title = {{A comparison of variational bounds for the information bottleneck functional}},
volume = {22},
year = {2020}
}
@article{Fernandez2019,
abstract = {Evolutionary fuzzy systems are one of the greatest advances within the area of computational intelligence. They consist of evolutionary algorithms applied to the design of fuzzy systems. Thanks to this hybridization, superb abilities are provided to fuzzy modeling in many different data science scenarios. This contribution is intended to comprise a position paper developing a comprehensive analysis of the evolutionary fuzzy systems research field. To this end, the »4 W» questions are posed and addressed with the aim of understanding the current context of this topic and its significance. Specifically, it will be pointed out why evolutionary fuzzy systems are important from an explainable point of view, when they began, what they are used for, and where the attention of researchers should be directed to in the near future in this area. They must play an important role for the emerging area of eXplainable Artificial Intelligence (XAI) learning from data.},
author = {Fernandez, Alberto and Herrera, Francisco and Cordon, Oscar and {Jose Del Jesus}, Maria and Marcelloni, Francesco},
doi = {10.1109/MCI.2018.2881645},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fernandez et al. - 2019 - Evolutionary fuzzy systems for explainable artificial intelligence Why, when, what for, and where to.pdf:pdf},
issn = {15566048},
journal = {IEEE Computational Intelligence Magazine},
number = {1},
pages = {69--81},
publisher = {IEEE},
title = {{Evolutionary fuzzy systems for explainable artificial intelligence: Why, when, what for, and where to?}},
volume = {14},
year = {2019}
}
@article{Chalkiadakis2018,
author = {Chalkiadakis, Ioannis},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chalkiadakis - 2018 - A brief survey of visualization methods for deep learning models from the perspective of Explainable AI (2).pdf:pdf},
pages = {1--20},
title = {{A brief survey of visualization methods for deep learning models from the perspective of Explainable AI .}},
year = {2018}
}
@article{Kanamori2020b,
abstract = {Counterfactual Explanation (CE) is one of the post-hoc explanation methods that provides a perturbation vector so as to alter the prediction result obtained from a classifier. Users can directly interpret the perturbation as an”action” for obtaining their desired decision results. However, an action extracted by existing methods often becomes unrealistic for users because they do not adequately care about the characteristics corresponding to the empirical data distribution such as feature-correlations and outlier risk. To suggest an executable action for users, we propose a new framework of CE for extracting an action by evaluating its reality on the empirical data distribution. The key idea of our proposed method is to define a new cost function based on the Mahalanobis' distance and the local outlier factor. Then, we propose a mixed-integer linear optimization approach to extracting an optimal action by minimizing our cost function. By experiments on real datasets, we confirm the effectiveness of our method in comparison with existing methods for CE.},
author = {Kanamori, Kentaro and Takagi, Takuya and Kobayashi, Ken and Arimura, Hiroki},
doi = {10.24963/ijcai.2020/395},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kanamori et al. - 2020 - DACE Distribution-aware counterfactual explanation by mixed-integer linear optimization(2).pdf:pdf},
isbn = {9780999241165},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Machine Learning: Classification,Machine Learning: Explainable Machine Learning,Machine Learning: Interpretability},
pages = {2855--2862},
title = {{DACE: Distribution-aware counterfactual explanation by mixed-integer linear optimization}},
volume = {2021-Janua},
year = {2020}
}
@article{Paez2019,
abstract = {In this paper I argue that the search for explainable models and interpretable decisions in AI must be reformulated in terms of the broader project of offering a pragmatic and naturalistic account of understanding in AI. Intuitively, the purpose of providing an explanation of a model or a decision is to make it understandable to its stakeholders. But without a previous grasp of what it means to say that an agent understands a model or a decision, the explanatory strategies will lack a well-defined goal. Aside from providing a clearer objective for XAI, focusing on understanding also allows us to relax the factivity condition on explanation, which is impossible to fulfill in many machine learning models, and to focus instead on the pragmatic conditions that determine the best fit between a model and the methods and devices deployed to understand it. After an examination of the different types of understanding discussed in the philosophical and psychological literature, I conclude that interpretative or approximation models not only provide the best way to achieve the objectual understanding of a machine learning model, but are also a necessary condition to achieve post hoc interpretability. This conclusion is partly based on the shortcomings of the purely functionalist approach to post hoc interpretability that seems to be predominant in most recent literature.},
author = {P{\'{a}}ez, Andr{\'{e}}s},
doi = {10.1007/s11023-019-09502-w},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/P{\'{a}}ez - 2019 - The Pragmatic Turn in Explainable Artificial Intelligence (XAI).pdf:pdf},
isbn = {0123456789},
issn = {15728641},
journal = {Minds and Machines},
keywords = {Black box models,Explainable artificial intelligence,Understanding,,Explanation,Machine learning,Model transparency,Post-hoc interpretability},
number = {3},
pages = {441--459},
publisher = {Springer Netherlands},
title = {{The Pragmatic Turn in Explainable Artificial Intelligence (XAI)}},
url = {https://doi.org/10.1007/s11023-019-09502-w},
volume = {29},
year = {2019}
}
@article{Seo2020,
abstract = {Recently, many methods to interpret and visualize deep neural network predictions have been proposed, and significant progress has been made. However, a more class-discriminative and visually pleasing explanation is required. Thus, this paper proposes a region-based approach that estimates feature importance in terms of appropriately segmented regions. By fusing the saliency maps generated from multi-scale segmentations, a more class-discriminative and visually pleasing map is obtained. This paper incorporates this regional multi-scale concept into a prediction difference method that is model-agnostic. An input image is segmented in several scales using the superpixel method, and exclusion of a region is simulated by sampling a normal distribution constructed via the boundary prior. The experimental results demonstrate that the regional multi-scale method produces much more class-discriminative and visually pleasing saliency maps.},
archivePrefix = {arXiv},
arxivId = {1807.11720},
author = {Seo, Dasom and Oh, Kanghan and Oh, Il Seok},
doi = {10.1109/ACCESS.2019.2963055},
eprint = {1807.11720},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Seo, Oh, Oh - 2020 - Regional multi-scale approach for visually pleasing explanations of deep neural networks.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Computer vision,explainable artificial intelligence,machine learning,neural networks},
number = {Nips},
pages = {8572--8582},
title = {{Regional multi-scale approach for visually pleasing explanations of deep neural networks}},
volume = {8},
year = {2020}
}
@article{Ancona2017,
abstract = {Understanding the flow of information in Deep Neural Networks (DNNs) is a challenging problem that has gain increasing attention over the last few years. While several methods have been proposed to explain network predictions, there have been only a few attempts to compare them from a theoretical perspective. What is more, no exhaustive empirical comparison has been performed in the past. In this work, we analyze four gradient-based attribution methods and formally prove conditions of equivalence and approximation between them. By reformulating two of these methods, we construct a unified framework which enables a direct comparison, as well as an easier implementation. Finally, we propose a novel evaluation metric, called Sensitivity-n and test the gradient-based attribution methods alongside with a simple perturbation-based attribution method on several datasets in the domains of image and text classification, using various network architectures.},
archivePrefix = {arXiv},
arxivId = {1711.06104},
author = {Ancona, Marco and Ceolini, Enea and {\"{O}}ztireli, Cengiz and Gross, Markus},
eprint = {1711.06104},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ancona et al. - 2017 - A unified view of gradient-based attribution methods for Deep Neural Networks.pdf:pdf},
issn = {23318422},
journal = {arXiv},
number = {Section 3},
title = {{A unified view of gradient-based attribution methods for Deep Neural Networks}},
year = {2017}
}
@article{Ghorbani2020,
abstract = {We develop Neuron Shapley as a new framework to quantify the contribution of individual neurons to the prediction and performance of a deep network. By accounting for interactions across neurons, Neuron Shapley is more effective in identifying important filters compared to common approaches based on activation patterns. Interestingly, removing just 30 filters with the highest Shapley scores effectively destroys the prediction accuracy of Inception-v3 on ImageNet. Visualization of these few critical filters provides insights into how the network functions. Neuron Shapley is a flexible framework and can be applied to identify responsible neurons in many tasks. We illustrate additional applications of identifying filters that are responsible for biased prediction in facial recognition and filters that are vulnerable to adversarial attacks. Removing these filters is a quick way to repair models. Computing exact Shapley values is computationally infeasible and therefore sampling-based approximations are used in practice. We introduce a new multi-armed bandit algorithm that is able to efficiently detect neurons with the largest Shapley value orders of magnitude faster than existing Shapley value approximation methods.},
archivePrefix = {arXiv},
arxivId = {2002.09815},
author = {Ghorbani, Amirata and Zou, James},
eprint = {2002.09815},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ghorbani, Zou - 2020 - Neuron Shapley Discovering the Responsible Neurons.pdf:pdf},
issn = {10495258},
number = {NeurIPS},
title = {{Neuron Shapley: Discovering the Responsible Neurons}},
url = {http://arxiv.org/abs/2002.09815},
year = {2020}
}
@article{Muddamsetty2020,
abstract = {A new brand of technical artificial intelligence (Explainable AI) research has focused on trying to open up the 'black box' and provide some explainability. This paper presents a novel visual explanation method for deep learning networks in the form of a saliency map that can effectively localize entire object regions. In contrast to the current state-of-the art methods, the proposed method shows quite promising visual explanations that can gain greater trust of human expert. Both quantitative and qualitative evaluations are carried out on both general and clinical data sets to confirm the effectiveness of the proposed method.},
archivePrefix = {arXiv},
arxivId = {2006.03122},
author = {Muddamsetty, Satya M. and Mohammad, N. S.Jahromi and Moeslund, Thomas B.},
doi = {10.1109/ICIP40778.2020.9190952},
eprint = {2006.03122},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Muddamsetty, Mohammad, Moeslund - 2020 - SIDU Similarity Difference and Uniqueness Method for Explainable AI.pdf:pdf},
isbn = {9781728163956},
issn = {15224880},
journal = {Proceedings - International Conference on Image Processing, ICIP},
keywords = {CNN,Clinical application,Explainable AI,Visual explanation},
pages = {3269--3273},
title = {{SIDU: Similarity Difference and Uniqueness Method for Explainable AI}},
volume = {2020-Octob},
year = {2020}
}
@article{Samek2017a,
author = {Srinivasan, Vignesh and Lapuschkin, Sebastian and Hellge, Cornelius and Muller, Klaus-Robert and Samek, Wojciech},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Srinivasan et al. - 2017 - INTERPRETABLE HUMAN ACTION RECOGNITION IN COMPRESSED DOMAIN.pdf:pdf},
isbn = {9781509041176},
journal = {IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2017},
pages = {1692--1696},
title = {{INTERPRETABLE HUMAN ACTION RECOGNITION IN COMPRESSED DOMAIN}},
year = {2017}
}
@article{Nguyen2019,
abstract = {A neuroscience method to understanding the brain is to find and study the preferred stimuli that highly activate an individual cell or groups of cells. Recent advances in machine learning enable a family of methods to synthesize preferred stimuli that cause a neuron in an artificial or biological brain to fire strongly. Those methods are known as Activation Maximization (AM) [10] or Feature Visualization via Optimization. In this chapter, we (1) review existing AM techniques in the literature; (2) discuss a probabilistic interpretation for AM; and (3) review the applications of AM in debugging and explaining networks.},
archivePrefix = {arXiv},
arxivId = {1904.08939},
author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
doi = {10.1007/978-3-030-28954-6_4},
eprint = {1904.08939},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nguyen, Yosinski, Clune - 2019 - Understanding Neural Networks via Feature Visualization A Survey.pdf:pdf},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Activation Maximization,Feature visualization,Generative models,Generator network,Neural networks,Optimization},
number = {Ml},
pages = {55--76},
title = {{Understanding Neural Networks via Feature Visualization: A Survey}},
volume = {11700 LNCS},
year = {2019}
}
@article{Joshi2019,
abstract = {Machine learning based decision making systems are increasingly affecting humans. An individual can suffer an undesirable outcome under such decision making systems (e.g. denied credit) irrespective of whether the decision is fair or accurate. Individual recourse pertains to the problem of providing an actionable set of changes a person can undertake in order to improve their outcome. We propose a recourse algorithm that models the underlying data distribution or manifold. We then provide a mechanism to generate the smallest set of changes that will improve an individual's outcome. This mechanism can be easily used to provide recourse for any differentiable machine learning based decision making system. Further, the resulting algorithm is shown to be applicable to both supervised classification and causal decision making systems. Our work attempts to fill gaps in existing fairness literature that have primarily focused on discovering and/or algorithmically enforcing fairness constraints on decision making systems. This work also provides an alternative approach to generating counterfactual explanations.},
archivePrefix = {arXiv},
arxivId = {1907.09615},
author = {Joshi, Shalmali and Koyejo, Oluwasanmi and Vijitbenjaronk, Warut and Kim, Been and Ghosh, Joydeep},
eprint = {1907.09615},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Joshi et al. - 2019 - Towards Realistic Individual Recourse and Actionable Explanations in Black-Box Decision Making Systems(2).pdf:pdf},
number = {Ml},
pages = {1--19},
title = {{Towards Realistic Individual Recourse and Actionable Explanations in Black-Box Decision Making Systems}},
url = {http://arxiv.org/abs/1907.09615},
year = {2019}
}
@article{Madaan2020,
abstract = {Machine Learning has seen tremendous growth recently, which has led to larger adoption of ML systems for educational assessments, credit risk, healthcare, employment, criminal justice, to name a few. The trustworthiness of ML and NLP systems is a crucial aspect and requires a guarantee that the decisions they make are fair and robust. Aligned with this, we propose a framework GYC, to generate a set of counterfactual text samples, which are crucial for testing these ML systems. Our main contributions include a) We introduce GYC, a framework to generate counterfactual samples such that the generation is plausible, diverse, goal-oriented, and effective, b) We generate counterfactual samples, that can direct the generation towards a corresponding condition such as named-entity tag, semantic role label, or sentiment. Our experimental results on various domains show that GYC generates counterfactual text samples exhibiting the above four properties. GYC generates counterfactuals that can act as test cases to evaluate a model and any text debiasing algorithm.},
archivePrefix = {arXiv},
arxivId = {2012.04698},
author = {Madaan, Nishtha and Padhi, Inkit and Panwar, Naveen and Saha, Diptikalyan},
eprint = {2012.04698},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Madaan et al. - 2020 - Generate Your Counterfactuals Towards Controlled Counterfactual Generation for Text.pdf:pdf},
title = {{Generate Your Counterfactuals: Towards Controlled Counterfactual Generation for Text}},
url = {http://arxiv.org/abs/2012.04698},
year = {2020}
}
@article{Manuscript2020,
abstract = {Cladding austenitic stainless steels are trendy these days in pressure vessels to enhance surface qualities. In this work, austenitic stainless steel clad layers deposited by flux cored arc welding process on structural steel plates used in boiler construction are investigated. To facilitate this, composite clad layers are produced with 316L stainless steel deposits on low carbon structural steel plates based on the design of experiments. Results exhibit an incremental trend of thermal conductivity with respect to percentage of weld dilution. A linear mathematical relationship is established between the percentage of weld dilution and post-weld thermal conductivity of clad layers for the prediction of thermal conductivity of clad layer deposits. These results could be supportive in the fabrication industries for producing energy efficient thermal equipment},
author = {Manuscript, Accepted},
title = {{Ac ce us}},
year = {2020}
}
@article{Choi2016,
abstract = {Accuracy and interpretability are two dominant features of successful predictive models. Typically, a choice must be made in favor of complex black box models such as recurrent neural networks (RNN) for accuracy versus less accurate but more interpretable traditional models such as logistic regression. This tradeoff poses challenges in medicine where both accuracy and interpretability are important. We addressed this challenge by developing the REverse Time AttentIoN model (RETAIN) for application to Electronic Health Records (EHR) data. RETAIN achieves high accuracy while remaining clinically interpretable and is based on a two-level neural attention model that detects influential past visits and significant clinical variables within those visits (e.g. key diagnoses). RETAIN mimics physician practice by attending the EHR data in a reverse time order so that recent clinical visits are likely to receive higher attention. RETAIN was tested on a large health system EHR dataset with 14 million visits completed by 263K patients over an 8 year period and demonstrated predictive accuracy and computational scalability comparable to state-of-the-art methods such as RNN, and ease of interpretability comparable to traditional models.},
archivePrefix = {arXiv},
arxivId = {1608.05745},
author = {Choi, Edward and Bahadori, Mohammad Taha and Kulas, Joshua A. and Schuetz, Andy and Stewart, Walter F. and Sun, Jimeng},
eprint = {1608.05745},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Choi et al. - 2016 - RETAIN An interpretable predictive model for healthcare using reverse time attention mechanism.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {Nips},
pages = {3512--3520},
title = {{RETAIN: An interpretable predictive model for healthcare using reverse time attention mechanism}},
year = {2016}
}
@article{Zhang2018a,
abstract = {We aim to model the top-down attention of a convolutional neural network (CNN) classifier for generating task-specific attention maps. Inspired by a top-down human visual attention model, we propose a new backpropagation scheme, called Excitation Backprop, to pass along top-down signals downwards in the network hierarchy via a probabilistic Winner-Take-All process. Furthermore, we introduce the concept of contrastive attention to make the top-down attention maps more discriminative. We show a theoretic connection between the proposed contrastive attention formulation and the Class Activation Map computation. Efficient implementation of Excitation Backprop for common neural network layers is also presented. In experiments, we visualize the evidence of a model's classification decision by computing the proposed top-down attention maps. For quantitative evaluation, we report the accuracy of our method in weakly supervised localization tasks on the MS COCO, PASCAL VOC07 and ImageNet datasets. The usefulness of our method is further validated in the text-to-region association task. On the Flickr30k Entities dataset, we achieve promising performance in phrase localization by leveraging the top-down attention of a CNN model that has been trained on weakly labeled web images. Finally, we demonstrate applications of our method in model interpretation and data annotation assistance for facial expression analysis and medical imaging tasks.},
archivePrefix = {arXiv},
arxivId = {1608.00507},
author = {Zhang, Jianming and Bargal, Sarah Adel and Lin, Zhe and Brandt, Jonathan and Shen, Xiaohui and Sclaroff, Stan},
doi = {10.1007/s11263-017-1059-x},
eprint = {1608.00507},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2018 - Top-Down Neural Attention by Excitation Backprop.pdf:pdf},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Convolutional neural network,Selective tuning,Top-down attention},
number = {10},
pages = {1084--1102},
title = {{Top-Down Neural Attention by Excitation Backprop}},
volume = {126},
year = {2018}
}
@article{Arras2016,
abstract = {Text documents can be described by a number of abstract concepts such as semantic category, writing style, or sentiment. Machine learning (ML) models have been trained to automatically map documents to these abstract concepts, allowing to annotate very large text collections, more than could be processed by a human in a lifetime. Besides predicting the text's category very accurately, it is also highly desirable to understand how and why the categorization process takes place. In this paper, we demonstrate that such understanding can be achieved by tracing the classification decision back to individual words using layer-wise relevance propagation (LRP), a recently developed technique for explaining predictions of complex non-linear classifiers. We train two word-based ML models, a convolutional neural network (CNN) and a bag-of-words SVM classifier, on a topic categorization task and adapt the LRP method to decompose the predictions of these models onto words. Resulting scores indicate how much individual words contribute to the overall classification decision. This enables one to distill relevant information from text documents without an explicit semantic information extraction step. We further use the word-wise relevance scores for generating novel vector-based document representations which capture semantic information. Based on these document vectors, we introduce a measure of model explanatory power and show that, although the SVM and CNN models perform similarly in terms of classification accuracy, the latter exhibits a higher level of explainability which makes it more comprehensible for humans and potentially more useful for other applications.},
archivePrefix = {arXiv},
arxivId = {1612.07843},
author = {Arras, Leila and Horn, Franziska and Montavon, Gr{\'{e}}goire and M{\"{u}}ller, Klaus-Robert and Samek, Wojciech},
eprint = {1612.07843},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Arras et al. - 2016 - What is Relevant in a Text Document.pdf:pdf},
isbn = {1111111111},
journal = {PLoS One},
number = {8},
pages = {1--19},
title = {{What is Relevant in a Text Document?}},
url = {http://arxiv.org/abs/1612.07843},
volume = {12},
year = {2016}
}
@article{Karimi2019,
abstract = {Predictive models are being increasingly used to support consequential decision making at the individual level in contexts such as pretrial bail and loan approval. As a result, there is increasing social and legal pressure to provide explanations that help the affected individuals not only to understand why a prediction was output, but also how to act to obtain a desired outcome. To this end, several works have proposed optimization-based methods to generate nearest counterfactual explanations. However, these methods are often restricted to a particular subset of models (e.g., decision trees or linear models) and differentiable distance functions. In contrast, we build on standard theory and tools from formal verification and propose a novel algorithm that solves a sequence of satisfiability problems, where both the distance function (objective) and predictive model (constraints) are represented as logic formulae. As shown by our experiments on real-world data, our algorithm is: i) model-agnostic ({non-}linear, {non-}differentiable, {non-}convex); ii) data-type-agnostic (heterogeneous features); iii) distance-agnostic ($\ell_0, \ell_1, \ell_\infty$, and combinations thereof); iv) able to generate plausible and diverse counterfactuals for any sample (i.e., 100% coverage); and v) at provably optimal distances.},
archivePrefix = {arXiv},
arxivId = {1905.11190},
author = {Karimi, Amir-Hossein and Barthe, Gilles and Balle, Borja and Valera, Isabel},
eprint = {1905.11190},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Karimi et al. - 2019 - Model-Agnostic Counterfactual Explanations for Consequential Decisions(2).pdf:pdf},
title = {{Model-Agnostic Counterfactual Explanations for Consequential Decisions}},
url = {http://arxiv.org/abs/1905.11190},
volume = {108},
year = {2019}
}
@article{Verma2020,
abstract = {Machine learning plays a role in many deployed decision systems, often in ways that are difficult or impossible to understand by human stakeholders. Explaining, in a human-understandable way, the relationship between the input and output of machine learning models is essential to the development of trustworthy machine-learning-based systems. A burgeoning body of research seeks to define the goals and methods of explainability in machine learning. In this paper, we seek to review and categorize research on counterfactual explanations, a specific class of explanation that provides a link between what could have happened had input to a model been changed in a particular way. Modern approaches to counterfactual explainability in machine learning draw connections to the established legal doctrine in many countries, making them appealing to fielded systems in high-impact areas such as finance and healthcare. Thus, we design a rubric with desirable properties of counterfactual explanation algorithms and comprehensively evaluate all currently-proposed algorithms against that rubric. Our rubric provides easy comparison and comprehension of the advantages and disadvantages of different approaches and serves as an introduction to major research themes in this field. We also identify gaps and discuss promising research directions in the space of counterfactual explainability.},
archivePrefix = {arXiv},
arxivId = {2010.10596},
author = {Verma, Sahil and Dickerson, John and Hines, Keegan},
eprint = {2010.10596},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Verma, Dickerson, Hines - 2020 - Counterfactual Explanations for Machine Learning A Review(2).pdf:pdf},
title = {{Counterfactual Explanations for Machine Learning: A Review}},
url = {http://arxiv.org/abs/2010.10596},
year = {2020}
}
@article{Zhou2020,
abstract = {Recently generating natural language explanations has shown very promising results in not only offering interpretable explanations but also providing additional information and supervision for prediction. However, existing approaches usually require a large set of human annotated explanations for training while collecting a large set of explanations is not only time consuming but also expensive. In this paper, we develop a general framework for interpretable natural language understanding that requires only a small set of human annotated explanations for training. Our framework treats natural language explanations as latent variables that model the underlying reasoning process of a neural model. We develop a variational EM framework for optimization where an explanation generation module and an explanation-augmented prediction module are alternatively optimized and mutually enhance each other. Moreover, we further propose an explanation-based self-training method under this framework for semi-supervised learning. It alternates between assigning pseudo-labels to unlabeled data and generating new explanations to iteratively improve each other. Experiments on two natural language understanding tasks demonstrate that our framework can not only make effective predictions in both supervised and semi-supervised settings, but also generate good natural language explanation.},
archivePrefix = {arXiv},
arxivId = {2011.05268},
author = {Zhou, Wangchunshu and Hu, Jinyi and Zhang, Hanlin and Liang, Xiaodan and Sun, Maosong and Xiong, Chenyan and Tang, Jian},
eprint = {2011.05268},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhou et al. - 2020 - Towards Interpretable Natural Language Understanding with Explanations as Latent Variables.pdf:pdf},
number = {NeurIPS},
pages = {1--16},
title = {{Towards Interpretable Natural Language Understanding with Explanations as Latent Variables}},
url = {http://arxiv.org/abs/2011.05268},
year = {2020}
}
@article{Mahendran2015,
abstract = {Image representations, from SIFT and Bag of Visual Words to Convolutional Neural Networks (CNNs), are a crucial component of almost any image understanding system. Nevertheless, our understanding of them remains limited. In this paper we conduct a direct analysis of the visual information contained in representations by asking the following question: given an encoding of an image, to which extent is it possible to reconstruct the image itself? To answer this question we contribute a general framework to invert representations. We show that this method can invert representations such as HOG more accurately than recent alternatives while being applicable to CNNs too. We then use this technique to study the inverse of recent state-of-the-art CNN image representations for the first time. Among our findings, we show that several layers in CNNs retain photographically accurate information about the image, with different degrees of geometric and photometric invariance.},
archivePrefix = {arXiv},
arxivId = {1412.0035},
author = {Mahendran, Aravindh and Vedaldi, Andrea},
doi = {10.1109/CVPR.2015.7299155},
eprint = {1412.0035},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mahendran, Vedaldi - 2015 - Understanding deep image representations by inverting them.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {5188--5196},
title = {{Understanding deep image representations by inverting them}},
volume = {07-12-June},
year = {2015}
}
@article{Dong2017,
abstract = {Interpretability of deep neural networks (DNNs) is essential since it enables users to understand the overall strengths and weaknesses of the models, conveys an understanding of how the models will behave in the future, and how to diagnose and correct potential problems. However, it is challenging to reason about what a DNN actually does due to its opaque or black-box nature. To address this issue, we propose a novel technique to improve the interpretability of DNNs by leveraging the rich semantic information embedded in human descriptions. By concentrating on the video captioning task, we first extract a set of semantically meaningful topics from the human descriptions that cover a wide range of visual concepts, and integrate them into the model with an interpretive loss. We then propose a prediction difference maximization algorithm to interpret the learned features of each neuron. Experimental results demonstrate its effectiveness in video captioning using the interpretable features, which can also be transferred to video action recognition. By clearly understanding the learned features, users can easily revise false predictions via a human-in-the-loop procedure.},
archivePrefix = {arXiv},
arxivId = {1703.04096},
author = {Dong, Yinpeng and Su, Hang and Zhu, Jun and Zhang, Bo},
doi = {10.1109/CVPR.2017.110},
eprint = {1703.04096},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dong et al. - 2017 - Improving interpretability of deep neural networks with semantic information(2).pdf:pdf},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {975--983},
title = {{Improving interpretability of deep neural networks with semantic information}},
volume = {2017-Janua},
year = {2017}
}
@article{Montavon2016,
abstract = {We summarize the main concepts behind a recently proposed method for explaining neural network predictions called deep Taylor decomposition. For conciseness, we only present the case of simple neural networks of ReLU neurons organized in a directed acyclic graph. More struc- tured networks with special layers are discussed in the original paper (Montavon et al., 2015).},
author = {Montavon, Gr{\'{e}}goire and Bach, Sebastian and Binder, Alexander and Samek, Wojciech and M{\"{u}}ller, Klaus-Robert},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Montavon et al. - 2016 - Deep Taylor Decomposition of Neural Networks.pdf:pdf},
journal = {ICML'16 Workshop on Visualization for Deep Learning},
pages = {1--3},
title = {{Deep Taylor Decomposition of Neural Networks}},
year = {2016}
}
@article{Chen2020b,
abstract = {While machine learning techniques have been successfully applied in several fields, the black-box nature of the models presents challenges for interpreting and explaining the results. We develop a new framework called Adaptive Explainable Neural Networks (AxNN) for achieving the dual goals of good predictive performance and model interpretability. For predictive performance, we build a structured neural network made up of ensembles of generalized additive model networks and additive index models (through explainable neural networks) using a two-stage process. This can be done using either a boosting or a stacking ensemble. For interpretability, we show how to decompose the results of AxNN into main effects and higher-order interaction effects. The computations are inherited from Google's open source tool AdaNet and can be efficiently accelerated by training with distributed computing. The results are illustrated on simulated and real datasets.},
author = {Chen, Jie and Vaughan, Joel and Nair, Vijay and Sudjianto, Agus},
doi = {10.2139/ssrn.3569318},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2020 - Adaptive Explainable Neural Networks (Axnns).pdf:pdf},
issn = {1556-5068},
journal = {SSRN Electronic Journal},
keywords = {abbreviations used in the,additive index models,boosting,generalized additive models,interpretable machine,learning,main effects and interactions,paper,stacking},
pages = {1--22},
title = {{Adaptive Explainable Neural Networks (Axnns)}},
year = {2020}
}
@article{Guidotti2018,
abstract = {In the last years many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness sometimes at the cost of scarifying accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, delineating explicitly or implicitly its own definition of interpretability and explanation. The aim of this paper is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.},
archivePrefix = {arXiv},
arxivId = {1802.01933},
author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Pedreschi, Dino and Giannotti, Fosca},
eprint = {1802.01933},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guidotti et al. - 2018 - A survey of methods for explaining black box models.pdf:pdf},
issn = {23318422},
journal = {arXiv},
keywords = {Explanations,Interpretability,Open the black box,Transparent models},
pages = {1--45},
title = {{A survey of methods for explaining black box models}},
year = {2018}
}
@article{Lundberg2019,
abstract = {Tree-based machine learning models such as random forests, decision trees, and gradient boosted trees are the most popular non-linear predictive models used in practice today, yet comparatively little attention has been paid to explaining their predictions. Here we significantly improve the interpretability of tree-based models through three main contributions: 1) The first polynomial time algorithm to compute optimal explanations based on game theory. 2) A new type of explanation that directly measures local feature interaction effects. 3) A new set of tools for understanding global model structure based on combining many local explanations of each prediction. We apply these tools to three medical machine learning problems and show how combining many high-quality local explanations allows us to represent global structure while retaining local faithfulness to the original model. These tools enable us to i) identify high magnitude but low frequency non-linear mortality risk factors in the general US population, ii) highlight distinct population sub-groups with shared risk characteristics, iii) identify non-linear interaction effects among risk factors for chronic kidney disease, and iv) monitor a machine learning model deployed in a hospital by identifying which features are degrading the model's performance over time. Given the popularity of tree-based machine learning models, these improvements to their interpretability have implications across a broad set of domains.},
archivePrefix = {arXiv},
arxivId = {1905.04610},
author = {Lundberg, Scott M. and Erion, Gabriel and Chen, Hugh and DeGrave, Alex and Prutkin, Jordan M. and Nair, Bala and Katz, Ronit and Himmelfarb, Jonathan and Bansal, Nisha and Lee, Su In},
eprint = {1905.04610},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lundberg et al. - 2019 - Explainable AI for trees From local explanations to global understanding.pdf:pdf},
issn = {23318422},
journal = {arXiv},
pages = {1--72},
title = {{Explainable AI for trees: From local explanations to global understanding}},
year = {2019}
}
@article{Zafar2019,
abstract = {Local Interpretable Model-Agnostic Explanations (LIME) is a popular technique used to increase the interpretability and explainability of black box Machine Learning (ML) algorithms. LIME typically generates an explanation for a single prediction by any ML model by learning a simpler interpretable model (e.g. linear classifier) around the prediction through generating simulated data around the instance by random perturbation, and obtaining feature importance through applying some form of feature selection. While LIME and similar local algorithms have gained popularity due to their simplicity, the random perturbation and feature selection methods result in instability in the generated explanations, where for the same prediction, different explanations can be generated. This is a critical issue that can prevent deployment of LIME in a Computer-Aided Diagnosis (CAD) system, where stability is of utmost importance to earn the trust of medical professionals. In this paper, we propose a deterministic version of LIME. Instead of random perturbation, we utilize agglomerative Hierarchical Clustering (HC) to group the training data together and K-Nearest Neighbour (KNN) to select the relevant cluster of the new instance that is being explained. After finding the relevant cluster, a linear model is trained over the selected cluster to generate the explanations. Experimental results on three different medical datasets show the superiority for Deterministic Local Interpretable Model-Agnostic Explanations (DLIME), where we quantitatively determine the stability of DLIME compared to LIME utilizing the Jaccard similarity among multiple generated explanations.},
archivePrefix = {arXiv},
arxivId = {1906.10263},
author = {Zafar, Muhammad Rehman and Khan, Naimul Mefraz},
eprint = {1906.10263},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zafar, Khan - 2019 - DLIME A Deterministic Local Interpretable Model-Agnostic Explanations approach for Computer-Aided Diagnosis Systems.pdf:pdf},
issn = {23318422},
journal = {arXiv},
keywords = {Deterministic,Explainable AI (XAI),Explanation,Healthcare,Interpretable Machine Learning,LIME,Model Agnostic},
title = {{DLIME: A Deterministic Local Interpretable Model-Agnostic Explanations approach for Computer-Aided Diagnosis Systems}},
year = {2019}
}
@article{Nemirovsky2020,
abstract = {The prevalence of machine learning models in various industries has led to growing demands for model interpretability and for the ability to provide meaningful recourse to users. For example, patients hoping to improve their diagnoses or loan applicants seeking to increase their chances of approval. Counterfactuals can help in this regard by identifying input perturbations that would result in more desirable prediction outcomes. Meaningful counterfactuals should be able to achieve the desired outcome, but also be realistic, actionable, and efficient to compute. Current approaches achieve desired outcomes with moderate actionability but are severely limited in terms of realism and latency. To tackle these limitations, we apply Generative Adversarial Nets (GANs) toward counterfactual search. We also introduce a novel Residual GAN (RGAN) that helps to improve counterfactual realism and actionability compared to regular GANs. The proposed CounteRGAN method utilizes an RGAN and a target classifier to produce counterfactuals capable of providing meaningful recourse. Evaluations on two popular datasets highlight how the CounteRGAN is able to overcome the limitations of existing methods, including latency improvements of >50x to >90,000x, making meaningful recourse available in real-time and applicable to a wide range of domains.},
archivePrefix = {arXiv},
arxivId = {2009.05199},
author = {Nemirovsky, Daniel and Thiebaut, Nicolas and Xu, Ye and Gupta, Abhishek},
eprint = {2009.05199},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nemirovsky et al. - 2020 - CounteRGAN Generating Realistic Counterfactuals with Residual Generative Adversarial Nets.pdf:pdf},
title = {{CounteRGAN: Generating Realistic Counterfactuals with Residual Generative Adversarial Nets}},
url = {http://arxiv.org/abs/2009.05199},
year = {2020}
}
@article{Lipton2018,
archivePrefix = {arXiv},
arxivId = {1606.03490},
author = {Lipton, Zachary C.},
doi = {10.1145/3233231},
eprint = {1606.03490},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lipton - 2018 - The mythos of model interpretability.pdf:pdf},
issn = {15577317},
journal = {Communications of the ACM},
number = {10},
pages = {35--43},
title = {{The mythos of model interpretability}},
volume = {61},
year = {2018}
}
@article{Capra2020a,
abstract = {Currently, Machine Learning (ML) is becoming ubiquitous in everyday life. Deep Learning (DL) is already present in many applications ranging from computer vision for medicine to autonomous driving of modern cars as well as other sectors in security, healthcare, and finance. However, to achieve impressive performance, these algorithms employ very deep networks, requiring a significant computational power, both during the training and inference time. A single inference of a DL model may require billions of multiply-and-accumulated operations, making the DL extremely compute- and energy-hungry. In a scenario where several sophisticated algorithms need to be executed with limited energy and low latency, the need for cost-effective hardware platforms capable of implementing energy-efficient DL execution arises. This paper first introduces the key properties of two brain-inspired models like Deep Neural Network (DNN), and Spiking Neural Network (SNN), and then analyzes techniques to produce efficient and high-performance designs. This work summarizes and compares the works for four leading platforms for the execution of algorithms such as CPU, GPU, FPGA and ASIC describing the main solutions of the state-of-the-art, giving much prominence to the last two solutions since they offer greater design flexibility and bear the potential of high energy-efficiency, especially for the inference process. In addition to hardware solutions, this paper discusses some of the important security issues that these DNN and SNN models may have during their execution, and offers a comprehensive section on benchmarking, explaining how to assess the quality of different networks and hardware systems designed for them.},
archivePrefix = {arXiv},
arxivId = {2012.11233},
author = {Capra, Maurizio and Bussolino, Beatrice and Marchisio, Alberto and Masera, Guido and Martina, Maurizio and Shafique, Muhammad},
doi = {10.1109/ACCESS.2020.3039858},
eprint = {2012.11233},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Capra et al. - 2020 - Hardware and Software Optimizations for Accelerating Deep Neural Networks Survey of Current Trends, Challenges, an.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {AI,CNNs,DNNs,ML,Machine learning,VLSI,adversarial attacks,area,artificial intelligence,capsule networks,computer architecture,convolutional neural networks,data flow,deep learning,deep neural networks,efficiency,energy,hardware accelerator,latency,optimization,performance,power consumption,spiking neural networks},
number = {Ml},
pages = {225134--225180},
title = {{Hardware and Software Optimizations for Accelerating Deep Neural Networks: Survey of Current Trends, Challenges, and the Road Ahead}},
volume = {8},
year = {2020}
}
@article{Robnik-Sikonja2018,
abstract = {State-of-the-art prediction models are getting increasingly complex and incomprehensible for humans. This is problematic for many application areas, especially those where knowledge discovery is just as important as predictive performance, for example medicine or business consulting. As machine learning and artificial intelligence are playing an increasingly large role in the society through data based decision making, this is problematic also from broader perspective and worries general public as well as legislators. As a possible solution, several explanation methods have been recently proposed, which can explain predictions of otherwise opaque models. These methods can be divided into two main approaches: gradient based approaches limited to neural networks, and more general perturbation based approaches, which can be used with arbitrary prediction models. We present an overview of perturbation based approaches, and focus on a recently introduced implementation of two successful methods developed in Slovenia, EXPLAIN and IME. We first describe their working principles and visualizations of explanations, followed by the implementation in ExplainPrediction package for R environment.},
author = {Robnik-{\v{S}}ikonja, Marko},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Robnik-{\v{S}}ikonja - 2018 - Explanation of prediction models with explain prediction.pdf:pdf},
issn = {03505596},
journal = {Informatica (Slovenia)},
keywords = {Comprehensibility of models,Explanation of models,Machine learning,Perturbation methods},
number = {1},
pages = {13--22},
title = {{Explanation of prediction models with explain prediction}},
volume = {42},
year = {2018}
}
@article{OShaughnessy2020,
abstract = {We develop a method for generating causal post-hoc explanations of black-box classifiers based on a learned low-dimensional representation of the data. The explanation is causal in the sense that changing learned latent factors produces a change in the classifier output statistics. To construct these explanations, we design a learning framework that leverages a generative model and information-theoretic measures of causal influence. Our objective function encourages both the generative model to faithfully represent the data distribution and the latent factors to have a large causal influence on the classifier output. Our method learns both global and local explanations, is compatible with any classifier that admits class probabilities and a gradient, and does not require labeled attributes or knowledge of causal structure. Using carefully controlled test cases, we provide intuition that illuminates the function of our objective. We then demonstrate the practical utility of our method on image recognition tasks.},
archivePrefix = {arXiv},
arxivId = {2006.13913},
author = {O'Shaughnessy, Matthew and Canal, Gregory and Connor, Marissa and Davenport, Mark and Rozell, Christopher},
eprint = {2006.13913},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/O'Shaughnessy et al. - 2020 - Generative causal explanations of black-box classifiers(2).pdf:pdf},
number = {NeurIPS},
pages = {1--34},
title = {{Generative causal explanations of black-box classifiers}},
url = {http://arxiv.org/abs/2006.13913},
volume = {1},
year = {2020}
}
@article{Lakkaraju2019,
abstract = {As predictive models increasingly assist human experts (e.g., doctors) in day-to-day decision making, it is crucial for experts to be able to explore and understand how such models behave in different feature subspaces in order to know if and when to trust them. To this end, we propose Model Understanding through Subspace Explanations (MUSE), a novel model agnostic framework which facilitates understanding of a given black box model by explaining how it behaves in subspaces characterized by certain features of interest. Our framework provides end users (e.g., doctors) with the flexibility of customizing the model explanations by allowing them to input the features of interest. The construction of explanations is guided by a novel objective function that we propose to simultaneously optimize for fidelity to the original model, unambiguity and interpretability of the explanation. More specifically, our objective allows us to learn, with optimality guarantees, a small number of compact decision sets each of which captures the behavior of a given black box model in unambiguous, well-defined regions of the feature space. Experimental evaluation with real-world datasets and user studies demonstrate that our approach can generate customizable, highly compact, easy-to-understand, yet accurate explanations of various kinds of predictive models compared to state-of-the-art baselines.},
author = {Lakkaraju, Himabindu and Caruana, Rich and Kamar, Ece and Leskovec, Jure},
doi = {10.1145/3306618.3314229},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lakkaraju et al. - 2019 - Faithful and customizable explanations of black box models.pdf:pdf},
isbn = {9781450363242},
journal = {AIES 2019 - Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
keywords = {Black box models,Decision making,Interpretable machine learning},
pages = {131--138},
title = {{Faithful and customizable explanations of black box models}},
year = {2019}
}
@article{Janizek2021,
abstract = {Recent work has shown great promise in explaining neural network behavior. In particular, feature attribution methods explain the features that are important to a model's prediction on a given input. However, for many tasks, simply identifying significant features may be insuficient for understanding model behavior. The interactions between features within the model may better explain not only the model, but why certain features outrank others in importance. In this work, we present Integrated Hessians, an extension of Integrated Gradients (Sundararajan et al., 2017) that explains pairwise feature interactions in neural networks. Integrated Hessians overcomes several theoretical limitations of previous methods, and unlike them, is not limited to a specific architecture or class of neural network. Additionally, we find that our method is faster than existing methods when the number of features is large, and outperforms previous methods on existing quantitative benchmarks.},
archivePrefix = {arXiv},
arxivId = {2002.04138},
author = {Janizek, Joseph D. and Sturmfels, Pascal and Lee, Su In},
eprint = {2002.04138},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Janizek, Sturmfels, Lee - 2021 - Explaining explanations Axiomatic feature interactions for deep networks.pdf:pdf},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Aumann-Shapley value,Feature attribution,Feature interaction,Interpretability,Neural networks},
title = {{Explaining explanations: Axiomatic feature interactions for deep networks}},
volume = {22},
year = {2021}
}
@article{LeonSixt2021,
abstract = {Current state of the art computer vision applications rely on highly complex models. Their interpretability is mostly limited to post-hoc methods which are not guaranteed to be faithful to the model. To elucidate a model's decision, we present a novel interpretable model based on an invertible deep convolutional network. Our model generates meaningful, faithful, and ideal counterfactuals. Using PCA on the classifier's input, we can also create "isofactuals"-image interpolations with the same outcome but visually meaningful different features. Counter-and isofactuals can be used to identify positive and negative evidence in an image. This can also be visualized with heatmaps. We evaluate our approach against gradient-based at-tribution methods, which we find to produce meaningless adversarial perturbations. Using our method, we reveal biases in three different datasets. In a human subject experiment, we test whether non-experts find our method useful to spot spurious correlations learned by a model. Our work is a step towards more trustworthy explanations for computer vision. For code and datasets see 1 .},
author = {{Leon Sixt} and $\sim$Leon_Sixt1 and {, Martin Schuessler, Philipp Wei{\ss}}, Tim Landgraf},
journal = {Not accepted - ICLR},
number = {2018},
pages = {1--20},
title = {{Interpretability Through Invertibility: A Deep Convolutional Network With Ideal Counterfactuals And Isosurfaces}},
url = {https://paperswithcode.com/paper/interpretability-through-invertibility-a-deep},
year = {2021}
}
@article{Lakkaraju2017,
abstract = {We propose Black Box Explanations through Transparent Approximations (BETA), a novel model agnostic framework for explaining the behavior of any black-box classifier by simultaneously optimizing for fidelity to the original model and interpretability of the explanation. To this end, we develop a novel objective function which allows us to learn (with optimality guarantees), a small number of compact decision sets each of which explains the behavior of the black box model in unambiguous, well-defined regions of feature space. Furthermore, our framework also is capable of accepting user input when generating these approximations, thus allowing users to interactively explore how the black-box model behaves in different subspaces that are of interest to the user. To the best of our knowledge, this is the first approach which can produce global explanations of the behavior of any given black box model through joint optimization of unambiguity, fidelity, and interpretability, while also allowing users to explore model behavior based on their preferences. Experimental evaluation with real-world datasets and user studies demonstrates that our approach can generate highly compact, easy-to-understand, yet accurate approximations of various kinds of predictive models compared to state-of-the-art baselines.},
archivePrefix = {arXiv},
arxivId = {1707.01154},
author = {Lakkaraju, Himabindu and Kamar, Ece and Caruana, Rich and Leskovec, Jure},
eprint = {1707.01154},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lakkaraju et al. - 2017 - Interpretable & Explorable Approximations of Black Box Models.pdf:pdf},
title = {{Interpretable & Explorable Approximations of Black Box Models}},
url = {http://arxiv.org/abs/1707.01154},
year = {2017}
}
@article{Manuscript2020,
abstract = {Cladding austenitic stainless steels are trendy these days in pressure vessels to enhance surface qualities. In this work, austenitic stainless steel clad layers deposited by flux cored arc welding process on structural steel plates used in boiler construction are investigated. To facilitate this, composite clad layers are produced with 316L stainless steel deposits on low carbon structural steel plates based on the design of experiments. Results exhibit an incremental trend of thermal conductivity with respect to percentage of weld dilution. A linear mathematical relationship is established between the percentage of weld dilution and post-weld thermal conductivity of clad layers for the prediction of thermal conductivity of clad layer deposits. These results could be supportive in the fabrication industries for producing energy efficient thermal equipment},
author = {Manuscript, Accepted},
title = {{Ac ce us}},
year = {2020}
}
@article{Gurumoorthy2019,
abstract = {Prototypical examples that best summarize and compactly represent an underlying complex data distribution, communicate meaningful insights to humans in domains where simple explanations are hard to extract. In this paper, we present algorithms with strong theoretical guarantees to mine these data sets and select prototypes, a.k.a. representatives that optimally describes them. Our work notably generalizes the recent work by Kim et al. (2016) where in addition to selecting prototypes, we also associate non-negative weights which are indicative of their importance. This extension provides a single coherent framework under which both prototypes and criticisms (i.e. outliers) can be found. Furthermore, our framework works for any symmetric positive definite kernel thus addressing one of the key open questions laid out in Kim et al. (2016). By establishing that our objective function enjoys a key property of that of weak submodularity, we present a fast ProtoDash algorithm and also derive approximation guarantees for the same. We demonstrate the efficacy of our method on diverse domains such as retail, digit recognition (MNIST) and on publicly available 40 health questionnaires obtained from the Center for Disease Control (CDC) website maintained by the US Dept. of Health. We validate the results quantitatively as well as qualitatively based on expert feedback and recently published scientific studies on public health, thus showcasing the power of our technique in providing actionability (for retail), utility (for MNIST), and insight (on CDC datasets), which arguably are the hallmarks of an effective interpretable machine learning method.},
archivePrefix = {arXiv},
arxivId = {1707.01212},
author = {Gurumoorthy, Karthik S. and Dhurandhar, Amit and Cecchi, Guillermo and Aggarwal, Charu},
doi = {10.1109/ICDM.2019.00036},
eprint = {1707.01212},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gurumoorthy et al. - 2019 - Efficient data representation by selecting prototypes with importance weights.pdf:pdf},
isbn = {9781728146034},
issn = {15504786},
journal = {Proceedings - IEEE International Conference on Data Mining, ICDM},
keywords = {Data summarization,Outlier detection,Prototype selection,Submodularity},
pages = {260--269},
title = {{Efficient data representation by selecting prototypes with importance weights}},
volume = {2019-Novem},
year = {2019}
}
@article{Li2018,
abstract = {Deep neural networks are widely used for classification. These deep models often suffer from a lack of interpretability - they are particularly difficult to understand because of their non-linear nature. As a result, neural networks are often treated as “black box” models, and in the past, have been trained purely to optimize the accuracy of predictions. In this work, we create a novel network architecture for deep learning that naturally explains its own reasoning for each prediction. This architecture contains an autoencoder and a special prototype layer, where each unit of that layer stores a weight vector that resembles an encoded training input. The encoder of the autoencoder allows us to do comparisons within the latent space, while the decoder allows us to visualize the learned prototypes. The training objective has four terms: an accuracy term, a term that encourages every prototype to be similar to at least one encoded input, a term that encourages every encoded input to be close to at least one prototype, and a term that encourages faithful reconstruction by the autoencoder. The distances computed in the prototype layer are used as part of the classification process. Since the prototypes are learned during training, the learned network naturally comes with explanations for each prediction, and the explanations are loyal to what the network actually computes.},
archivePrefix = {arXiv},
arxivId = {1710.04806},
author = {Li, Oscar and Liu, Hao and Chen, Chaofan and Rudin, Cynthia},
eprint = {1710.04806},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2018 - Deep learning for case-based reasoning through prototypes A neural network that explains its predictions.pdf:pdf},
isbn = {9781577358008},
journal = {32nd AAAI Conference on Artificial Intelligence, AAAI 2018},
pages = {3530--3537},
title = {{Deep learning for case-based reasoning through prototypes: A neural network that explains its predictions}},
year = {2018}
}
@article{Sze2017,
abstract = {Deep neural networks (DNNs) are currently widely used for many artificial intelligence (AI) applications including computer vision, speech recognition, and robotics. While DNNs deliver state-of-the-art accuracy on many AI tasks, it comes at the cost of high computational complexity. Accordingly, techniques that enable efficient processing of DNNs to improve energy efficiency and throughput without sacrificing application accuracy or increasing hardware cost are critical to the wide deployment of DNNs in AI systems. This article aims to provide a comprehensive tutorial and survey about the recent advances toward the goal of enabling efficient processing of DNNs. Specifically, it will provide an overview of DNNs, discuss various hardware platforms and architectures that support DNNs, and highlight key trends in reducing the computation cost of DNNs either solely via hardware design changes or via joint hardware design and DNN algorithm changes. It will also summarize various development resources that enable researchers and practitioners to quickly get started in this field, and highlight important benchmarking metrics and design considerations that should be used for evaluating the rapidly growing number of DNN hardware designs, optionally including algorithmic codesigns, being proposed in academia and industry. The reader will take away the following concepts from this article: understand the key design considerations for DNNs; be able to evaluate different DNN hardware implementations with benchmarks and comparison metrics; understand the tradeoffs between various hardware architectures and platforms; be able to evaluate the utility of various DNN design techniques for efficient processing; and understand recent implementation trends and opportunities.},
archivePrefix = {arXiv},
arxivId = {1703.09039},
author = {Sze, Vivienne and Chen, Yu Hsin and Yang, Tien Ju and Emer, Joel S.},
doi = {10.1109/JPROC.2017.2761740},
eprint = {1703.09039},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sze et al. - 2017 - Efficient Processing of Deep Neural Networks A Tutorial and Survey.pdf:pdf},
issn = {15582256},
journal = {Proceedings of the IEEE},
keywords = {ASIC,VLSI,computer architecture,convolutional neural networks,dataflow processing,deep learning,deep neural networks,energy-efficient accelerators,low power,machine learning,spatial architectures},
number = {12},
pages = {2295--2329},
title = {{Efficient Processing of Deep Neural Networks: A Tutorial and Survey}},
volume = {105},
year = {2017}
}
@article{Guo2018,
abstract = {Current visualization based network interpretation methodssuffer from lacking semantic-level information. In this paper, we introduce the novel task of interpreting classification models using fine grained textual summarization. Along with the label prediction, the network will generate a sentence explaining its decision. Constructing a fully annotated dataset of filter|text pairs is unrealistic because of image to filter response function complexity. We instead propose a weakly-supervised learning algorithm leveraging off-the-shelf image caption annotations. Central to our algorithm is the filter-level attribute probability density function (p.d.f.), learned as a conditional probability through Bayesian inference with the input image and its feature map as latent variables. We show our algorithm faithfully reflects the features learned by the model using rigorous applications like attribute based image retrieval and unsupervised text grounding. We further show that the textual summarization process can help in understanding network failure patterns and can provide clues for further improvements.},
archivePrefix = {arXiv},
arxivId = {1805.08969},
author = {Guo, Pei and Anderson, Connor and Pearson, Kolten and Farrell, Ryan},
eprint = {1805.08969},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guo et al. - 2018 - Neural Network Interpretation via Fine Grained Textual Summarization.pdf:pdf},
title = {{Neural Network Interpretation via Fine Grained Textual Summarization}},
url = {http://arxiv.org/abs/1805.08969},
year = {2018}
}
@article{Manuscript2020,
abstract = {Cladding austenitic stainless steels are trendy these days in pressure vessels to enhance surface qualities. In this work, austenitic stainless steel clad layers deposited by flux cored arc welding process on structural steel plates used in boiler construction are investigated. To facilitate this, composite clad layers are produced with 316L stainless steel deposits on low carbon structural steel plates based on the design of experiments. Results exhibit an incremental trend of thermal conductivity with respect to percentage of weld dilution. A linear mathematical relationship is established between the percentage of weld dilution and post-weld thermal conductivity of clad layers for the prediction of thermal conductivity of clad layer deposits. These results could be supportive in the fabrication industries for producing energy efficient thermal equipment},
author = {Manuscript, Accepted},
title = {{Ac ce us}},
year = {2020}
}
@inproceedings{Pollack2020,
author = {Pollack, Brian and Chen, Junxiang},
booktitle = {ICLR International Conference on Learning Representations},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pollack, Chen - 2020 - EXPLANATION BY PROGRESSIVE EXAGGERATION.pdf:pdf},
pages = {1--20},
title = {{EXPLANATION BY PROGRESSIVE EXAGGERATION}},
year = {2020}
}
@article{Yuan2020b,
abstract = {Deep models are commonly treated as black-boxes and lack interpretability. Here, we propose a novel approach to interpret deep image classifiers by generating discrete masks. Our method follows the generative adversarial network formalism. The deep model to be interpreted is the discriminator while we train a generator to explain it. The generator is trained to capture discriminative image regions that should convey the same or similar meaning as the original image from the model's perspective. It produces a probability map from which a discrete mask can be sampled. Then the discriminator is used to measure the quality of the sampled mask and provide feedbacks for updating. Due to the sampling operations, the generator cannot be trained directly by back-propagation. We propose to update it using policy gradient. Furthermore, we propose to incorporate gradients as auxiliary information to reduce the search space and facilitate training. We conduct both quantitative and qualitative experiments on the ILSVRC dataset. Experimental results indicate that our method can provide reasonable explanations for predictions and outperform existing approaches. In addition, our method can pass the model randomization test, indicating that it is reasoning the attribution of network predictions.},
author = {Yuan, Hao and Cai, Lei and Hu, Xia and Wang, Jie and Ji, Shuiwang},
doi = {10.1109/tpami.2020.3028783},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yuan et al. - 2020 - Interpreting Image Classifiers by Generating Discrete Masks.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {c},
pages = {1--1},
title = {{Interpreting Image Classifiers by Generating Discrete Masks}},
volume = {8828},
year = {2020}
}
@article{Manuscript2020,
abstract = {Cladding austenitic stainless steels are trendy these days in pressure vessels to enhance surface qualities. In this work, austenitic stainless steel clad layers deposited by flux cored arc welding process on structural steel plates used in boiler construction are investigated. To facilitate this, composite clad layers are produced with 316L stainless steel deposits on low carbon structural steel plates based on the design of experiments. Results exhibit an incremental trend of thermal conductivity with respect to percentage of weld dilution. A linear mathematical relationship is established between the percentage of weld dilution and post-weld thermal conductivity of clad layers for the prediction of thermal conductivity of clad layer deposits. These results could be supportive in the fabrication industries for producing energy efficient thermal equipment},
author = {Manuscript, Accepted},
title = {{Ac ce us}},
year = {2020}
}
@article{Fu2020,
abstract = {To have a better understanding and usage of Convolution Neural Networks (CNNs), the visualization and interpretation of CNNs has attracted increasing attention in recent years. In particular, several Class Activation Mapping (CAM) methods have been proposed to discover the connection between CNN's decision and image regions. However, in spite of the reasonable visualization, most of these methods lack clear and sufficient theoretical support. In this paper, we introduce two axioms – Sensitivity and Conservation – to the visualization paradigm of the CAM methods. Meanwhile, a dedicated Axiom-based Grad-CAM (XGrad-CAM) is proposed to satisfy these axioms as much as possible. Experiments demonstrate that XGrad-CAM is an enhanced version of Grad-CAM in terms of sensitivity and conservation. It is able to achieve better visualization performance than Grad-CAM, while also be class-discriminative and easy-to-implement compared with Grad-CAM++ and Ablation-CAM. Code is available at https://github.com/Fu0511/XGrad-CAM.},
archivePrefix = {arXiv},
arxivId = {2008.02312},
author = {Fu, Ruigang and Hu, Qingyong and Dong, Xiaohu and Guo, Yulan and Gao, Yinghui and Li, Biao},
eprint = {2008.02312},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fu et al. - 2020 - Axiom-based Grad-CAM Towards accurate visualization and explanation of CNNs.pdf:pdf},
issn = {23318422},
journal = {arXiv},
title = {{Axiom-based Grad-CAM: Towards accurate visualization and explanation of CNNs}},
year = {2020}
}
@article{Murdoch2017,
abstract = {Although deep learning models have proven effective at solving problems in natural language processing, the mechanism by which they come to their conclusions is often unclear. As a result, these models are generally treated as black boxes, yielding no insight of the underlying learned patterns. In this paper we consider Long Short Term Memory networks (LSTMs) and demonstrate a new approach for tracking the importance of a given input to the LSTM for a given output. By identifying consistently important patterns of words, we are able to distill state of the art LSTMs on sentiment analysis and question answering into a set of representative phrases. This representation is then quantitatively validated by using the extracted phrases to construct a simple, rule-based classifier which approximates the output of the LSTM.},
archivePrefix = {arXiv},
arxivId = {1702.02540},
author = {Murdoch, W. James and Szlam, Arthur},
eprint = {1702.02540},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Murdoch, Szlam - 2017 - Automatic rule extraction from long short term memory networks.pdf:pdf},
issn = {23318422},
journal = {arXiv},
number = {2016},
pages = {1--12},
title = {{Automatic rule extraction from long short term memory networks}},
year = {2017}
}
@article{Kanehira2019a,
abstract = {This paper addresses the generation of explanations with visual examples. Given an input sample, we build a system that not only classifies it to a specific category, but also outputs linguistic explanations and a set of visual examples that render the decision interpretable. Focusing especially on the complementarity of the multimodal information, i.e., linguistic and visual examples, we attempt to achieve it by maximizing the interaction information, which provides a natural definition of complementarity from an information theoretical viewpoint. We propose a novel framework to generate complemental explanations, on which the joint distribution of the variables to explain, and those to be explained is parameterized by three different neural networks: predictor, linguistic explainer, and example selector. Explanation models are trained collaboratively to maximize the interaction information to ensure the generated explanation are complemental to each other for the target. The results of experiments conducted on several datasets demonstrate the effectiveness of the proposed method.},
archivePrefix = {arXiv},
arxivId = {1812.01280},
author = {Kanehira, Atsushi and Harada, Tatsuya},
doi = {10.1109/CVPR.2019.00880},
eprint = {1812.01280},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kanehira, Harada - 2019 - Learning to explain with complemental examples(3).pdf:pdf},
isbn = {9781728132938},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {Vision + Language,Vision Applications and Systems},
pages = {8595--8603},
title = {{Learning to explain with complemental examples}},
volume = {2019-June},
year = {2019}
}
@article{BarredoArrieta2020,
abstract = {In the last few years, Artificial Intelligence (AI) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability, an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI (namely, expert systems and rule based models). Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in this article examines the existing literature and contributions already done in the field of XAI, including a prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to define explainability in Machine Learning, establishing a novel definition of explainable Machine Learning that covers such prior conceptual propositions with a major focus on the audience for which the explainability is sought. Departing from this definition, we propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at explaining Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This critical literature analysis serves as the motivating background for a series of challenges faced by XAI, such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to the field of XAI with a thorough taxonomy that can serve as reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.},
archivePrefix = {arXiv},
arxivId = {1910.10045},
author = {{Barredo Arrieta}, Alejandro and D{\'{i}}az-Rodr{\'{i}}guez, Natalia and {Del Ser}, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garcia, Salvador and Gil-Lopez, Sergio and Molina, Daniel and Benjamins, Richard and Chatila, Raja and Herrera, Francisco},
doi = {10.1016/j.inffus.2019.12.012},
eprint = {1910.10045},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Barredo Arrieta et al. - 2020 - Explainable Explainable Artificial Intelligence (XAI) Concepts, taxonomies, opportunities and challenges.pdf:pdf},
issn = {15662535},
journal = {Information Fusion},
keywords = {Accountability,Comprehensibility,Data Fusion,Deep Learning,Explainable Artificial Intelligence,Fairness,Interpretability,Machine Learning,Privacy,Responsible Artificial Intelligence,Transparency},
number = {December 2019},
pages = {82--115},
publisher = {Elsevier B.V.},
title = {{Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI}},
volume = {58},
year = {2020}
}
@article{Selvaraju2016,
abstract = {We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing input regions that are 'important' for predictions -- or visual explanations. Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses class-specific gradient information to localize important regions. These localizations are combined with existing pixel-space visualizations to create a novel high-resolution and class-discriminative visualization called Guided Grad-CAM. These methods help better understand CNN-based models, including image captioning and visual question answering (VQA) models. We evaluate our visual explanations by measuring their ability to discriminate between classes, to inspire trust in humans, and their correlation with occlusion maps. Grad-CAM provides a new way to understand CNN-based models. We have released code, an online demo hosted on CloudCV, and a full version of this extended abstract.},
archivePrefix = {arXiv},
arxivId = {1611.07450},
author = {Selvaraju, Ramprasaath R and Das, Abhishek and Vedantam, Ramakrishna and Cogswell, Michael and Parikh, Devi and Batra, Dhruv},
eprint = {1611.07450},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Selvaraju et al. - 2016 - Grad-CAM Why did you say that.pdf:pdf},
pages = {1--4},
title = {{Grad-CAM: Why did you say that?}},
url = {http://arxiv.org/abs/1611.07450},
year = {2016}
}
@article{Ghosal2018,
abstract = {Current approaches for accurate identification, classification, and quantification of biotic and abiotic stresses in crop research and production are predominantly visual and require specialized training. However, such techniques are hindered by subjectivity resulting from inter- and intrarater cognitive variability. This translates to erroneous decisions and a significant waste of resources. Here, we demonstrate a machine learning framework's ability to identify and classify a diverse set of foliar stresses in soybean [Glycine max (L.) Merr.] with remarkable accuracy. We also present an explanation mechanism, using the top-K high-resolution feature maps that isolate the visual symptoms used to make predictions. This unsupervised identification of visual symptoms provides a quantitative measure of stress severity, allowing for identification (type of foliar stress), classification (low, medium, or high stress), and quantification (stress severity) in a single framework without detailed symptom annotation by experts. We reliably identified and classified several biotic (bacterial and fungal diseases) and abiotic (chemical injury and nutrient deficiency) stresses by learning from over 25,000 images. The learned model is robust to input image perturbations, demonstrating viability for high-throughput deployment. We also noticed that the learned model appears to be agnostic to species, seemingly demonstrating an ability of transfer learning. The availability of an explainable model that can consistently, rapidly, and accurately identify and quantify foliar stresses would have significant implications in scientific research, plant breeding, and crop production. The trained model could be deployed in mobile platforms (e.g., unmanned air vehicles and automated ground scouts) for rapid, large-scale scouting or as a mobile application for real-time detection of stress by farmers and researchers.},
author = {Ghosal, Sambuddha and Blystone, David and Singh, Asheesh K. and Ganapathysubramanian, Baskar and Singh, Arti and Sarkar, Soumik},
doi = {10.1073/pnas.1716999115},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ghosal et al. - 2018 - An explainable deep machine vision framework for plant stress phenotyping.pdf:pdf},
issn = {10916490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Explainable deep learning,Machine learning,Plant stress phenotyping,Precision agriculture,Resolving rater variabilities},
number = {18},
pages = {4613--4618},
pmid = {29666265},
title = {{An explainable deep machine vision framework for plant stress phenotyping}},
volume = {115},
year = {2018}
}
@article{Turner2016,
abstract = {We propose a general model explanation system (MES) for "explaining" the output of black box classifiers. This paper describes extensions to Turner (2015), which is referred to frequently in the text. We use the motivating example of a classifier trained to detect fraud in a credit card transaction history. The key aspect is that we provide explanations applicable to a single prediction, rather than provide an interpretable set of parameters. We focus on explaining positive predictions (alerts). However, the presented methodology is symmetrically applicable to negative predictions.},
archivePrefix = {arXiv},
arxivId = {1606.09517},
author = {Turner, Ryan},
eprint = {1606.09517},
pages = {1--5},
title = {{A Model Explanation System: Latest Updates and Extensions}},
url = {http://arxiv.org/abs/1606.09517},
year = {2016}
}
@article{Hind2019,
abstract = {Artificial intelligence systems are being increasingly deployed due to their potential to increase the efficiency, scale, consistency, fairness, and accuracy of decisions. However, as many of these systems are opaque in their operation, there is a growing demand for such systems to provide explanations for their decisions. Conventional approaches to this problem attempt to expose or discover the inner workings of a machine learning model with the hope that the resulting explanations will be meaningful to the consumer. In contrast, this paper suggests a new approach to this problem. It introduces a simple, practical framework, called Teaching Explanations for Decisions (TED), that provides meaningful explanations that match the mental model of the consumer. We illustrate the generality and effectiveness of this approach with two different examples, resulting in highly accurate explanations with no loss of prediction accuracy for these two examples.},
archivePrefix = {arXiv},
arxivId = {1811.04896},
author = {Hind, Michael and Wei, Dennis and Campbell, Murray and Codella, Noel C.F. and Dhurandhar, Amit and Mojsilovi{\'{c}}, Aleksandra and {Natesan Ramamurthy}, Karthikeyan and Varshney, Kush R.},
doi = {10.1145/3306618.3314273},
eprint = {1811.04896},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hind et al. - 2019 - TED Teaching AI to explain its decisions.pdf:pdf},
isbn = {9781450363242},
journal = {AIES 2019 - Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
keywords = {AI Ethics,Elicitation,Explainable AI,Machine Learning,Meaningful Explanation,Supervised Classification},
pages = {123--129},
title = {{TED: Teaching AI to explain its decisions}},
year = {2019}
}
@article{Sha2017,
abstract = {The increasing accumulation of healthcare data provides researchers with ample opportunities to build machine learning approaches for clinical decision support and to improve the quality of health care. Several studies have developed conventional machine learning approaches that rely heavily on manual feature engineering and result in task-specific models for health care. In contrast, healthcare researchers have begun to use deep learning, which has emerged as a revolutionary machine learning technique that obviates manual feature engineering but still achieves impressive results in research fields such as image classification. However, few of them have addressed the lack of the interpretability of deep learning models although interpretability is essential for the successful adoption of machine learning approaches by healthcare communities. In addition, the unique characteristics of healthcare data such as high dimensionality and temporal dependencies pose challenges for building models on healthcare data. To address these challenges, we develop a gated recurrent unit-based recurrent neural network with hierarchical attention for mortality prediction, and then, using the diagnostic codes from the Medical Information Mart for Intensive Care, we evaluate the model. We find that the prediction accuracy of the model outperforms baseline models and demonstrate the interpretability of the model in visualizations.},
author = {Sha, Ying and Wang, May D.},
doi = {10.1145/3107411.3107445},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sha, Wang - 2017 - Interpretable predictions of clinical outcomes with an attention-based recurrent neural network.pdf:pdf},
isbn = {9781450347228},
journal = {ACM-BCB 2017 - Proceedings of the 8th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics},
keywords = {Attention,Deep learning,Electronic health records,Health care,Interpretability,Recurrent neural networks,Visualization},
pages = {233--240},
title = {{Interpretable predictions of clinical outcomes with an attention-based recurrent neural network}},
year = {2017}
}
@article{Manuscript2020,
abstract = {Cladding austenitic stainless steels are trendy these days in pressure vessels to enhance surface qualities. In this work, austenitic stainless steel clad layers deposited by flux cored arc welding process on structural steel plates used in boiler construction are investigated. To facilitate this, composite clad layers are produced with 316L stainless steel deposits on low carbon structural steel plates based on the design of experiments. Results exhibit an incremental trend of thermal conductivity with respect to percentage of weld dilution. A linear mathematical relationship is established between the percentage of weld dilution and post-weld thermal conductivity of clad layers for the prediction of thermal conductivity of clad layer deposits. These results could be supportive in the fabrication industries for producing energy efficient thermal equipment},
author = {Manuscript, Accepted},
title = {{Ac ce us}},
year = {2020}
}
@inproceedings{Zeiler2010,
abstract = {Building robust low and mid-level image representa- tions, beyond edge primitives, is a long-standing goal in vision. Many existing feature detectors spatially pool edge information which destroys cues such as edge intersections, parallelism and symmetry. We present a learning frame- work where features that capture these mid-level cues spon- taneously emerge from image data. Our approach is based on the convolutional decomposition ofimages under a spar- sity constraint and is totally unsupervised. By building a hierarchy ofsuch decompositions we can learn rich feature sets that are a robust image representation for both the anal- ysis and synthesis ofimages.},
author = {Zeiler, Matthew D. and Krishnan, Dilip and Taylor, Graham W. and Fergus, Rob},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1016/j.ins.2020.01.028},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zeiler et al. - 2010 - Deconvolutional Networks.pdf:pdf},
issn = {00200255},
title = {{Deconvolutional Networks}},
year = {2010}
}
@article{Konig2008,
abstract = {This paper presents G-REX, a versatile data mining framework based on Genetic Programming. What differs G-REX from other GP frameworks is that it doesn't strive to be a general purpose framework. This allows G-REX to include more functionality specific to data mining like preprocessing, evaluation-and optimization methods, but also a multitude of predefined classification and regression models. Examples of predefined models are decision trees, decision lists, k-NN with attribute weights, hybrid kNN-rules, fuzzy-rules and several different regression models. The main strength is, however, the flexibility, making it easy to modify, extend and combine all of the predefined functionality. G-REX is, in addition, available in a special Weka package adding useful evolutionary functionality to the standard data mining tool Weka. {\textcopyright} 2008 IEEE.},
author = {Konig, Rikard and Johansson, Ulf and Niklasson, Lars},
doi = {10.1109/ICDMW.2008.117},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Konig, Johansson, Niklasson - 2008 - G-REX A versatile framework for evolutionary data mining.pdf:pdf},
isbn = {9780769535036},
journal = {Proceedings - IEEE International Conference on Data Mining Workshops, ICDM Workshops 2008},
pages = {971--974},
title = {{G-REX: A versatile framework for evolutionary data mining}},
year = {2008}
}
@article{Harradon2018,
abstract = {Deep neural networks are complex and opaque. As they enter application in a variety of important and safety critical domains, users seek methods to explain their output predictions. We develop an approach to explaining deep neural networks by constructing causal models on salient concepts contained in a CNN. We develop methods to extract salient concepts throughout a target network by using autoencoders trained to extract human-understandable representations of network activations. We then build a bayesian causal model using these extracted concepts as variables in order to explain image classification. Finally, we use this causal model to identify and visualize features with significant causal influence on final classification.},
archivePrefix = {arXiv},
arxivId = {1802.00541},
author = {Harradon, Michael and Druce, Jeff and Ruttenberg, Brian},
eprint = {1802.00541},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Harradon, Druce, Ruttenberg - 2018 - Causal Learning and Explanation of Deep Neural Networks via Autoencoded Activations.pdf:pdf},
title = {{Causal Learning and Explanation of Deep Neural Networks via Autoencoded Activations}},
url = {http://arxiv.org/abs/1802.00541},
year = {2018}
}
@inproceedings{Wagner2019,
author = {Wagner, Jorg and Kohler, Jan Mathias and Gindele, Tobias},
booktitle = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1007/978-981-13-3648-5_33},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wagner, Kohler, Gindele - 2019 - Interpretable and Fine-Grained Visual Explanations for Convolutional Neural Networks.pdf:pdf},
isbn = {9789811336478},
issn = {18761119},
title = {{Interpretable and Fine-Grained Visual Explanations for Convolutional Neural Networks}},
year = {2019}
}
@article{Assaf2019,
abstract = {We demonstrate that CNN deep neural networks can not only be used for making predictions based on multivariate time series data, but also for explaining these predictions. This is important for a number of applications where predictions are the basis for decisions and actions. Hence, confidence in the prediction result is crucial. We design a two stage convolutional neural network architecture which uses particular kernel sizes. This allows us to utilise gradient based techniques for generating saliency maps for both the time dimension and the features. These are then used for explaining which features during which time interval are responsible for a given prediction, as well as explaining during which time intervals was the joint contribution of all features most important for that prediction. We demonstrate our approach for predicting the average energy production of photovoltaic power plants and for explaining these predictions.},
author = {Assaf, Roy and Schumann, Anika},
doi = {10.24963/ijcai.2019/932},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Assaf, Schumann - 2019 - Explainable deep neural networks for multivariate time series predictions.pdf:pdf},
isbn = {9780999241141},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {AI: Knowledge Representation and Reasoning,AI: Machine Learning,Applications: Energy},
number = {2},
pages = {6488--6490},
title = {{Explainable deep neural networks for multivariate time series predictions}},
volume = {2019-Augus},
year = {2019}
}
@article{Mordvintsev2015,
author = {Mordvintsev, A. and Olah, C. and Tyka, M.},
journal = {Google Research Blog},
title = {{Inceptionism: Going deeper into neural networks}},
year = {2015}
}
@article{Holzinger2018,
abstract = {The success of statistical machine learning (ML) methods made the field of Artificial Intelligence (AI) so popular again, after the last AI winter. Meanwhile deep learning approaches even exceed human performance in particular tasks. However, such approaches have some disadvantages besides of needing big quality data, much computational power and engineering effort; those approaches are becoming increasingly opaque, and even if we understand the underlying mathematical principles of such models they still lack explicit declarative knowledge. For example, words are mapped to high-dimensional vectors, making them unintelligible to humans. What we need in the future are context-adaptive procedures, i.e. systems that construct contextual explanatory models for classes of real-world phenomena. This is the goal of explainable AI, which is not a new field; rather, the problem of explainability is as old as AI itself. While rule-based approaches of early AI were comprehensible 'glass-box' approaches at least in narrow domains, their weakness was in dealing with uncertainties of the real world. Maybe one step further is in linking probabilistic learning methods with large knowledge representations (ontologies) and logical approaches, thus making results re-traceable, explainable and comprehensible on demand.},
author = {Holzinger, Andreas},
doi = {10.1109/DISA.2018.8490530},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Holzinger - 2018 - From machine learning to explainable AI(2).pdf:pdf},
isbn = {9781538651025},
journal = {DISA 2018 - IEEE World Symposium on Digital Intelligence for Systems and Machines, Proceedings},
pages = {55--66},
publisher = {IEEE},
title = {{From machine learning to explainable AI}},
year = {2018}
}
@article{Selvaraju2016,
abstract = {We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing input regions that are 'important' for predictions -- or visual explanations. Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses class-specific gradient information to localize important regions. These localizations are combined with existing pixel-space visualizations to create a novel high-resolution and class-discriminative visualization called Guided Grad-CAM. These methods help better understand CNN-based models, including image captioning and visual question answering (VQA) models. We evaluate our visual explanations by measuring their ability to discriminate between classes, to inspire trust in humans, and their correlation with occlusion maps. Grad-CAM provides a new way to understand CNN-based models. We have released code, an online demo hosted on CloudCV, and a full version of this extended abstract.},
archivePrefix = {arXiv},
arxivId = {1611.07450},
author = {Selvaraju, Ramprasaath R and Das, Abhishek and Vedantam, Ramakrishna and Cogswell, Michael and Parikh, Devi and Batra, Dhruv},
eprint = {1611.07450},
pages = {1--4},
title = {{Grad-CAM: Why did you say that?}},
url = {http://arxiv.org/abs/1611.07450},
year = {2016}
}
@article{Guidotti2018a,
abstract = {The recent years have witnessed the rise of accurate but obscure decision systems which hide the logic of their internal decision processes to the users. The lack of explanations for the decisions of black box systems is a key ethical issue, and a limitation to the adoption of machine learning components in socially sensitive and safety-critical contexts. In this paper we focus on the problem of black box outcome explanation, i.e., explaining the reasons of the decision taken on a specific instance. We propose LORE, an agnostic method able to provide interpretable and faithful explanations. LORE first leans a local interpretable predictor on a synthetic neighborhood generated by a genetic algorithm. Then it derives from the logic of the local interpretable predictor a meaningful explanation consisting of: A decision rule, which explains the reasons of the decision; and a set of counterfactual rules, suggesting the changes in the instance's features that lead to a different outcome. Wide experiments show that LORE outperforms existing methods and baselines both in the quality of explanations and in the accuracy in mimicking the black box.},
archivePrefix = {arXiv},
arxivId = {1805.10820},
author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Pedreschi, Dino and Turini, Franco and Giannotti, Fosca},
eprint = {1805.10820},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guidotti et al. - 2018 - Local rule-based explanations of black box decision systems.pdf:pdf},
issn = {23318422},
journal = {arXiv},
keywords = {Decision Systems,Explanation,Rules},
number = {May},
title = {{Local rule-based explanations of black box decision systems}},
year = {2018}
}
@article{Ribeiro2016a,
abstract = {At the core of interpretable machine learning is the question of whether humans are able to make accurate predictions about a model's behavior. Assumed in this question are three properties of the interpretable output: coverage, precision, and effort. Coverage refers to how often humans think they can predict the model's behavior, precision to how accurate humans are in those predictions, and effort is either the up-front effort required in interpreting the model, or the effort required to make predictions about a model's behavior. In this work, we propose anchor-LIME (aLIME), a model-agnostic technique that produces high-precision rule-based explanations for which the coverage boundaries are very clear. We compare aLIME to linear LIME with simulated experiments, and demonstrate the flexibility of aLIME with qualitative examples from a variety of domains and tasks.},
archivePrefix = {arXiv},
arxivId = {1611.05817},
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
eprint = {1611.05817},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ribeiro, Singh, Guestrin - 2016 - Nothing Else Matters Model-Agnostic Explanations By Identifying Prediction Invariance.pdf:pdf},
number = {Nips},
title = {{Nothing Else Matters: Model-Agnostic Explanations By Identifying Prediction Invariance}},
url = {http://arxiv.org/abs/1611.05817},
year = {2016}
}
@article{Bien2011,
author = {Bien, Jacob and Tibshirani, Robert},
doi = {10.1214/11},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bien, Tibshirani - 2011 - PROTOTYPE SELECTION FOR INTERPRETABLE CLASSIFICATION.pdf:pdf},
journal = {The annuals of applied statistics},
number = {4},
pages = {2403--2424},
title = {{PROTOTYPE SELECTION FOR INTERPRETABLE CLASSIFICATION}},
volume = {5},
year = {2011}
}
@article{Chen2020a,
abstract = {The automatic text-based diagnosis remains a challenging task for clinical use because it requires appropriate balance between accuracy and interpretability. In this paper, we attempt to propose a solution by introducing a novel framework that stacks Bayesian Network Ensembles on top of Entity-Aware Con-volutional Neural Networks (CNN) towards building an accurate yet interpretable diagnosis system. The proposed framework takes advantage of the high accuracy and generality of deep neural networks as well as the interpretability of Bayesian Networks, which is critical for AI-empowered healthcare. The evaluation conducted on the real Electronic Medical Record (EMR) documents from hospitals and annotated by professional doctors proves that, the proposed framework outper-forms the previous automatic diagnosis methods in accuracy performance and the diagnosis explanation of the framework is reasonable.},
author = {Chen, Jun and Dai, Xiaoya and Yuan, Quan and Lu, Chao and Huang, Haifeng},
doi = {10.18653/v1/2020.acl-main.286},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2020 - Towards Interpretable Clinical Diagnosis with Bayesian Network Ensembles Stacked on Entity-Aware CNNs.pdf:pdf},
pages = {3143--3153},
title = {{Towards Interpretable Clinical Diagnosis with Bayesian Network Ensembles Stacked on Entity-Aware CNNs}},
year = {2020}
}
@article{Wolanin2020,
author = {Wolanin, Aleksandra and Camps-Valls, Gustau and Gomez-Chova, Luis},
doi = {10.1016/j.enzmictec.2006.09.022},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wolanin, Camps-Valls, Gomez-Chova - 2020 - Estimating and understanding crop yields with explainable deep learning in the Indian Wheat B.pdf:pdf},
issn = {01681656},
journal = {Environmental Research Letters},
pmid = {16945445},
title = {{Estimating and understanding crop yields with explainable deep learning in the Indian Wheat Belt}},
year = {2020}
}
@article{Ehsan2019,
abstract = {Automated rationale generation is an approach for real-time explanation generation whereby a computational model learns to translate an autonomous agent's internal state and action data representations into natural language. Training on human explanation data can enable agents to learn to generate human-like explanations for their behavior. In this paper, using the context of an agent that plays Frogger, we describe (a) how to collect a corpus of explanations, (b) how to train a neural rationale generator to produce different styles of rationales, and (c) how people perceive these rationales. We conducted two user studies. The first study establishes the plausibility of each type of generated rationale and situates their user perceptions along the dimensions of confidence, humanlike-ness, adequate justification, and understandability. The second study further explores user preferences between the generated rationales with regard to confidence in the autonomous agent, communicating failure and unexpected behavior. Overall, we find alignment between the intended differences in features of the generated rationales and the perceived differences by users. Moreover, context permitting, participants preferred detailed rationales to form a stable mental model of the agent's behavior.},
archivePrefix = {arXiv},
arxivId = {1901.03729},
author = {Ehsan, Upol and Tambwekar, Pradyumna and Chan, Larry and Harrison, Brent and Riedl, Mark O.},
doi = {10.1145/3301275.3302316},
eprint = {1901.03729},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ehsan et al. - 2019 - Automated rationale generation A technique for explainable AI and its effects on human perceptions.pdf:pdf},
journal = {International Conference on Intelligent User Interfaces, Proceedings IUI},
keywords = {Algorithmic decision-making,Algorithmic explanation,Artificial Intelligence,Explainable AI,Interpretability,Machine Learning,Rationale generation,Transparency,User perception},
pages = {263--274},
title = {{Automated rationale generation: A technique for explainable AI and its effects on human perceptions}},
volume = {Part F1476},
year = {2019}
}
@article{Vaswani2017,
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
archivePrefix = {arXiv},
arxivId = {1706.03762},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
eprint = {1706.03762},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vaswani et al. - 2017 - Attention is all you need.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {Nips},
pages = {5999--6009},
title = {{Attention is all you need}},
volume = {2017-Decem},
year = {2017}
}
@article{Vermeire2020,
abstract = {The complexity of state-of-the-art modeling techniques for image classification impedes the ability to explain model predictions in an interpretable way. Existing explanation methods generally create importance rankings in terms of pixels or pixel groups. However, the resulting explanations lack an optimal size, do not consider feature dependence and are only related to one class. Counterfactual explanation methods are considered promising to explain complex model decisions, since they are associated with a high degree of human interpretability. In this paper, SEDC is introduced as a model-agnostic instance-level explanation method for image classification to obtain visual counterfactual explanations. For a given image, SEDC searches a small set of segments that, in case of removal, alters the classification. As image classification tasks are typically multiclass problems, SEDC-T is proposed as an alternative method that allows specifying a target counterfactual class. We compare SEDC(-T) with popular feature importance methods such as LRP, LIME and SHAP, and we describe how the mentioned importance ranking issues are addressed. Moreover, concrete examples and experiments illustrate the potential of our approach (1) to obtain trust and insight, and (2) to obtain input for model improvement by explaining misclassifications.},
archivePrefix = {arXiv},
arxivId = {2004.07511},
author = {Vermeire, Tom and Martens, David},
eprint = {2004.07511},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vermeire, Martens - 2020 - Explainable Image Classification with Evidence Counterfactual(2).pdf:pdf},
title = {{Explainable Image Classification with Evidence Counterfactual}},
url = {http://arxiv.org/abs/2004.07511},
year = {2020}
}
@article{Carvalho2019,
abstract = {Machine learning systems are becoming increasingly ubiquitous. These systems's adoption has been expanding, accelerating the shift towards a more algorithmic society, meaning that algorithmically informed decisions have greater potential for significant social impact. However, most of these accurate decision support systems remain complex black boxes, meaning their internal logic and inner workings are hidden to the user and even experts cannot fully understand the rationale behind their predictions. Moreover, new regulations and highly regulated domains have made the audit and verifiability of decisions mandatory, increasing the demand for the ability to question, understand, and trust machine learning systems, for which interpretability is indispensable. The research community has recognized this interpretability problem and focused on developing both interpretable models and explanation methods over the past few years. However, the emergence of these methods shows there is no consensus on how to assess the explanation quality. Which are the most suitable metrics to assess the quality of an explanation? The aim of this article is to provide a review of the current state of the research field on machine learning interpretability while focusing on the societal impact and on the developed methods and metrics. Furthermore, a complete literature review is presented in order to identify future directions of work on this field.},
author = {Carvalho, Diogo V. and Pereira, Eduardo M. and Cardoso, Jaime S.},
doi = {10.3390/electronics8080832},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Carvalho, Pereira, Cardoso - 2019 - Machine learning interpretability A survey on methods and metrics.pdf:pdf},
issn = {20799292},
journal = {Electronics (Switzerland)},
keywords = {Explainability,Interpretability,Machine learning,XAI},
number = {8},
pages = {1--34},
title = {{Machine learning interpretability: A survey on methods and metrics}},
volume = {8},
year = {2019}
}
@article{Ming2019,
abstract = {With the growing adoption of machine learning techniques, there is a surge of research interest towards making machine learning systems more transparent and interpretable. Various visualizations have been developed to help model developers understand, diagnose, and refine machine learning models. However, a large number of potential but neglected users are the domain experts with little knowledge of machine learning but are expected to work with machine learning systems. In this paper, we present an interactive visualization technique to help users with little expertise in machine learning to understand, explore and validate predictive models. By viewing the model as a black box, we extract a standardized rule-based knowledge representation from its input-output behavior. Then, we design RuleMatrix, a matrix-based visualization of rules to help users navigate and verify the rules and the black-box model. We evaluate the effectiveness of RuleMatrix via two use cases and a usability study.},
archivePrefix = {arXiv},
arxivId = {1807.06228},
author = {Ming, Yao and Qu, Huamin and Bertini, Enrico},
doi = {10.1109/TVCG.2018.2864812},
eprint = {1807.06228},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ming, Qu, Bertini - 2019 - RuleMatrix Visualizing and Understanding Classifiers with Rules.pdf:pdf},
issn = {19410506},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {explainable machine learning,rule visualization,visual analytics},
number = {1},
pages = {342--352},
title = {{RuleMatrix: Visualizing and Understanding Classifiers with Rules}},
volume = {25},
year = {2019}
}
@article{Bertossi2020,
abstract = {We propose answer-set programs that specify and compute counterfactual interventions as a basis for causality-based explanations to decisions produced by classification models. They can be applied with black-box models and models that can be specified as logic programs, such as rule-based classifiers. The main focus is on the specification and computation of maximum responsibility causal explanations. The use of additional semantic knowledge is investigated.},
archivePrefix = {arXiv},
arxivId = {2004.13237},
author = {Bertossi, Leopoldo},
doi = {10.1007/978-3-030-57977-7_5},
eprint = {2004.13237},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bertossi - 2020 - An ASP-Based Approach to Counterfactual Explanations for Classification.pdf:pdf},
isbn = {9783030579760},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {70--81},
title = {{An ASP-Based Approach to Counterfactual Explanations for Classification}},
volume = {12173 LNCS},
year = {2020}
}
@article{Schnake2020,
abstract = {Graph Neural Networks (GNNs) are a popular approach for predicting graph structured data. As GNNs tightly entangle the input graph into the neural network structure, common explainable AI approaches are not applicable. To a large extent, GNNs have remained black-boxes for the user so far. In this paper, we show that GNNs can in fact be naturally explained using higher-order expansions, i.e. by identifying groups of edges that jointly contribute to the prediction. Practically, we find that such explanations can be extracted using a nested attribution scheme, where existing techniques such as layer-wise relevance propagation (LRP) can be applied at each step. The output is a collection of walks into the input graph that are relevant for the prediction. Our novel explanation method, which we denote by GNN-LRP, is applicable to a broad range of graph neural networks and lets us extract practically relevant insights on sentiment analysis of text data, structure-property relationships in quantum chemistry, and image classification.},
archivePrefix = {arXiv},
arxivId = {2006.03589},
author = {Schnake, Thomas and Eberle, Oliver and Lederer, Jonas and Nakajima, Shinichi and Sch{\"{u}}tt, Kristof T. and M{\"{u}}ller, Klaus-Robert and Montavon, Gr{\'{e}}goire},
eprint = {2006.03589},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schnake et al. - 2020 - Higher-Order Explanations of Graph Neural Networks via Relevant Walks.pdf:pdf},
title = {{Higher-Order Explanations of Graph Neural Networks via Relevant Walks}},
url = {http://arxiv.org/abs/2006.03589},
year = {2020}
}
@article{UlHassan2019,
abstract = {The performance of deep Convolutional Neural Networks (CNN) has been reaching or even exceeding the human level on large number of tasks. Some examples are image classification, Mastering Go game, speech understanding etc. However, their lack of decomposability into intuitive and understandable components make them hard to interpret, i.e. no information is provided about what makes them arrive at their prediction. We propose a technique to interpret CNN classification task and justify the classification result with visual explanation and visual search. The model consists of two sub networks: A deep recurrent neural network for generating textual justification and a deep convolutional network for image analysis. This multimodal approach generates the textual justification about the classification decision. To verify the textual justification, we use the visual search to extract the similar content from the training set. We evaluate our strategy on a novel CUB dataset with the ground-Truth attributes. We make use of these attributes to further strengthen the justification by providing the attributes of images.},
author = {{Ul Hassan}, Muneeb and Mulhem, Philippe and Pellerin, Denis and Quenot, Georges},
doi = {10.1109/CBMI.2019.8877393},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ul Hassan et al. - 2019 - Explaining Visual Classification using Attributes.pdf:pdf},
isbn = {9781728146737},
issn = {19493991},
journal = {Proceedings - International Workshop on Content-Based Multimedia Indexing},
keywords = {Deep Neural Networks,Explainable AI,Interpretability},
pages = {1--6},
publisher = {IEEE},
title = {{Explaining Visual Classification using Attributes}},
volume = {2019-Septe},
year = {2019}
}
@article{Samek2017,
abstract = {With the availability of large databases and recent improvements in deep learning methodology, the performance of AI systems is reaching or even exceeding the human level on an increasing number of complex tasks. Impressive examples of this development can be found in domains such as image classification, sentiment analysis, speech understanding or strategic game playing. However, because of their nested non-linear structure, these highly successful machine learning and artificial intelligence models are usually applied in a black box manner, i.e., no information is provided about what exactly makes them arrive at their predictions. Since this lack of transparency can be a major drawback, e.g., in medical applications, the development of methods for visualizing, explaining and interpreting deep learning models has recently attracted increasing attention. This paper summarizes recent developments in this field and makes a plea for more interpretability in artificial intelligence. Furthermore, it presents two approaches to explaining predictions of deep learning models, one method which computes the sensitivity of the prediction with respect to changes in the input and one approach which meaningfully decomposes the decision in terms of the input variables. These methods are evaluated on three classification tasks.},
archivePrefix = {arXiv},
arxivId = {1708.08296},
author = {Samek, Wojciech and Wiegand, Thomas and M{\"{u}}ller, Klaus Robert},
eprint = {1708.08296},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Samek, Wiegand, M{\"{u}}ller - 2017 - Explainable artificial intelligence Understanding, visualizing and interpreting deep learning models.pdf:pdf},
issn = {23318422},
journal = {arXiv},
keywords = {Black box models,Deep neural networks,Index Terms— Artificial intelligence,Interpretability,Layer-wise relevance propagation,Sensitivity analysis},
number = {1},
pages = {1--10},
title = {{Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models}},
year = {2017}
}
@article{Selvaraju2016,
abstract = {We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing input regions that are 'important' for predictions -- or visual explanations. Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses class-specific gradient information to localize important regions. These localizations are combined with existing pixel-space visualizations to create a novel high-resolution and class-discriminative visualization called Guided Grad-CAM. These methods help better understand CNN-based models, including image captioning and visual question answering (VQA) models. We evaluate our visual explanations by measuring their ability to discriminate between classes, to inspire trust in humans, and their correlation with occlusion maps. Grad-CAM provides a new way to understand CNN-based models. We have released code, an online demo hosted on CloudCV, and a full version of this extended abstract.},
archivePrefix = {arXiv},
arxivId = {1611.07450},
author = {Selvaraju, Ramprasaath R and Das, Abhishek and Vedantam, Ramakrishna and Cogswell, Michael and Parikh, Devi and Batra, Dhruv},
eprint = {1611.07450},
pages = {1--4},
title = {{Grad-CAM: Why did you say that?}},
url = {http://arxiv.org/abs/1611.07450},
year = {2016}
}
@article{Zhou2016,
abstract = {In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network (CNN) to have remarkable localization ability despite being trained on imagelevel labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that exposes the implicit attention of CNNs on an image. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014 without training on any bounding box annotation. We demonstrate in a variety of experiments that our network is able to localize the discriminative image regions despite just being trained for solving classification task1.},
archivePrefix = {arXiv},
arxivId = {1512.04150},
author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
doi = {10.1109/CVPR.2016.319},
eprint = {1512.04150},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhou et al. - 2016 - Learning Deep Features for Discriminative Localization.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {2921--2929},
title = {{Learning Deep Features for Discriminative Localization}},
volume = {2016-Decem},
year = {2016}
}
@article{Alemi2017,
abstract = {We present a variational approximation to the information bottleneck of Tishby et al. (1999). This variational approach allows us to parameterize the information bottleneck model using a neural network and leverage the reparameterization trick for efficient training. We call this method “Deep Variational Information Bottleneck”, or Deep VIB. We show that models trained with the VIB objective outperform those that are trained with other forms of regularization, in terms of generalization performance and robustness to adversarial attack.},
archivePrefix = {arXiv},
arxivId = {1612.00410},
author = {Alemi, Alexander A. and Fischer, Ian and Dillon, Joshua V. and Murphy, Kevin},
eprint = {1612.00410},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Alemi et al. - 2017 - Deep variational information bottleneck.pdf:pdf},
journal = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
pages = {1--19},
title = {{Deep variational information bottleneck}},
year = {2017}
}
@article{Li2016,
abstract = {While neural networks have been successfully applied to many natural language processing tasks, they come at the cost of interpretability. In this paper, we propose a general methodology to analyze and interpret decisions from a neural model by observing the effects on the model of erasing various parts of the representation, such as input word-vector dimensions, intermediate hidden units, or input words. We present several approaches to analyzing the effects of such erasure, from computing the relative difference in evaluation metrics, to using reinforcement learning to erase the minimum set of input words in order to flip a neural model's decision. In a comprehensive analysis of multiple NLP tasks, including linguistic feature classification, sentence-level sentiment analysis, and document level sentiment aspect prediction, we show that the proposed methodology not only offers clear explanations about neural model decisions, but also provides a way to conduct error analysis on neural models.},
archivePrefix = {arXiv},
arxivId = {1612.08220},
author = {Li, Jiwei and Monroe, Will and Jurafsky, Dan},
eprint = {1612.08220},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Monroe, Jurafsky - 2016 - Understanding Neural Networks through Representation Erasure.pdf:pdf},
title = {{Understanding Neural Networks through Representation Erasure}},
url = {http://arxiv.org/abs/1612.08220},
year = {2016}
}
@article{Desai2020,
abstract = {In response to recent criticism of gradient-based visualization techniques, we propose a new methodology to generate visual explanations for deep Convolutional Neural Networks (CNN) - based models. Our approach - Ablation-based Class Activation Mapping (Ablation CAM) uses ablation analysis to determine the importance (weights) of individual feature map units w.r.t. class. Further, this is used to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Our objective and subjective evaluations show that this gradient-free approach works better than state-of-the-art Grad-CAM technique. Moreover, further experiments are carried out to show that Ablation-CAM is class discriminative as well as can be used to evaluate trust in a model.},
author = {Desai, Saurabh and Ramaswamy, Harish G.},
doi = {10.1109/WACV45572.2020.9093360},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Desai, Ramaswamy - 2020 - Ablation-CAM Visual explanations for deep convolutional network via gradient-free localization.pdf:pdf},
isbn = {9781728165530},
journal = {Proceedings - 2020 IEEE Winter Conference on Applications of Computer Vision, WACV 2020},
pages = {972--980},
title = {{Ablation-CAM: Visual explanations for deep convolutional network via gradient-free localization}},
year = {2020}
}
@article{Grun2016,
abstract = {Over the last decade, Convolutional Neural Networks (CNN) saw a tremendous surge in performance. However, understanding what a network has learned still proves to be a challenging task. To remedy this unsatisfactory situation, a number of groups have recently proposed different methods to visualize the learned models. In this work we suggest a general taxonomy to classify and compare these methods, subdividing the literature into three main categories and providing researchers with a terminology to base their works on. Furthermore, we introduce the FeatureVis library for MatConvNet: an extendable, easy to use open source library for visualizing CNNs. It contains implementations from each of the three main classes of visualization methods and serves as a useful tool for an enhanced understanding of the features learned by intermediate layers, as well as for the analysis of why a network might fail for certain examples.},
archivePrefix = {arXiv},
arxivId = {1606.07757},
author = {Gr{\"{u}}n, Felix and Rupprecht, Christian and Navab, Nassir and Tombari, Federico},
eprint = {1606.07757},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gr{\"{u}}n et al. - 2016 - A Taxonomy and Library for Visualizing Learned Features in Convolutional Neural Networks.pdf:pdf},
title = {{A Taxonomy and Library for Visualizing Learned Features in Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1606.07757},
volume = {48},
year = {2016}
}
@article{Selvaraju2016,
abstract = {We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing input regions that are 'important' for predictions -- or visual explanations. Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses class-specific gradient information to localize important regions. These localizations are combined with existing pixel-space visualizations to create a novel high-resolution and class-discriminative visualization called Guided Grad-CAM. These methods help better understand CNN-based models, including image captioning and visual question answering (VQA) models. We evaluate our visual explanations by measuring their ability to discriminate between classes, to inspire trust in humans, and their correlation with occlusion maps. Grad-CAM provides a new way to understand CNN-based models. We have released code, an online demo hosted on CloudCV, and a full version of this extended abstract.},
archivePrefix = {arXiv},
arxivId = {1611.07450},
author = {Selvaraju, Ramprasaath R and Das, Abhishek and Vedantam, Ramakrishna and Cogswell, Michael and Parikh, Devi and Batra, Dhruv},
eprint = {1611.07450},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Selvaraju et al. - 2016 - Grad-CAM Why did you say that.pdf:pdf},
pages = {1--4},
title = {{Grad-CAM: Why did you say that?}},
url = {http://arxiv.org/abs/1611.07450},
year = {2016}
}
@article{Schlichtkrull2020,
abstract = {Graph neural networks (GNNs) have become a popular approach to integrating structural inductive biases into NLP models. However, there has been little work on interpreting them, and specifically on understanding which parts of the graphs (e.g. syntactic trees or co-reference structures) contribute to a prediction. In this work, we introduce a post-hoc method for interpreting the predictions of GNNs which identifies unnecessary edges. Given a trained GNN model, we learn a simple classifier that, for every edge in every layer, predicts if that edge can be dropped. We demonstrate that such a classifier can be trained in a fully differentiable fashion, employing stochastic gates and encouraging sparsity through the expected L0 norm. We use our technique as an attribution method to analyze GNN models for two tasks – question answering and semantic role labeling – providing insights into the information flow in these models. We show that we can drop a large proportion of edges without deteriorating the performance of the model, while we can analyse the remaining edges for interpreting model predictions.},
archivePrefix = {arXiv},
arxivId = {2010.00577},
author = {Schlichtkrull, Michael Sejr and de Cao, Nicola and Titov, Ivan},
eprint = {2010.00577},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schlichtkrull, de Cao, Titov - 2020 - Interpreting graph neural networks for NLP with differentiable edge masking.pdf:pdf},
issn = {23318422},
journal = {arXiv},
number = {January},
title = {{Interpreting graph neural networks for NLP with differentiable edge masking}},
year = {2020}
}
@article{Yuan2020a,
abstract = {Graphs neural networks (GNNs) learn node features by aggregating and combining neighbor information, which have achieved promising performance on many graph tasks. However, GNNs are mostly treated as black-boxes and lack human intelligible explanations. Thus, they cannot be fully trusted and used in certain application domains if GNN models cannot be explained. In this work, we propose a novel approach, known as XGNN, to interpret GNNs at the model-level. Our approach can provide high-level insights and generic understanding of how GNNs work. In particular, we propose to explain GNNs by training a graph generator so that the generated graph patterns maximize a certain prediction of the model. We formulate the graph generation as a reinforcement learning task, where for each step, the graph generator predicts how to add an edge into the current graph. The graph generator is trained via a policy gradient method based on information from the trained GNNs. In addition, we incorporate several graph rules to encourage the generated graphs to be valid. Experimental results on both synthetic and real-world datasets show that our proposed methods help understand and verify the trained GNNs. Furthermore, our experimental results indicate that the generated graphs can provide guidance on how to improve the trained GNNs.},
archivePrefix = {arXiv},
arxivId = {2006.02587},
author = {Yuan, Hao and Tang, Jiliang and Hu, Xia and Ji, Shuiwang},
doi = {10.1145/3394486.3403085},
eprint = {2006.02587},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yuan et al. - 2020 - XGNN Towards Model-Level Explanations of Graph Neural Networks.pdf:pdf},
isbn = {9781450379984},
journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
keywords = {deep learning,graph neural networks,interpretability},
pages = {430--438},
title = {{XGNN: Towards Model-Level Explanations of Graph Neural Networks}},
year = {2020}
}
@article{Dinghuai,
author = {Dinghuai, Zhang},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dinghuai - Unknown - the Interpretability of Deep Neural Networks.pdf:pdf},
pages = {1--6},
title = {{the Interpretability of Deep Neural Networks}}
}
@article{Stepin2021,
abstract = {A number of algorithms in the field of artificial intelligence offer poorly interpretable decisions. To disclose the reasoning behind such algorithms, their output can be explained by means of so-called evidence-based (or factual) explanations. Alternatively, contrastive and counterfactual explanations justify why the output of the algorithms is not any different and how it could be changed, respectively. It is of crucial importance to bridge the gap between theoretical approaches to contrastive and counterfactual explanation and the corresponding computational frameworks. In this work we conduct a systematic literature review which provides readers with a thorough and reproducible analysis of the interdisciplinary research field under study. We first examine theoretical foundations of contrastive and counterfactual accounts of explanation. Then, we report the state-of-the-art computational frameworks for contrastive and counterfactual explanation generation. In addition, we analyze how grounded such frameworks are on the insights from the inspected theoretical approaches. As a result, we highlight a variety of properties of the approaches under study and reveal a number of shortcomings thereof. Moreover, we define a taxonomy regarding both theoretical and practical approaches to contrastive and counterfactual explanation.},
author = {Stepin, Ilia and Alonso, Jose M. and Catala, Alejandro and Pereira-Farina, Martin},
doi = {10.1109/ACCESS.2021.3051315},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Stepin et al. - 2021 - A Survey of Contrastive and Counterfactual Explanation Generation Methods for Explainable Artificial Intellige(2).pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Computational intelligence,contrastive explanations,counterfactuals,explainable artificial intelligence,systematic literature review},
pages = {11974--12001},
title = {{A Survey of Contrastive and Counterfactual Explanation Generation Methods for Explainable Artificial Intelligence}},
volume = {9},
year = {2021}
}
@article{Arras2019,
abstract = {While neural networks have acted as a strong unifying force in the design of modern AI systems, the neural network architectures themselves remain highly heterogeneous due to the variety of tasks to be solved. In this chapter, we explore how to adapt the Layer-wise Relevance Propagation (LRP) technique used for explaining the predictions of feed-forward networks to the LSTM architecture used for sequential data modeling and forecasting. The special accumulators and gated interactions present in the LSTM require both a new propagation scheme and an extension of the underlying theoretical framework to deliver faithful explanations.},
archivePrefix = {arXiv},
arxivId = {1909.12114},
author = {Arras, Leila and Arjona-Medina, Jos{\'{e}} and Widrich, Michael and Montavon, Gr{\'{e}}goire and Gillhofer, Michael and M{\"{u}}ller, Klaus Robert and Hochreiter, Sepp and Samek, Wojciech},
doi = {10.1007/978-3-030-28954-6_11},
eprint = {1909.12114},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Arras et al. - 2019 - Explaining and Interpreting LSTMs.pdf:pdf},
isbn = {9783030289546},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Explainable artificial intelligence,Interpretability,LSTM,Model transparency,Recurrent neural networks},
number = {2019},
pages = {211--238},
title = {{Explaining and Interpreting LSTMs}},
volume = {11700 LNCS},
year = {2019}
}
@article{Luss2019a,
abstract = {Explaining decisions of deep neural networks is a hot research topic with applications in medical imaging, video surveillance, and self driving cars. Many methods have been proposed in literature to explain these decisions by identifying relevance of different pixels, limiting the types of explanations possible. In this paper, we propose a method that can generate contrastive explanations for such data where we not only highlight aspects that are in themselves sufficient to justify the classification by the deep model, but also new aspects which if added will change the classification. In order to move beyond the limitations of previous explanations, our key contribution is how we define "addition" for such rich data in a formal yet humanly interpretable way that leads to meaningful results. This was one of the open questions laid out in in Dhurandhar et.al. (2018) [6], which proposed a general framework for creating (local) contrastive explanations for deep models, but is limited to simple use cases such as black/white images. We showcase the efficacy of our approach on three diverse image data sets (faces, skin lesions, and fashion apparel) in creating intuitive explanations that are also quantitatively superior compared with other state-of-the-art interpretability methods. A thorough user study with 200 individuals asks how well the various methods are understood by humans and demonstrates which aspects of contrastive explanations are most desirable.},
archivePrefix = {arXiv},
arxivId = {1905.12698},
author = {Luss, Ronny and Chen, Pin-Yu and Dhurandhar, Amit and Sattigeri, Prasanna and Zhang, Yunfeng and Shanmugam, Karthikeyan and Tu, Chun-Chen},
eprint = {1905.12698},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Luss et al. - 2019 - Generating Contrastive Explanations with Monotonic Attribute Functions.pdf:pdf},
pages = {1--26},
title = {{Generating Contrastive Explanations with Monotonic Attribute Functions}},
url = {http://arxiv.org/abs/1905.12698},
year = {2019}
}
@article{Nguyen2017,
abstract = {Generating high-resolution, photo-realistic images has been a long-standing goal in machine learning. Recently, Nguyen et al. [36] showed one interesting way to synthesize novel images by performing gradient ascent in the latent space of a generator network to maximize the activations of one or multiple neurons in a separate classifier network. In this paper we extend this method by introducing an additional prior on the latent code, improving both sample quality and sample diversity, leading to a state-of-the-art generative model that produces high quality images at higher resolutions (227 × 227) than previous generative models, and does so for all 1000 ImageNet categories. In addition, we provide a unified probabilistic interpretation of related activation maximization methods and call the general class of models “Plug and Play Generative Networks.” PPGNs are composed of 1) a generator network G that is capable of drawing a wide range of image types and 2) a replaceable “condition” network C that tells the generator what to draw. We demonstrate the generation of images conditioned on a class (when C is an ImageNet or MIT Places classification network) and also conditioned on a caption (when C is an image captioning network). Our method also improves the state of the art of Multifaceted Feature Visualization [39], which generates the set of synthetic inputs that activate a neuron in order to better understand how deep neural networks operate. Finally, we show that our model performs reasonably well at the task of image inpainting. While image models are used in this paper, the approach is modality-agnostic and can be applied to many types of data.},
archivePrefix = {arXiv},
arxivId = {arXiv:submit/1738978},
author = {Nguyen, Anh and Yosinski, Jason and Bengio, Yoshua and Dosovitskiy, Alexey and Clune, Jeff},
eprint = {1738978},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nguyen et al. - 2017 - Plug & Play Generative Networks Conditional Iterative Generation of Images in Latent Space.pdf:pdf},
journal = {Iccv},
number = {1},
pages = {4467--4477},
primaryClass = {arXiv:submit},
title = {{Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space}},
year = {2017}
}
@article{Windisch2020,
abstract = {Purpose: While neural networks gain popularity in medical research, attempts to make the decisions of a model explainable are often only made towards the end of the development process once a high predictive accuracy has been achieved. Methods: In order to assess the advantages of implementing features to increase explainability early in the development process, we trained a neural network to differentiate between MRI slices containing either a vestibular schwannoma, a glioblastoma, or no tumor. Results: Making the decisions of a network more explainable helped to identify potential bias and choose appropriate training data. Conclusion: Model explainability should be considered in early stages of training a neural network for medical purposes as it may save time in the long run and will ultimately help physicians integrate the network's predictions into a clinical decision.},
author = {Windisch, Paul and Weber, Pascal and F{\"{u}}rweger, Christoph and Ehret, Felix and Kufeld, Markus and Zwahlen, Daniel and Muacevic, Alexander},
doi = {10.1007/s00234-020-02465-1},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Windisch et al. - 2020 - Implementation of model explainability for a basic brain tumor detection using convolutional neural networks on.pdf:pdf},
isbn = {0023402002465},
issn = {14321920},
journal = {Neuroradiology},
keywords = {Artificial intelligence,Deep learning,Explainability,Gliobastoma,Machine learning,Vestibular Schwannoma},
number = {11},
pages = {1515--1518},
pmid = {32500277},
publisher = {Neuroradiology},
title = {{Implementation of model explainability for a basic brain tumor detection using convolutional neural networks on MRI slices}},
volume = {62},
year = {2020}
}
@article{Zhao2018,
abstract = {The convolutional neural network (CNN) has become a powerful tool for various biomedical image analysis tasks, but there is a lack of visual explanation for the machinery of CNNs. In this paper, we present a novel algorithm, Respond-weighted Class Activation Mapping (Respond-CAM), for making CNN-based models interpretable by visualizing input regions that are important for predictions, especially for biomedical 3D imaging data inputs. Our method uses the gradients of any target concept (e.g. the score of target class) that flow into a convolutional layer. The weighted feature maps are combined to produce a heatmap that highlights the important regions in the image for predicting the target concept. We prove a preferable sum-to-score property of the Respond-CAM and verify its significant improvement on 3D images from the current state-of-the-art approach. Our tests on Cellular Electron Cryo-Tomography 3D images show that Respond-CAM achieves superior performance on visualizing the CNNs with 3D biomedical image inputs, and is able to get reasonably good results on visualizing the CNNs with natural image inputs. The Respond-CAM is an efficient and reliable approach for visualizing the CNN machinery, and is applicable to a wide variety of CNN model families and image analysis tasks. Our code is available at: https://github.com/xulabs/projects/tree/master/respond_cam.},
archivePrefix = {arXiv},
arxivId = {1806.00102},
author = {Zhao, Guannan and Zhou, Bo and Wang, Kaiwen and Jiang, Rui and Xu, Min},
doi = {10.1007/978-3-030-00928-1_55},
eprint = {1806.00102},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao et al. - 2018 - Respond-CAM Analyzing deep models for 3D imaging data by visualizations.pdf:pdf},
isbn = {9783030009274},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {485--492},
title = {{Respond-CAM: Analyzing deep models for 3D imaging data by visualizations}},
volume = {11070 LNCS},
year = {2018}
}
@article{Dosovitskiy2017,
abstract = {We train generative 'up-convolutional' neural networks which are able to generate images of objects given object style, viewpoint, and color. We train the networks on rendered 3D models of chairs, tables, and cars. Our experiments show that the networks do not merely learn all images by heart, but rather find a meaningful representation of 3D models allowing them to assess the similarity of different models, interpolate between given views to generate the missing ones, extrapolate views, and invent new objects not present in the training set by recombining training instances, or even two different object classes. Moreover, we show that such generative networks can be used to find correspondences between different objects from the dataset, outperforming existing approaches on this task.},
archivePrefix = {arXiv},
arxivId = {1411.5928},
author = {Dosovitskiy, Alexey and Springenberg, Jost Tobias and Tatarchenko, Maxim and Brox, Thomas},
doi = {10.1109/TPAMI.2016.2567384},
eprint = {1411.5928},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dosovitskiy et al. - 2017 - Learning to Generate Chairs, Tables and Cars with Convolutional Networks.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Convolutional networks,generative models,image generation,up-convolutional networks},
number = {4},
pages = {692--705},
pmid = {27187944},
title = {{Learning to Generate Chairs, Tables and Cars with Convolutional Networks}},
volume = {39},
year = {2017}
}
@article{Shen2019,
abstract = {While deep learning methods have demonstrated performance comparable to human readers in tasks such as computer-aided diagnosis, these models are difficult to interpret, do not incorporate prior domain knowledge, and are often considered as a “black-box.” The lack of model interpretability hinders them from being fully understood by end users such as radiologists. In this paper, we present a novel interpretable deep hierarchical semantic convolutional neural network (HSCNN) to predict whether a given pulmonary nodule observed on a computed tomography (CT) scan is malignant. Our network provides two levels of output: 1) low-level semantic features; and 2) a high-level prediction of nodule malignancy. The low-level outputs reflect diagnostic features often reported by radiologists and serve to explain how the model interprets the images in an expert-interpretable manner. The information from these low-level outputs, along with the representations learned by the convolutional layers, are then combined and used to infer the high-level output. This unified architecture is trained by optimizing a global loss function including both low- and high-level tasks, thereby learning all the parameters within a joint framework. Our experimental results using the Lung Image Database Consortium (LIDC) show that the proposed method not only produces interpretable lung cancer predictions but also achieves better results compared to using a 3D CNN alone.},
archivePrefix = {arXiv},
arxivId = {1806.00712},
author = {Shen, Shiwen and Han, Simon X. and Aberle, Denise R. and Bui, Alex A. and Hsu, William},
doi = {10.1016/j.eswa.2019.01.048},
eprint = {1806.00712},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shen et al. - 2019 - An interpretable deep hierarchical semantic convolutional neural network for lung nodule malignancy classificati(2).pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Computed tomography,Convolutional neural networks,Deep learning,Lung cancer diagnosis,Lung nodule classification,Model interpretability},
pages = {84--95},
publisher = {Elsevier Ltd},
title = {{An interpretable deep hierarchical semantic convolutional neural network for lung nodule malignancy classification}},
url = {https://doi.org/10.1016/j.eswa.2019.01.048},
volume = {128},
year = {2019}
}
@article{Baldassarre2019,
abstract = {Graph Networks are used to make decisions in potentially complex scenarios but it is usually not obvious how or why they made them. In this work, we study the explainability of Graph Network decisions using two main classes of techniques, gradient-based and decomposition-based, on a toy dataset and a chemistry task. Our study sets the ground for future development as well as application to real-world problems.},
archivePrefix = {arXiv},
arxivId = {1905.13686},
author = {Baldassarre, Federico and Azizpour, Hossein},
eprint = {1905.13686},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baldassarre, Azizpour - 2019 - Explainability Techniques for Graph Convolutional Networks.pdf:pdf},
issn = {23318422},
journal = {arXiv},
title = {{Explainability Techniques for Graph Convolutional Networks}},
year = {2019}
}
@article{Manuscript2020,
abstract = {Cladding austenitic stainless steels are trendy these days in pressure vessels to enhance surface qualities. In this work, austenitic stainless steel clad layers deposited by flux cored arc welding process on structural steel plates used in boiler construction are investigated. To facilitate this, composite clad layers are produced with 316L stainless steel deposits on low carbon structural steel plates based on the design of experiments. Results exhibit an incremental trend of thermal conductivity with respect to percentage of weld dilution. A linear mathematical relationship is established between the percentage of weld dilution and post-weld thermal conductivity of clad layers for the prediction of thermal conductivity of clad layer deposits. These results could be supportive in the fabrication industries for producing energy efficient thermal equipment},
author = {Manuscript, Accepted},
title = {{Ac ce us}},
year = {2020}
}
@article{Tishby2000,
abstract = {We define the relevant information in a signal $x\in X$ as being the information that this signal provides about another signal $y\in \Y$. Examples include the information that face images provide about the names of the people portrayed, or the information that speech sounds provide about the words spoken. Understanding the signal $x$ requires more than just predicting $y$, it also requires specifying which features of $\X$ play a role in the prediction. We formalize this problem as that of finding a short code for $\X$ that preserves the maximum information about $\Y$. That is, we squeeze the information that $\X$ provides about $\Y$ through a `bottleneck' formed by a limited set of codewords $\tX$. This constrained optimization problem can be seen as a generalization of rate distortion theory in which the distortion measure $d(x,\x)$ emerges from the joint statistics of $\X$ and $\Y$. This approach yields an exact set of self consistent equations for the coding rules $X \to \tX$ and $\tX \to \Y$. Solutions to these equations can be found by a convergent re-estimation method that generalizes the Blahut-Arimoto algorithm. Our variational principle provides a surprisingly rich framework for discussing a variety of problems in signal processing and learning, as will be described in detail elsewhere.},
archivePrefix = {arXiv},
arxivId = {physics/0004057},
author = {Tishby, Naftali and Pereira, Fernando C. and Bialek, William},
eprint = {0004057},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tishby, Pereira, Bialek - 2000 - The information bottleneck method.pdf:pdf},
pages = {1--16},
primaryClass = {physics},
title = {{The information bottleneck method}},
url = {http://arxiv.org/abs/physics/0004057},
year = {2000}
}
@article{Dhurandhar2019,
abstract = {Recently, a method [7] was proposed to generate contrastive explanations for differentiable models such as deep neural networks, where one has complete access to the model. In this work, we propose a method, Model Agnostic Contrastive Explanations Method (MACEM), to generate contrastive explanations for \emph{any} classification model where one is able to \emph{only} query the class probabilities for a desired input. This allows us to generate contrastive explanations for not only neural networks, but models such as random forests, boosted trees and even arbitrary ensembles that are still amongst the state-of-the-art when learning on structured data [13]. Moreover, to obtain meaningful explanations we propose a principled approach to handle real and categorical features leading to novel formulations for computing pertinent positives and negatives that form the essence of a contrastive explanation. A detailed treatment of the different data types of this nature was not performed in the previous work, which assumed all features to be positive real valued with zero being indicative of the least interesting value. We part with this strong implicit assumption and generalize these methods so as to be applicable across a much wider range of problem settings. We quantitatively and qualitatively validate our approach over 5 public datasets covering diverse domains.},
archivePrefix = {arXiv},
arxivId = {1906.00117},
author = {Dhurandhar, Amit and Pedapati, Tejaswini and Balakrishnan, Avinash and Chen, Pin-Yu and Shanmugam, Karthikeyan and Puri, Ruchir},
eprint = {1906.00117},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dhurandhar et al. - 2019 - Model Agnostic Contrastive Explanations for Structured Data(2).pdf:pdf},
title = {{Model Agnostic Contrastive Explanations for Structured Data}},
url = {http://arxiv.org/abs/1906.00117},
year = {2019}
}
@article{Murdoch2019a,
abstract = {Machine-learning models have demonstrated great success in learning complex patterns that enable them to make predictions about unobserved data. In addition to using models for prediction, the ability to interpret what a model has learned is receiving an increasing amount of attention. However, this increased focus has led to considerable confusion about the notion of interpretability. In particular, it is unclear how the wide array of proposed interpretation methods are related and what common concepts can be used to evaluate them. We aim to address these concerns by defining interpretability in the context of machine learning and introducing the predictive, descriptive, relevant (PDR) framework for discussing interpretations. The PDR framework provides 3 overarching desiderata for evaluation: predictive accuracy, descriptive accuracy, and relevancy, with relevancy judged relative to a human audience. Moreover, to help manage the deluge of interpretation methods, we introduce a categorization of existing techniques into model-based and post hoc categories, with subgroups including sparsity, modularity, and simulatability. To demonstrate how practitioners can use the PDR framework to evaluate and understand interpretations, we provide numerous real-world examples. These examples highlight the often underappreciated role played by human audiences in discussions of interpretability. Finally, based on our framework, we discuss limitations of existing methods and directions for future work. We hope that this work will provide a common vocabulary that will make it easier for both practitioners and researchers to discuss and choose from the full range of interpretation methods.},
archivePrefix = {arXiv},
arxivId = {arXiv:1901.04592v1},
author = {Murdoch, W. James and Singh, Chandan and Kumbier, Karl and Abbasi-Asl, Reza and Yu, Bin},
doi = {10.1073/pnas.1900654116},
eprint = {arXiv:1901.04592v1},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Murdoch et al. - 2019 - Definitions, methods, and applications in interpretable machine learning(3).pdf:pdf},
issn = {10916490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Explainability,Interpretability,Machine learning,Relevancy},
number = {44},
pages = {22071--22080},
pmid = {31619572},
title = {{Definitions, methods, and applications in interpretable machine learning}},
volume = {116},
year = {2019}
}
@article{Meyes,
author = {Meyes, R and Lu, M and Meisen, T},
file = {:home/anna/Desktop/network explanation/class distribution/Ablation Studies to Uncover Structure of Learned Representations in Artificial Neural Networks.pdf:pdf},
isbn = {1601325010},
keywords = {ablations,ai transparency,artificial neural networks,explainable ai,learning,representations},
pages = {185--191},
title = {{Ablation Studies to Uncover Structure of Learned Representations in Artificial Neural Networks}}
}
@article{Ribeiro2016,
abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
archivePrefix = {arXiv},
arxivId = {1602.04938},
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
doi = {10.1145/2939672.2939778},
eprint = {1602.04938},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ribeiro, Singh, Guestrin - 2016 - Why should i trust you Explaining the predictions of any classifier.pdf:pdf},
isbn = {9781450342322},
journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1135--1144},
title = {{"Why should i trust you?" Explaining the predictions of any classifier}},
volume = {13-17-Augu},
year = {2016}
}
@article{Selvaraju2016,
abstract = {We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing input regions that are 'important' for predictions -- or visual explanations. Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses class-specific gradient information to localize important regions. These localizations are combined with existing pixel-space visualizations to create a novel high-resolution and class-discriminative visualization called Guided Grad-CAM. These methods help better understand CNN-based models, including image captioning and visual question answering (VQA) models. We evaluate our visual explanations by measuring their ability to discriminate between classes, to inspire trust in humans, and their correlation with occlusion maps. Grad-CAM provides a new way to understand CNN-based models. We have released code, an online demo hosted on CloudCV, and a full version of this extended abstract.},
archivePrefix = {arXiv},
arxivId = {1611.07450},
author = {Selvaraju, Ramprasaath R and Das, Abhishek and Vedantam, Ramakrishna and Cogswell, Michael and Parikh, Devi and Batra, Dhruv},
eprint = {1611.07450},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Selvaraju et al. - 2016 - Grad-CAM Why did you say that.pdf:pdf},
pages = {1--4},
title = {{Grad-CAM: Why did you say that?}},
url = {http://arxiv.org/abs/1611.07450},
year = {2016}
}
@article{Artelt2020a,
abstract = {The increasing use of machine learning in practice and legal regulations like EU's GDPR cause the necessity to be able to explain the prediction and behavior of machine learning models. A prominent example of particularly intuitive explanations of AI models in the context of decision making are counterfactual explanations. Yet, it is still an open research problem how to efficiently compute counterfactual explanations for many models. We investigate how to efficiently compute counterfactual explanations for an important class of models, prototype-based classifiers such as learning vector quantization models. In particular, we derive specific convex and non-convex programs depending on the used metric.},
archivePrefix = {arXiv},
arxivId = {1908.00735},
author = {Artelt, Andre and Hammer, Barbara},
eprint = {1908.00735},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Artelt, Hammer - 2020 - Efficient computation of counterfactual explanations of LVQ models(2).pdf:pdf},
isbn = {9782875870742},
journal = {ESANN 2020 - Proceedings, 28th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning},
pages = {19--24},
title = {{Efficient computation of counterfactual explanations of LVQ models}},
year = {2020}
}
@article{Becker2018,
abstract = {Interpretability of deep neural networks is a recently emerging area of machine learning research targeting a better understanding of how models perform feature selection and derive their classification decisions. This paper explores the interpretability of neural networks in the audio domain by using the previously proposed technique of layer-wise relevance propagation (LRP). We present a novel audio dataset of English spoken digits which we use for classification tasks on spoken digits and speaker's gender. We use LRP to identify relevant features for two neural network architectures that process either waveform or spectrogram representations of the data. Based on the relevance scores obtained from LRP, hypotheses about the neural networks' feature selection are derived and subsequently tested through systematic manipulations of the input data. The results confirm that the networks are highly reliant on features marked as relevant by LRP.},
archivePrefix = {arXiv},
arxivId = {1807.03418},
author = {Becker, S{\"{o}}ren and Ackermann, Marcel and Lapuschkin, Sebastian and M{\"{u}}ller, Klaus-Robert and Samek, Wojciech},
eprint = {1807.03418},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Becker et al. - 2018 - Interpreting and Explaining Deep Neural Networks for Classification of Audio Signals.pdf:pdf},
pages = {2--6},
title = {{Interpreting and Explaining Deep Neural Networks for Classification of Audio Signals}},
url = {http://arxiv.org/abs/1807.03418},
year = {2018}
}
@article{Rieger2018,
abstract = {Machine learning algorithms such as neural networks are more useful, when their predictions can be explained, e.g. in terms of input variables. Often simpler models are more interpretable than more complex models with higher performance. In practice, one can choose a readily interpretable (possibly less predictive) model. Another solution is to directly explain the original, highly predictive model. In this chapter, we present a middle-ground approach where the original neural network architecture is modified parsimoniously in order to reduce common biases observed in the explanations. Our approach leads to explanations that better separate classes in feed-forward networks, and that also better identify relevant time steps in recurrent neural networks.},
author = {Rieger, Laura and Chormai, Pattarawat and Montavon, Gr{\'{e}}goire and Hansen, Lars Kai and M{\"{u}}ller, Klaus-Robert},
doi = {10.1007/978-3-319-98131-4_5},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rieger et al. - 2018 - Structuring Neural Networks for More Explainable Predictions.pdf:pdf},
isbn = {9783319981314},
keywords = {chormai contributed equally to,convolutional neural networks,hansen,interpretable machine learning,k,l,laura rieger and pattarawat,recurrent neural networks,rieger,this work},
pages = {115--131},
title = {{Structuring Neural Networks for More Explainable Predictions}},
year = {2018}
}
@article{Ehsan2018,
abstract = {We introduce AI rationalization, an approach for generating explanations of autonomous system behavior as if a human had performed the behavior. We describe a rationalization technique that uses neural machine translation to translate internal state-action representations of an autonomous agent into natural language. We evaluate our technique in the Frogger game environment, training an autonomous game playing agent to rationalize its action choices using natural language. A natural language training corpus is collected from human players thinking out loud as they play the game. We motivate the use of rationalization as an approach to explanation generation and show the results of two experiments evaluating the effectiveness of rationalization. Results of these evaluations show that neural machine translation is able to accurately generate rationalizations that describe agent behavior, and that rationalizations are more satisfying to humans than other alternative methods of explanation.},
archivePrefix = {arXiv},
arxivId = {1702.07826},
author = {Ehsan, Upol and Harrison, Brent and Chan, Larry and Riedl, Mark O.},
doi = {10.1145/3278721.3278736},
eprint = {1702.07826},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ehsan et al. - 2018 - Rationalization A Neural Machine Translation Approach to Generating Natural Language Explanations.pdf:pdf},
isbn = {9781450360128},
journal = {AIES 2018 - Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
keywords = {ai rationalization,artificial intelligence,explainable ai,interpretability,machine learning,transparency,user perception},
pages = {81--87},
title = {{Rationalization: A Neural Machine Translation Approach to Generating Natural Language Explanations}},
year = {2018}
}
@article{Shrikumar2017a,
abstract = {The purported "black box" nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLlFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLlFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLlFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLlFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. Video tutorial: http://goo.gl/qKb7pL, code: http://goo.gl/RM8jvH.},
archivePrefix = {arXiv},
arxivId = {1704.02685},
author = {Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
eprint = {1704.02685},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shrikumar, Greenside, Kundaje - 2017 - Learning important features through propagating activation differences.pdf:pdf},
isbn = {9781510855144},
journal = {34th International Conference on Machine Learning, ICML 2017},
pages = {4844--4866},
title = {{Learning important features through propagating activation differences}},
volume = {7},
year = {2017}
}
@article{Manuscript2020,
abstract = {Cladding austenitic stainless steels are trendy these days in pressure vessels to enhance surface qualities. In this work, austenitic stainless steel clad layers deposited by flux cored arc welding process on structural steel plates used in boiler construction are investigated. To facilitate this, composite clad layers are produced with 316L stainless steel deposits on low carbon structural steel plates based on the design of experiments. Results exhibit an incremental trend of thermal conductivity with respect to percentage of weld dilution. A linear mathematical relationship is established between the percentage of weld dilution and post-weld thermal conductivity of clad layers for the prediction of thermal conductivity of clad layer deposits. These results could be supportive in the fabrication industries for producing energy efficient thermal equipment},
author = {Manuscript, Accepted},
title = {{Ac ce us}},
year = {2020}
}
@article{Caruana1999,
abstract = {We show how to generate case-based explanations for non-case-based learning methods such as artificial neural nets or decision trees. The method uses the trained model (e.g., the neural net or the decision tree) as a distance metric to determine which cases in the training set are most similar to the case that needs to be explained. This approach is well suited to medical domains, where it is important to understand predictions made by complex machine learning models, and where training and clinical practice makes users adept at case interpretation.},
author = {Caruana, R. and Kangarloo, H. and Dionisio, J. D. and Sinha, U. and Johnson, D.},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Caruana et al. - 1999 - Case-based explanation of non-case-based learning methods.pdf:pdf},
issn = {1531605X},
journal = {Proceedings / AMIA ... Annual Symposium. AMIA Symposium},
pages = {212--215},
pmid = {10566351},
title = {{Case-based explanation of non-case-based learning methods.}},
year = {1999}
}
@article{Islam2021,
abstract = {The lack of explainability of a decision from an Artificial Intelligence (AI) based "black box" system/model, despite its superiority in many real-world applications, is a key stumbling block for adopting AI in many high stakes applications of different domain or industry. While many popular Explainable Artificial Intelligence (XAI) methods or approaches are available to facilitate a human-friendly explanation of the decision, each has its own merits and demerits, with a plethora of open challenges. We demonstrate popular XAI methods with a mutual case study/task (i.e., credit default prediction), analyze for competitive advantages from multiple perspectives (e.g., local, global), provide meaningful insight on quantifying explainability, and recommend paths towards responsible or human-centered AI using XAI as a medium. Practitioners can use this work as a catalog to understand, compare, and correlate competitive advantages of popular XAI methods. In addition, this survey elicits future research directions towards responsible or human-centric AI systems, which is crucial to adopt AI in high stakes applications.},
archivePrefix = {arXiv},
arxivId = {2101.09429},
author = {Islam, Sheikh Rabiul and Eberle, William and Ghafoor, Sheikh Khaled and Ahmed, Mohiuddin},
eprint = {2101.09429},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Islam et al. - 2021 - Explainable Artificial Intelligence Approaches A Survey.pdf:pdf},
pages = {1--14},
title = {{Explainable Artificial Intelligence Approaches: A Survey}},
url = {http://arxiv.org/abs/2101.09429},
year = {2021}
}
@article{Guan2019,
abstract = {We define a unified information-based measure to provide quantitative explanations on how intermediate layers of deep Natural Language Processing (NLP) models leverage information of input words. Our method advances existing explanation methods by addressing issues in coherency and generality. Explanations generated by using our method are consistent and faithful across different timestamps, layers, and models. We show how our method can be applied to four widely used models in NLP and explain their performances on three real-world benchmark datasets.},
author = {Guan, Chaoyu and Wang, Xiting and Zhang, Quanshi and Chen, Runjin and He, Di and Xie, Xing},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guan et al. - 2019 - Towards a deep and unified understanding of deep neural models in NLP(2).pdf:pdf},
isbn = {9781510886988},
journal = {36th International Conference on Machine Learning, ICML 2019},
pages = {4366--4375},
title = {{Towards a deep and unified understanding of deep neural models in NLP}},
volume = {2019-June},
year = {2019}
}
@article{Hoffman2018,
abstract = {What makes for an explanation of "black box" AI systems such as Deep Nets? We reviewed the pertinent literatures on explanation and derived key ideas. This set the stage for our empirical inquiries, which include conceptual cognitive modeling, the analysis of a corpus of cases of "naturalistic explanation" of computational systems, computational cognitive modeling, and the development of measures for performance evaluation. The purpose of our work is to contribute to the program of research on "Explainable AI." In this report we focus on our initial synthetic modeling activities and the development of measures for the evaluation of explainability in human-machine work systems.},
author = {Hoffman, Robert R. and Klein, Gary and Mueller, Shane T.},
doi = {10.1177/1541931218621047},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hoffman, Klein, Mueller - 2018 - Explaining explanation for explainable AI(2).pdf:pdf},
isbn = {9781510889538},
issn = {10711813},
journal = {Proceedings of the Human Factors and Ergonomics Society},
pages = {197--201},
title = {{Explaining explanation for "explainable AI}},
volume = {1},
year = {2018}
}
@article{Kolchinsky2019,
abstract = {Information bottleneck (IB) is a technique for extracting information in one random variable X that is relevant for predicting another random variable Y. IB works by encoding X in a compressed "bottleneck" random variable M from which Y can be accurately decoded. However, finding the optimal bottleneck variable involves a difficult optimization problem, which until recently has been considered for only two limited cases: discrete X and Y with small state spaces, and continuous X and Y with a Gaussian joint distribution (in which case optimal encoding and decoding maps are linear). We propose a method for performing IB on arbitrarily-distributed discrete and/or continuous X and Y, while allowing for nonlinear encoding and decoding maps. Our approach relies on a novel non-parametric upper bound for mutual information. We describe how to implement our method using neural networks. We then show that it achieves better performance than the recently-proposed "variational IB" method on several real-world datasets.},
archivePrefix = {arXiv},
arxivId = {1705.02436},
author = {Kolchinsky, Artemy and Tracey, Brendan D. and Wolpert, David H.},
doi = {10.3390/e21121181},
eprint = {1705.02436},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kolchinsky, Tracey, Wolpert - 2019 - Nonlinear information bottleneck.pdf:pdf},
issn = {10994300},
journal = {Entropy},
keywords = {Information bottleneck,Mutual information,Neural networks,Representation learning},
number = {12},
pages = {1--15},
title = {{Nonlinear information bottleneck}},
volume = {21},
year = {2019}
}
@article{Wang2018,
abstract = {Explainable recommendation, which provides explanations about why an item is recommended, has attracted increasing attention due to its ability in helping users make better decisions and increasing users' trust in the system. Existing explainable recommendation methods either ignore the working mechanism of the recommendation model or are designed for a specific recommendation model. Moreover, it is difficult for existing methods to ensure the presentation quality of the explanations (e.g., consistency). To solve these problems, we design a reinforcement learning framework for explainable recommendation. Our framework can explain any recommendation model (model-agnostic) and can flexibly control the explanation quality based on the application scenario. To demonstrate the effectiveness of our framework, we show how it can be used for generating sentence-level explanations. Specifically, we instantiate the explanation generator in the framework with a personalized-attention-based neural network. Offline experiments demonstrate that our method can well explain both collaborative filtering methods and deep-learning-based models. Evaluation with human subjects shows that the explanations generated by our method are significantly more useful than the explanations generated by the baselines.},
author = {Wang, Xiting and Chen, Yiru and Yang, Jie and Wu, Le and Wu, Zhengtao and Xie, Xing},
doi = {10.1109/ICDM.2018.00074},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2018 - A Reinforcement Learning Framework for Explainable Recommendation.pdf:pdf},
isbn = {9781538691588},
issn = {15504786},
journal = {Proceedings - IEEE International Conference on Data Mining, ICDM},
keywords = {Attention networks,Explainable recommendation,Personalized explanation,Reinforcement learning},
pages = {587--596},
title = {{A Reinforcement Learning Framework for Explainable Recommendation}},
volume = {2018-Novem},
year = {2018}
}
@article{Chen2020,
abstract = {This study introduces an explainable artificial intelligence (XAI) approach of convolutional neural networks (CNNs) for classification in vibration signals analysis. First, vibration signals are transformed into images by short-time Fourier transform (STFT). A CNN is applied as classification model, and Gradient class activation mapping (Grad-CAM) is utilized to generate the attention of model. By analyzing the attentions, the explanation of classification models for vibration signals analysis can be carried out. Finally, the verifications of attention are introduced by neural networks, adaptive network-based fuzzy inference system (ANFIS), and decision trees to demonstrate the proposed results. By the proposed methodology, the explanation of model using highlighted attentions is carried out.},
author = {Chen, Han Yun and Lee, Ching Hung},
doi = {10.1109/ACCESS.2020.3006491},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Lee - 2020 - Vibration Signals Analysis by Explainable Artificial Intelligence (XAI) Approach Application on Bearing Faults Diagno.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Convolutional neural network,explainable AI,fault diagnosis,vibration signal},
pages = {134246--134256},
title = {{Vibration Signals Analysis by Explainable Artificial Intelligence (XAI) Approach: Application on Bearing Faults Diagnosis}},
volume = {8},
year = {2020}
}
@article{Fong2019,
abstract = {Attribution is the problem of finding which parts of an image are the most responsible for the output of a deep neural network. An important family of attribution methods is based on measuring the effect of perturbations applied to the input image, either via exhaustive search or by finding representative perturbations via optimization. In this paper, we discuss some of the shortcomings of existing approaches to perturbation analysis and address them by introducing the concept of extremal perturbations, which are theoretically grounded and interpretable. We also introduce a number of technical innovations to compute these extremal perturbations, including a new area constraint and a parametric family of smooth perturbations, which allow us to remove all tunable weighing factors from the optimization problem. We analyze the effect of perturbations as a function of their area, demonstrating excellent sensitivity to the spatial properties of the network under stimulation. We also extend perturbation analysis to the intermediate layers of a deep neural network. This application allows us to show how compactly an image can be represented (in terms of the number of channels it requires). We also demonstrate that the consistency with which images of a given class rely on the same intermediate channel correlates well with class accuracy.},
archivePrefix = {arXiv},
arxivId = {1910.08485},
author = {Fong, Ruth and Patrick, Mandela and Vedaldi, Andrea},
doi = {10.1109/ICCV.2019.00304},
eprint = {1910.08485},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fong, Patrick, Vedaldi - 2019 - Understanding deep networks via extremal perturbations and smooth masks.pdf:pdf},
isbn = {9781728148038},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {2950--2958},
title = {{Understanding deep networks via extremal perturbations and smooth masks}},
volume = {2019-Octob},
year = {2019}
}
@article{Letham2015,
abstract = {We aim to produce predictive models that are not only accurate, but are also interpretable to human experts. Our models are decision lists, which consist of a series of if {\ldots} then. . . statements (e.g., if high blood pressure, then stroke) that discretize a high-dimensional, multivariate feature space into a series of simple, readily interpretable decision statements. We introduce a generative model called Bayesian Rule Lists that yields a posterior distribution over possible decision lists. It employs a novel prior structure to encourage sparsity. Our experiments show that Bayesian Rule Lists has predictive accuracy on par with the current top algorithms for prediction in machine learning. Our method is motivated by recent developments in personalized medicine, and can be used to produce highly accurate and interpretable medical scoring systems. We demonstrate this by producing an alternative to the CHADS2 score, actively used in clinical practice for estimating the risk of stroke in patients that have atrial fibrillation. Our model is as interpretable as CHADS2, but more accurate.},
archivePrefix = {arXiv},
arxivId = {1511.01644},
author = {Letham, Benjamin and Rudin, Cynthia and McCormick, Tyler H. and Madigan, David},
doi = {10.1214/15-AOAS848},
eprint = {1511.01644},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Letham et al. - 2015 - Interpretable classifiers using rules and bayesian analysis Building a better stroke prediction model.pdf:pdf},
issn = {19417330},
journal = {Annals of Applied Statistics},
keywords = {Bayesian analysis,Classification,Interpretability},
number = {3},
pages = {1350--1371},
title = {{Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model}},
volume = {9},
year = {2015}
}
@article{Mishra2017,
abstract = {The interpretability of a machine learning model is essential for gaining insight into model behaviour. While some machine learning models (e.g., decision trees) are transparent, the majority of models used today are still black-boxes. Recent work in machine learning aims to analyse these models by explaining the basis of their decisions. In this work, we extend one such technique, called local interpretable model-agnostic explanations, to music content analysis. We propose three versions of explanations: one version is based on temporal segmentation, and the other two are based on frequency and time-frequency segmentation. These explanations provide meaningful ways to understand the factors that influence the classification of specific input data. We apply our proposed methods to three singing voice detection systems: the first two are designed using decision tree and random forest classifiers, respectively; the third system is based on convolutional neural network. The explanations we generate provide insights into the model behaviour. We use these insights to demonstrate that despite achieving 71.4% classification accuracy, the decision tree model fails to generalise. We also demonstrate that the model-agnostic explanations for the neural network model agree in many cases with the model-dependent saliency maps. The experimental code and results are available online.1},
author = {Mishra, Saumitra and Sturm, Bob L. and Dixon, Simon},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mishra, Sturm, Dixon - 2017 - Local interpretable model-agnostic explanations for music content analysis.pdf:pdf},
isbn = {9789811151798},
journal = {Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017},
pages = {537--543},
title = {{Local interpretable model-agnostic explanations for music content analysis}},
year = {2017}
}
@article{Li2016a,
abstract = {While neural networks have been successfully applied to many NLP tasks the resulting vector-based models are very difficult to interpret. For example it's not clear how they achieve compositionality, building sentence meaning from the meanings of words and phrases. In this paper we describe strategies for visualizing compositionality in neural models for NLP, inspired by similar work in computer vision. We first plot unit values to visualize compositionality of negation, intensification, and concessive clauses, allowing us to see well-known markedness asymmetries in negation. We then introduce methods for visualizing a unit's salience, the amount that it contributes to the final composed meaning from first-order derivatives. Our general-purpose methods may have wide applications for understanding compositionality and other semantic properties of deep networks.},
archivePrefix = {arXiv},
arxivId = {1506.01066},
author = {Li, Jiwei and Chen, Xinlei and Hovy, Eduard and Jurafsky, Dan},
doi = {10.18653/v1/n16-1082},
eprint = {1506.01066},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2016 - Visualizing and understanding neural models in NLP(2).pdf:pdf},
isbn = {9781941643914},
journal = {2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2016 - Proceedings of the Conference},
pages = {681--691},
title = {{Visualizing and understanding neural models in NLP}},
year = {2016}
}
@article{Montavon2019,
abstract = {For a machine learning model to generalize well, one needs to ensure that its decisions are supported by meaningful patterns in the input data. A prerequisite is however for the model to be able to explain itself, e.g. by highlighting which input features it uses to support its prediction. Layer-wise Relevance Propagation (LRP) is a technique that brings such explainability and scales to potentially highly complex deep neural networks. It operates by propagating the prediction backward in the neural network, using a set of purposely designed propagation rules. In this chapter, we give a concise introduction to LRP with a discussion of (1) how to implement propagation rules easily and efficiently, (2) how the propagation procedure can be theoretically justified as a ‘deep Taylor decomposition', (3) how to choose the propagation rules at each layer to deliver high explanation quality, and (4) how LRP can be extended to handle a variety of machine learning scenarios beyond deep neural networks.},
author = {Montavon, Gr{\'{e}}goire and Binder, Alexander and Lapuschkin, Sebastian and Samek, Wojciech and M{\"{u}}ller, Klaus Robert},
doi = {10.1007/978-3-030-28954-6_10},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Montavon et al. - 2019 - Layer-Wise Relevance Propagation An Overview.pdf:pdf},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Deep Neural Networks,Deep Taylor Decomposition,Explanations,Layer-wise Relevance Propagation},
pages = {193--209},
title = {{Layer-Wise Relevance Propagation: An Overview}},
volume = {11700 LNCS},
year = {2019}
}
@article{Liu2019a,
abstract = {Deep Neural Networks have achieved huge success at a wide spectrum of applications from language modeling, computer vision to speech recognition. However, nowadays, good performance alone is not enough to satisfy the needs of practical deployment where interpretability is demanded for cases involving ethics and mission critical applications. The complex models of Deep Neural Networks make it hard to understand and reason the predictions, which hinders its further progress. To tackle this problem, we apply the Knowledge Distillation technique to distill Deep Neural Networks into decision trees in order to attain good performance and interpretability simultaneously. We formulate the problem at hand as a multi-output regression problem and the experiments demonstrate that the student model achieves significantly better accuracy performance (about 1% to 5%) than vanilla decision trees at the same level of tree depth. The experiments are implemented on the TensorFlow platform to make it scalable to big datasets. To the best of our knowledge, we are the first to distill Deep Neural Networks into vanilla decision trees on multi-class datasets.},
archivePrefix = {arXiv},
arxivId = {1812.10924},
author = {Liu, Xuan and Wang, Xiaoguang and Matwin, Stan},
doi = {10.1109/ICDMW.2018.00132},
eprint = {1812.10924},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu, Wang, Matwin - 2019 - Improving the interpretability of deep neural networks with knowledge distillation.pdf:pdf},
isbn = {9781538692882},
issn = {23759259},
journal = {IEEE International Conference on Data Mining Workshops, ICDMW},
keywords = {Decision Tree,Neural Networks,TensorFlow,dark knowledge,interpretation,knowledge distillation},
pages = {905--912},
title = {{Improving the interpretability of deep neural networks with knowledge distillation}},
volume = {2018-Novem},
year = {2019}
}
@article{VanLooveren2019,
abstract = {We propose a fast, model agnostic method for finding interpretable counterfactual explanations of classifier predictions by using class prototypes. We show that class prototypes, obtained using either an encoder or through class specific k-d trees, significantly speed up the the search for counterfactual instances and result in more interpretable explanations. We introduce two novel metrics to quantitatively evaluate local interpretability at the instance level. We use these metrics to illustrate the effectiveness of our method on an image and tabular dataset, respectively MNIST and Breast Cancer Wisconsin (Diagnostic). The method also eliminates the computational bottleneck that arises because of numerical gradient evaluation for $\textit{black box}$ models.},
archivePrefix = {arXiv},
arxivId = {1907.02584},
author = {{Van Looveren}, Arnaud and Klaise, Janis},
eprint = {1907.02584},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Van Looveren, Klaise - 2019 - Interpretable Counterfactual Explanations Guided by Prototypes(2).pdf:pdf},
title = {{Interpretable Counterfactual Explanations Guided by Prototypes}},
url = {http://arxiv.org/abs/1907.02584},
year = {2019}
}
@article{Gilpin2019,
abstract = {There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we describe foundational concepts of explainability and show how they can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.},
archivePrefix = {arXiv},
arxivId = {1806.00069},
author = {Gilpin, Leilani H. and Bau, David and Yuan, Ben Z. and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
doi = {10.1109/DSAA.2018.00018},
eprint = {1806.00069},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gilpin et al. - 2019 - Explaining explanations An overview of interpretability of machine learning(2).pdf:pdf},
isbn = {9781538650905},
journal = {Proceedings - 2018 IEEE 5th International Conference on Data Science and Advanced Analytics, DSAA 2018},
keywords = {Deep learning and deep analytics,Fairness and transparency in data science,Machine learning theories,Models and systems},
pages = {80--89},
title = {{Explaining explanations: An overview of interpretability of machine learning}},
year = {2019}
}
@article{Selvaraju2016,
abstract = {We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing input regions that are 'important' for predictions -- or visual explanations. Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses class-specific gradient information to localize important regions. These localizations are combined with existing pixel-space visualizations to create a novel high-resolution and class-discriminative visualization called Guided Grad-CAM. These methods help better understand CNN-based models, including image captioning and visual question answering (VQA) models. We evaluate our visual explanations by measuring their ability to discriminate between classes, to inspire trust in humans, and their correlation with occlusion maps. Grad-CAM provides a new way to understand CNN-based models. We have released code, an online demo hosted on CloudCV, and a full version of this extended abstract.},
archivePrefix = {arXiv},
arxivId = {1611.07450},
author = {Selvaraju, Ramprasaath R and Das, Abhishek and Vedantam, Ramakrishna and Cogswell, Michael and Parikh, Devi and Batra, Dhruv},
eprint = {1611.07450},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Selvaraju et al. - 2016 - Grad-CAM Why did you say that.pdf:pdf},
pages = {1--4},
title = {{Grad-CAM: Why did you say that?}},
url = {http://arxiv.org/abs/1611.07450},
year = {2016}
}
@article{Bach2015,
abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest.We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
author = {Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'{e}}goire and Klauschen, Frederick and M{\"{u}}ller, Klaus Robert and Samek, Wojciech},
doi = {10.1371/journal.pone.0130140},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bach et al. - 2015 - On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {7},
pages = {1--46},
pmid = {26161953},
title = {{On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation}},
volume = {10},
year = {2015}
}
@article{Tang2019,
abstract = {Neuropathologists assess vast brain areas to identify diverse and subtly-differentiated morphologies. Standard semi-quantitative scoring approaches, however, are coarse-grained and lack precise neuroanatomic localization. We report a proof-of-concept deep learning pipeline that identifies specific neuropathologies—amyloid plaques and cerebral amyloid angiopathy—in immunohistochemically-stained archival slides. Using automated segmentation of stained objects and a cloud-based interface, we annotate > 70,000 plaque candidates from 43 whole slide images (WSIs) to train and evaluate convolutional neural networks. Networks achieve strong plaque classification on a 10-WSI hold-out set (0.993 and 0.743 areas under the receiver operating characteristic and precision recall curve, respectively). Prediction confidence maps visualize morphology distributions at high resolution. Resulting network-derived amyloid beta (A$\beta$)-burden scores correlate well with established semi-quantitative scores on a 30-WSI blinded hold-out. Finally, saliency mapping demonstrates that networks learn patterns agreeing with accepted pathologic features. This scalable means to augment a neuropathologist's ability suggests a route to neuropathologic deep phenotyping.},
author = {Tang, Ziqi and Chuang, Kangway V and DeCarli, Charles and Jin, Lee Way and Beckett, Laurel and Keiser, Michael J and Dugger, Brittany N},
doi = {10.1038/s41467-019-10212-1},
isbn = {4146701910},
issn = {20411723},
journal = {Nature Communications},
number = {1},
pages = {1--14},
pmid = {31092819},
title = {{Interpretable classification of Alzheimer's disease pathologies with a convolutional neural network pipeline}},
volume = {10},
year = {2019}
}
@article{Qin2018,
abstract = {Nowadays, the Convolutional Neural Networks (CNNs) have achieved impressive performance on many computer vision related tasks, such as object detection, image recognition, image retrieval, etc. These achievements benefit from the CNNs' outstanding capability to learn the input features with deep layers of neuron structures and iterative training process. However, these learned features are hard to identify and interpret from a human vision perspective, causing a lack of understanding of the CNNs' internal working mechanism. To improve the CNN interpretability, the CNN visualization is well utilized as a qualitative analysis method, which translates the internal features into visually perceptible patterns. And many CNN visualization works have been proposed in the literature to interpret the CNN in perspectives of network structure, operation, and semantic concept. In this paper, we expect to provide a comprehensive survey of several representative CNN visualization methods, including Activation Maximization, Network Inversion, Deconvolutional Neural Networks (DeconvNet), and Network Dissection based visualization. These methods are presented in terms of motivations, algorithms, and experiment results. Based on these visualization methods, we also discuss their practical applications to demonstrate the significance of the CNN interpretability in areas of network design, optimization, security enhancement, etc.},
archivePrefix = {arXiv},
arxivId = {1804.11191},
author = {Qin, Zhuwei and Yu, Fuxun and Liu, Chenchen and Chen, Xiang},
doi = {10.3934/mfc.2018008},
eprint = {1804.11191},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Qin et al. - 2018 - How convolutional neural networks see the world - A survey of convolutional neural network visualization methods.pdf:pdf},
issn = {23318422},
journal = {arXiv},
keywords = {CNN feature,CNN visualization,Convolutional neural network,Deep learning,Network interpretability},
number = {2},
pages = {149--180},
title = {{How convolutional neural networks see the world - A survey of convolutional neural network visualization methods}},
volume = {1},
year = {2018}
}
@article{JoseOramas2017,
abstract = {Interpretation and explanation of deep models is critical towards wide adoption of systems that rely on them. In this paper, we propose a novel scheme for both interpretation as well as explanation in which, given a pretrained model, we automatically identify internal features relevant for the set of classes considered by the model, without relying on additional annotations. We interpret the model through average visualizations of this reduced set of features. Then, at test time, we explain the network prediction by accompanying the predicted class label with supporting visualizations derived from the identified features. In addition, we propose a method to address the artifacts introduced by strided operations in deconvNet-based visualizations. Moreover, we introduce an8Flower, a dataset specifically designed for objective quantitative evaluation of methods for visual explanation. Experiments on the MNIST, ILSVRC12, Fashion144k and an8Flower datasets show that our method produces detailed explanations with good coverage of relevant features of the classes of interest.},
archivePrefix = {arXiv},
arxivId = {1712.06302},
author = {{Jos{\'{e}} Oramas}, M. and Wang, Kaili and Tuytelaars, Tinne},
eprint = {1712.06302},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jos{\'{e}} Oramas, Wang, Tuytelaars - 2017 - Visual explanation by interpretation Improving visual feedback capabilities of deep neural netwo.pdf:pdf},
issn = {23318422},
journal = {arXiv},
number = {2015},
title = {{Visual explanation by interpretation: Improving visual feedback capabilities of deep neural networks}},
year = {2017}
}
@article{Goodman2017,
abstract = {In April 2016, for the first time in more than two decades, the European Parliament adopted a set of comprehensive regulations for the collection, storage, and use of personal information, the General Data Protection Regulation (GDPR)1 (European Union, Parliament and Council 2016). The new regulation has been described as a "Copernican Revolution" in data-protection law, "seeking to shift its focus away from paperbased, bureaucratic requirements and towards compliance in practice, harmonization of the law, and individual empowerment" (Kuner 2012). Much in the regulations is clearly aimed at perceived gaps and inconsistencies in the European Union's (EU) current approach to data protection. This includes, for example, the codification of the "right to be forgotten" (Article 17), and regulations for foreign companies collecting data from European citizens (Article 44).},
archivePrefix = {arXiv},
arxivId = {1606.08813},
author = {Goodman, Bryce and Flaxman, Seth},
doi = {10.1609/aimag.v38i3.2741},
eprint = {1606.08813},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Goodman, Flaxman - 2017 - European union regulations on algorithmic decision making and a right to explanation.pdf:pdf},
issn = {07384602},
journal = {AI Magazine},
number = {3},
pages = {50--57},
title = {{European union regulations on algorithmic decision making and a "right to explanation"}},
volume = {38},
year = {2017}
}
@article{Lin2019a,
abstract = {There has been a significant surge of interest recently around the concept of explainable artificial intelligence (XAI), where the goal is to produce an interpretation for a decision made by a machine learning algorithm. Of particular interest is the interpretation of how deep neural networks make decisions, given the complexity and `black box' nature of such networks. Given the infancy of the field, there has been very limited exploration into the assessment of the performance of explainability methods, with most evaluations centered around subjective visual interpretation of the produced interpretations. In this study, we explore a more machine-centric strategy for quantifying the performance of explainability methods on deep neural networks via the notion of decision-making impact analysis. We introduce two quantitative performance metrics: i) Impact Score, which assesses the percentage of critical factors with either strong confidence reduction impact or decision changing impact, and ii) Impact Coverage, which assesses the percentage coverage of adversarially impacted factors in the input. A comprehensive analysis using this approach was conducted on several state-of-the-art explainability methods (LIME, SHAP, Expected Gradients, GSInquire) on a ResNet-50 deep convolutional neural network using a subset of ImageNet for the task of image classification. Experimental results show that the critical regions identified by LIME within the tested images had the lowest impact on the decision-making process of the network ($\sim$38%), with progressive increase in decision-making impact for SHAP ($\sim$44%), Expected Gradients ($\sim$51%), and GSInquire ($\sim$76%). While by no means perfect, the hope is that the proposed machine-centric strategy helps push the conversation forward towards better metrics for evaluating explainability methods and improve trust in deep neural networks.},
archivePrefix = {arXiv},
arxivId = {1910.07387},
author = {Lin, Zhong Qiu and Shafiee, Mohammad Javad and Bochkarev, Stanislav and Jules, Michael St. and Wang, Xiao Yu and Wong, Alexander},
eprint = {1910.07387},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lin et al. - 2019 - Do Explanations Reflect Decisions A Machine-centric Strategy to Quantify the Performance of Explainability Algori(2).pdf:pdf},
pages = {1--9},
title = {{Do Explanations Reflect Decisions? A Machine-centric Strategy to Quantify the Performance of Explainability Algorithms}},
url = {http://arxiv.org/abs/1910.07387},
year = {2019}
}
@article{Manuscript2020,
abstract = {Cladding austenitic stainless steels are trendy these days in pressure vessels to enhance surface qualities. In this work, austenitic stainless steel clad layers deposited by flux cored arc welding process on structural steel plates used in boiler construction are investigated. To facilitate this, composite clad layers are produced with 316L stainless steel deposits on low carbon structural steel plates based on the design of experiments. Results exhibit an incremental trend of thermal conductivity with respect to percentage of weld dilution. A linear mathematical relationship is established between the percentage of weld dilution and post-weld thermal conductivity of clad layers for the prediction of thermal conductivity of clad layer deposits. These results could be supportive in the fabrication industries for producing energy efficient thermal equipment},
author = {Manuscript, Accepted},
title = {{Ac ce us}},
year = {2020}
}
@article{Kusner2017,
abstract = {Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.},
archivePrefix = {arXiv},
arxivId = {1703.06856},
author = {Kusner, Matt and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
eprint = {1703.06856},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kusner et al. - 2017 - Counterfactual fairness(2).pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {Nips},
pages = {4067--4077},
title = {{Counterfactual fairness}},
volume = {2017-Decem},
year = {2017}
}
@article{Shi2020,
abstract = {Explainability is a gateway between Artificial Intelligence and society as the current popular deep learning models are generally weak in explaining the reasoning process and prediction results. Local Interpretable Model-agnostic Explanation (LIME) is a recent technique that explains the predictions of any classifier faithfully by learning an interpretable model locally around the prediction. However, the sampling operation in the standard implementation of LIME is defective. Perturbed samples are generated from a uniform distribution, ignoring the complicated correlation between features. This paper proposes a novel Modified Perturbed Sampling operation for LIME (MPS-LIME), which is formalized as the clique set construction problem. In image classification, MPS-LIME converts the superpixel image into an undirected graph. Various experiments show that the MPS-LIME explanation of the black-box model achieves much better performance in terms of understandability, fidelity, and efficiency.},
archivePrefix = {arXiv},
arxivId = {2002.07434},
author = {Shi, Sheng and Zhang, Xinfeng and Fan, Wei},
eprint = {2002.07434},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shi, Zhang, Fan - 2020 - A modified perturbed sampling method for local interpretable model-agnostic explanation.pdf:pdf},
issn = {23318422},
journal = {arXiv},
keywords = {Clique,Explainable AI,Feature correlations,Local fidelity,Perturbed sampling},
pages = {1--5},
title = {{A modified perturbed sampling method for local interpretable model-agnostic explanation}},
year = {2020}
}
@article{Artelt2020,
abstract = {The increasing deployment of machine learning as well as legal regulations such as EU's GDPR cause a need for user-friendly explanations of decisions proposed by machine learning models. Counterfactual explanations are considered as one of the most popular techniques to explain a specific decision of a model. While the computation of “arbitrary” counterfactual explanations is well studied, it is still an open research problem how to efficiently compute plausible and feasible counterfactual explanations. We build upon recent work and propose and study a formal definition of plausible counterfactual explanations. In particular, we investigate how to use density estimators for enforcing plausibility and feasibility of counterfactual explanations. For the purpose of efficient computations, we propose convex density constraints that ensure that the resulting counterfactual is located in a region of the data space of high density.},
archivePrefix = {arXiv},
arxivId = {2002.04862},
author = {Artelt, Andr{\'{e}} and Hammer, Barbara},
doi = {10.1007/978-3-030-61609-0_28},
eprint = {2002.04862},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Artelt, Hammer - 2020 - Convex Density Constraints for Computing Plausible Counterfactual Explanations(2).pdf:pdf},
isbn = {9783030616083},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Counterfactual explanations,Transparency & interpretability,XAI},
pages = {353--365},
title = {{Convex Density Constraints for Computing Plausible Counterfactual Explanations}},
volume = {12396 LNCS},
year = {2020}
}
@article{Schwarzenberg2019,
abstract = {Representations in the hidden layers of Deep Neural Networks (DNN) are often hard to interpret since it is difficult to project them into an interpretable domain. Graph Convolutional Networks (GCN) allow this projection, but existing explainability methods do not exploit this fact, i.e. do not focus their explanations on intermediate states. In this work, we present a novel method that traces and visualizes features that contribute to a classification decision in the visible and hidden layers of a GCN. Our method exposes hidden cross-layer dynamics in the input graph structure. We experimentally demonstrate that it yields meaningful layerwise explanations for a GCN sentence classifier.},
archivePrefix = {arXiv},
arxivId = {1909.10911},
author = {Schwarzenberg, Robert and Hu, Marc &die;bner and Harbecke, David and Alt, Christoph and Hennig, Leonhard},
doi = {10.18653/v1/d19-5308},
eprint = {1909.10911},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schwarzenberg et al. - 2019 - Layerwise relevance visualization in convolutional text graph classifiers.pdf:pdf},
isbn = {9781950737864},
journal = {EMNLP-IJCNLP 2019 - Graph-Based Methods for Natural Language Processing - Proceedings of the 13th Workshop},
pages = {58--62},
title = {{Layerwise relevance visualization in convolutional text graph classifiers}},
year = {2019}
}
@article{Goyal2019,
abstract = {How can we understand classification decisions made by deep neural networks? Many existing explainability methods rely solely on correlations and fail to account for confounding, which may result in potentially misleading explanations. To overcome this problem, we define the Causal Concept Effect (CaCE) as the causal effect of (the presence or absence of) a human-interpretable concept on a deep neural net's predictions. We show that the CaCE measure can avoid errors stemming from confounding. Estimating CaCE is difficult in situations where we cannot easily simulate the do-operator. To mitigate this problem, we use a generative model, specifically a Variational AutoEncoder (VAE), to measure VAE-CaCE. In an extensive experimental analysis, we show that the VAE-CaCE is able to estimate the true concept causal effect, compared to baselines for a number of datasets including high dimensional images.},
archivePrefix = {arXiv},
arxivId = {1907.07165},
author = {Goyal, Yash and Feder, Amir and Shalit, Uri and Kim, Been},
eprint = {1907.07165},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Goyal et al. - 2019 - Explaining Classifiers with Causal Concept Effect (CaCE).pdf:pdf},
title = {{Explaining Classifiers with Causal Concept Effect (CaCE)}},
url = {http://arxiv.org/abs/1907.07165},
year = {2019}
}
@article{Kazhdan2020,
abstract = {Recurrent Neural Networks (RNNs) have achieved remarkable performance on a range of tasks. A key step to further empowering RNN-based approaches is improving their explainability and interpretability. In this work we present MEME: a model extraction approach capable of approximating RNNs with interpretable models represented by human-understandable concepts and their interactions. We demonstrate how MEME can be applied to two multivariate, continuous data case studies: Room Occupation Prediction, and In-Hospital Mortality Prediction. Using these case-studies, we show how our extracted models can be used to interpret RNNs both locally and globally, by approximating RNN decision-making via interpretable concept interactions.},
archivePrefix = {arXiv},
arxivId = {2012.06954},
author = {Kazhdan, Dmitry and Dimanov, Botty and Jamnik, Mateja and Li{\`{o}}, Pietro},
eprint = {2012.06954},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kazhdan et al. - 2020 - MEME Generating RNN Model Explanations via Model Extraction.pdf:pdf},
pages = {1--17},
title = {{MEME: Generating RNN Model Explanations via Model Extraction}},
url = {http://arxiv.org/abs/2012.06954},
year = {2020}
}
@article{Bau2018,
abstract = {Generative Adversarial Networks (GANs) have recently achieved impressive results for many real-world applications, and many GAN variants have emerged with improvements in sample quality and training stability. However, they have not been well visualized or understood. How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models. In this work, we present an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. We first identify a group of interpretable units that are closely related to object concepts using a segmentation-based network dissection method. Then, we quantify the causal effect of interpretable units by measuring the ability of interventions to control objects in the output. We examine the contextual relationship between these units and their surroundings by inserting the discovered object concepts into new images. We show several practical applications enabled by our framework, from comparing internal representations across different layers, models, and datasets, to improving GANs by locating and removing artifact-causing units, to interactively manipulating objects in a scene. We provide open source interpretation tools to help researchers and practitioners better understand their GAN models∗.},
archivePrefix = {arXiv},
arxivId = {1811.10597},
author = {Bau, David and Zhu, Jun Yan and Strobelt, Hendrik and Zhou, Bolei and Tenenbaum, Joshua B. and Freeman, William T. and Torralba, Antonio},
eprint = {1811.10597},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bau et al. - 2018 - Gan Dissection Visualizing and Understanding Generative Adversarial Networks.pdf:pdf},
issn = {23318422},
journal = {arXiv},
title = {{Gan Dissection: Visualizing and Understanding Generative Adversarial Networks}},
year = {2018}
}
@article{Che2016,
abstract = {Exponential surge in health care data, such as longitudinal data from electronic health records (EHR), sensor data from intensive care unit (ICU), etc., is providing new opportunities to discover meaningful data-driven characteristics and patterns ofdiseases. Recently, deep learning models have been employedfor many computational phenotyping and healthcare prediction tasks to achieve state-of-the-art performance. However, deep models lack interpretability which is crucial for wide adoption in medical research and clinical decision-making. In this paper, we introduce a simple yet powerful knowledge-distillation approach called interpretable mimic learning, which uses gradient boosting trees to learn interpretable models and at the same time achieves strong prediction performance as deep learning models. Experiment results on Pediatric ICU dataset for acute lung injury (ALI) show that our proposed method not only outperforms state-of-the-art approaches for morality and ventilator free days prediction tasks but can also provide interpretable models to clinicians.},
author = {Che, Zhengping and Purushotham, Sanjay and Khemani, Robinder and Liu, Yan},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Che et al. - 2016 - Interpretable Deep Models for ICU Outcome Prediction.pdf:pdf},
issn = {1942597X},
journal = {AMIA ... Annual Symposium proceedings. AMIA Symposium},
pages = {371--380},
pmid = {28269832},
title = {{Interpretable Deep Models for ICU Outcome Prediction}},
volume = {2016},
year = {2016}
}
@article{Manuscript2020,
abstract = {Cladding austenitic stainless steels are trendy these days in pressure vessels to enhance surface qualities. In this work, austenitic stainless steel clad layers deposited by flux cored arc welding process on structural steel plates used in boiler construction are investigated. To facilitate this, composite clad layers are produced with 316L stainless steel deposits on low carbon structural steel plates based on the design of experiments. Results exhibit an incremental trend of thermal conductivity with respect to percentage of weld dilution. A linear mathematical relationship is established between the percentage of weld dilution and post-weld thermal conductivity of clad layers for the prediction of thermal conductivity of clad layer deposits. These results could be supportive in the fabrication industries for producing energy efficient thermal equipment},
author = {Manuscript, Accepted},
title = {{Ac ce us}},
year = {2020}
}
@article{Fischer2020,
abstract = {Much of the field of Machine Learning exhibits a prominent set of failure modes, including vulnerability to adversarial examples, poor out-of-distribution (OoD) detection, miscalibration, and willingness to memorize random labelings of datasets. We characterize these as failures of robust generalization, which extends the traditional measure of generalization as accuracy or related metrics on a held-out set. We hypothesize that these failures to robustly generalize are due to the learning systems retaining too much information about the training data. To test this hypothesis, we propose the Minimum Necessary Information (MNI) criterion for evaluating the quality of a model. In order to train models that perform well with respect to the MNI criterion, we present a new objective function, the Conditional Entropy Bottleneck (CEB), which is closely related to the Information Bottleneck (IB). We experimentally test our hypothesis by comparing the performance of CEB models with deterministic models and Variational Information Bottleneck (VIB) models on a variety of different datasets and robustness challenges. We find strong empirical evidence supporting our hypothesis that MNI models improve on these problems of robust generalization.},
archivePrefix = {arXiv},
arxivId = {2002.05379},
author = {Fischer, Ian},
doi = {10.3390/e22090999},
eprint = {2002.05379},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fischer - 2020 - The conditional entropy bottleneck.pdf:pdf},
issn = {10994300},
journal = {Entropy},
keywords = {Information bottleneck,Information theory,Machine learning},
number = {9},
title = {{The conditional entropy bottleneck}},
volume = {22},
year = {2020}
}
@article{Jin2019,
abstract = {The impressive performance of neural networks on natural language processing tasks attributes to their ability to model complicated word and phrase compositions. To explain how the model handles semantic compositions, we study hierarchical explanation of neural network predictions. We identify non-additivity and context independent importance attributions within hierarchies as two desirable properties for highlighting word and phrase compositions. We show some prior efforts on hierarchical explanations, e.g. contextual decomposition, do not satisfy the desired properties mathematically, leading to inconsistent explanation quality in different models. In this paper, we start by proposing a formal and general way to quantify the importance of each word and phrase. Following the formulation, we propose Sampling and Contextual Decomposition (SCD) algorithm and Sampling and Occlusion (SOC) algorithm. Human and metrics evaluation on both LSTM models and BERT Transformer models on multiple datasets show that our algorithms outperform prior hierarchical explanation algorithms. Our algorithms help to visualize semantic composition captured by models, extract classification rules and improve human trust of models. Project page: https://inklab.usc.edu/hiexpl/},
archivePrefix = {arXiv},
arxivId = {1911.06194},
author = {Jin, Xisen and Wei, Zhongyu and Du, Junyi and Xue, Xiangyang and Ren, Xiang},
eprint = {1911.06194},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jin et al. - 2019 - Towards Hierarchical Importance Attribution Explaining Compositional Semantics for Neural Sequence Models(2).pdf:pdf},
number = {Cd},
pages = {1--15},
title = {{Towards Hierarchical Importance Attribution: Explaining Compositional Semantics for Neural Sequence Models}},
url = {http://arxiv.org/abs/1911.06194},
year = {2019}
}
@article{Pedreschi2018,
abstract = {Black box systems for automated decision making, often based on machine learning over (big) data, map a user's features into a class or a score without exposing the reasons why. This is problematic not only for lack of transparency, but also for possible biases hidden in the algorithms, due to human prejudices and collection artifacts hidden in the training data, which may lead to unfair or wrong decisions. We introduce the local-to-global framework for black box explanation, a novel approach with promising early results, which paves the road for a wide spectrum of future developments along three dimensions: (i) the language for expressing explanations in terms of highly expressive logic-based rules, with a statistical and causal interpretation; (ii) the inference of local explanations aimed at revealing the logic of the decision adopted for a specific instance by querying and auditing the black box in the vicinity of the target instance; (iii), the bottom-up generalization of the many local explanations into simple global ones, with algorithms that optimize the quality and comprehensibility of explanations.},
archivePrefix = {arXiv},
arxivId = {1806.09936},
author = {Pedreschi, Dino and Giannotti, Fosca and Guidotti, Riccardo and Monreale, Anna and Pappalardo, Luca and Ruggieri, Salvatore and Turini, Franco},
eprint = {1806.09936},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pedreschi et al. - 2018 - Open the Black Box Data-Driven Explanation of Black Box Decision Systems(2).pdf:pdf},
number = {1},
pages = {1--15},
title = {{Open the Black Box Data-Driven Explanation of Black Box Decision Systems}},
url = {http://arxiv.org/abs/1806.09936},
volume = {1},
year = {2018}
}
@article{Bastani2017,
abstract = {The ability to interpret machine learning models has become increasingly important now that machine learning is used to inform consequential decisions. We propose an approach called model extraction for interpreting complex, blackbox models. Our approach approximates the complex model using a much more interpretable model; as long as the approximation quality is good, then statistical properties of the complex model are reflected in the interpretable model. We show how model extraction can be used to understand and debug random forests and neural nets trained on several datasets from the UCI Machine Learning Repository, as well as control policies learned for several classical reinforcement learning problems.},
archivePrefix = {arXiv},
arxivId = {1706.09773},
author = {Bastani, Osbert and Kim, Carolyn and Bastani, Hamsa},
eprint = {1706.09773},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bastani, Kim, Bastani - 2017 - Interpretability via model extraction.pdf:pdf},
issn = {23318422},
journal = {arXiv},
title = {{Interpretability via model extraction}},
year = {2017}
}
@article{Vercheval2021,
abstract = {Conditional Variational Auto Encoders (VAE) are gathering significant attention as an Explainable Artificial Intelligence (XAI) tool. The codes in the latent space provide a theoretically sound way to produce counterfactuals, i.e. alterations resulting from an intervention on a targeted semantic feature. To be applied on real images more complex models are needed, such as Hierarchical CVAE. This comes with a challenge as the naive conditioning is no longer effective. In this paper we show how relaxing the effect of the posterior leads to successful counterfactuals and we introduce VAEX an Hierarchical VAE designed for this approach that can visually audit a classifier in applications.},
archivePrefix = {arXiv},
arxivId = {2102.00854},
author = {Vercheval, Nicolas and Pizurica, Aleksandra},
eprint = {2102.00854},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vercheval, Pizurica - 2021 - Hierarchical Variational Autoencoder for Visual Counterfactuals(2).pdf:pdf},
pages = {1--5},
title = {{Hierarchical Variational Autoencoder for Visual Counterfactuals}},
url = {http://arxiv.org/abs/2102.00854},
year = {2021}
}
@article{Choo2018,
abstract = {Recently, deep learning has been advancing the state of the art in artificial intelligence to a new level, and humans rely on artificial intelligence techniques more than ever. However, even with such unprecedented advancements, the lack of explanation regarding the decisions made by deep learning models and absence of control over their internal processes act as major drawbacks in critical decision-making processes, such as precision medicine and law enforcement. In response, efforts are being made to make deep learning interpretable and controllable by humans. This article reviews visual analytics, information visualization, and machine learning perspectives relevant to this aim, and discusses potential challenges and future research directions.},
archivePrefix = {arXiv},
arxivId = {1804.02527},
author = {Choo, Jaegul and Liu, Shixia},
doi = {10.1109/MCG.2018.042731661},
eprint = {1804.02527},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Choo, Liu - 2018 - Visual Analytics for Explainable Deep Learning(2).pdf:pdf},
issn = {02721716},
journal = {IEEE Computer Graphics and Applications},
keywords = {computer graphics,deep learning,explainable deep learning,interactive visualization},
number = {4},
pages = {84--92},
pmid = {29975192},
publisher = {IEEE},
title = {{Visual Analytics for Explainable Deep Learning}},
volume = {38},
year = {2018}
}
@article{Gosiewska2019,
abstract = {Explainable Artificial Intelligence (XAI)has received a great deal of attention recently. Explainability is being presented as a remedy for the distrust of complex and opaque models. Model agnostic methods such as LIME, SHAP, or Break Down promise instance-level interpretability for any complex machine learning model. But how faithful are these additive explanations? Can we rely on additive explanations for non-additive models? In this paper, we (1) examine the behavior of the most popular instance-level explanations under the presence of interactions, (2) introduce a new method that detects interactions for instance-level explanations, (3) perform a large scale benchmark to see how frequently additive explanations may be misleading.},
archivePrefix = {arXiv},
arxivId = {1903.11420},
author = {Gosiewska, Alicja and Biecek, Przemyslaw},
eprint = {1903.11420},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gosiewska, Biecek - 2019 - iBreakDown Uncertainty of Model Explanations for Non-additive Predictive Models.pdf:pdf},
number = {April},
title = {{iBreakDown: Uncertainty of Model Explanations for Non-additive Predictive Models}},
url = {http://arxiv.org/abs/1903.11420},
year = {2019}
}
@article{Sharma2019,
abstract = {As artificial intelligence plays an increasingly important role in our society, there are ethical and moral obligations for both businesses and researchers to ensure that their machine learning models are designed, deployed, and maintained responsibly. These models need to be rigorously audited for fairness, robustness, transparency, and interpretability. A variety of methods have been developed that focus on these issues in isolation, however, managing these methods in conjunction with model development can be cumbersome and timeconsuming. In this paper, we introduce a unified and model-agnostic approach to address these issues: Counterfactual Explanations for Robustness, Transparency, Interpretability, and Fairness of Artificial Intelligence models (CERTIFAI). Unlike previous methods in this domain, CERTIFAI is a general tool that can be applied to any black-box model and any type of input data. Given a model and an input instance, CERTIFAI uses a custom genetic algorithm to generate counterfactuals: instances close to the input that change the prediction of the model. We demonstrate how these counterfactuals can be used to examine issues of robustness, interpretability, transparency, and fairness. Additionally, we introduce CERScore, the first black-box model robustness score that performs comparably to methods that have access to model internals.},
archivePrefix = {arXiv},
arxivId = {1905.07857},
author = {Sharma, Shubham and Henderson, Jette and Ghosh, Joydeep},
doi = {10.1145/3375627.3375812},
eprint = {1905.07857},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sharma, Henderson, Ghosh - 2019 - CERTIFAI Counterfactual Explanations for Robustness, Transparency, Interpretability, and Fairness of A.pdf:pdf},
title = {{CERTIFAI: Counterfactual Explanations for Robustness, Transparency, Interpretability, and Fairness of Artificial Intelligence models}},
url = {http://arxiv.org/abs/1905.07857%0Ahttp://dx.doi.org/10.1145/3375627.3375812},
year = {2019}
}
@article{Samek2016,
abstract = {Complex nonlinear models such as deep neural network (DNNs) have become an important tool for image classification, speech recognition, natural language processing, and many other fields of application. These models however lack transparency due to their complex nonlinear structure and to the complex data distributions to which they typically apply. As a result, it is difficult to fully characterize what makes these models reach a particular decision for a given input. This lack of transparency can be a drawback, especially in the context of sensitive applications such as medical analysis or security. In this short paper, we summarize a recent technique introduced by Bach et al. [1] that explains predictions by decomposing the classification decision of DNN models in terms of input variables.},
archivePrefix = {arXiv},
arxivId = {1611.08191},
author = {Samek, Wojciech and Montavon, Gr{\'{e}}goire and Binder, Alexander and Lapuschkin, Sebastian and M{\"{u}}ller, Klaus-Robert},
eprint = {1611.08191},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Samek et al. - 2016 - Interpreting the Predictions of Complex ML Models by Layer-wise Relevance Propagation.pdf:pdf},
number = {Nips},
title = {{Interpreting the Predictions of Complex ML Models by Layer-wise Relevance Propagation}},
url = {http://arxiv.org/abs/1611.08191},
year = {2016}
}
@article{Gaur2021,
abstract = {The recent series of innovations in deep learning (DL) have shown enormous potential to impact individuals and society, both positively and negatively. DL models utilizing massive computing power and enormous datasets have significantly outperformed prior historical benchmarks on increasingly difficult, well-defined research tasks across technology domains such as computer vision, natural language processing, and human-computer interactions. However, DL's black-box nature and over-reliance on massive amounts of data condensed into labels and dense representations pose challenges for interpretability and explainability. Furthermore, DLs have not proven their ability to effectively utilize relevant domain knowledge critical to human understanding. This aspect was missing in early data-focused approaches and necessitated knowledge-infused learning (K-iL) to incorporate computational knowledge. This article demonstrates how knowledge, provided as a knowledge graph, is incorporated into DL using K-iL. Through examples from natural language processing applications in healthcare and education, we discuss the utility of K-iL towards interpretability and explainability.},
archivePrefix = {arXiv},
arxivId = {2010.08660},
author = {Gaur, Manas and Faldu, Keyur and Sheth, Amit},
doi = {10.1109/MIC.2020.3031769},
eprint = {2010.08660},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gaur, Faldu, Sheth - 2021 - Semantics of the Black-Box Can Knowledge Graphs Help Make Deep Learning Systems More Interpretable and Ex(2).pdf:pdf},
issn = {19410131},
journal = {IEEE Internet Computing},
number = {1},
pages = {51--59},
title = {{Semantics of the Black-Box: Can Knowledge Graphs Help Make Deep Learning Systems More Interpretable and Explainable?}},
volume = {25},
year = {2021}
}
@article{Chefer2021,
abstract = {Transformers are increasingly dominating multi-modal reasoning tasks, such as visual question answering, achieving state-of-the-art results thanks to their ability to contextualize information using the self-attention and co-attention mechanisms. These attention modules also play a role in other computer vision tasks including object detection and image segmentation. Unlike Transformers that only use self-attention, Transformers with co-attention require to consider multiple attention maps in parallel in order to highlight the information that is relevant to the prediction in the model's input. In this work, we propose the first method to explain prediction by any Transformer-based architecture, including bi-modal Transformers and Transformers with co-attentions. We provide generic solutions and apply these to the three most commonly used of these architectures: (i) pure self-attention, (ii) self-attention combined with co-attention, and (iii) encoder-decoder attention. We show that our method is superior to all existing methods which are adapted from single modality explainability.},
archivePrefix = {arXiv},
arxivId = {2103.15679},
author = {Chefer, Hila and Gur, Shir and Wolf, Lior},
eprint = {2103.15679},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chefer, Gur, Wolf - 2021 - Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers.pdf:pdf},
title = {{Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers}},
url = {http://arxiv.org/abs/2103.15679},
year = {2021}
}
@article{Gulshad2020,
abstract = {In this paper, we aim to explain the decisions of neural networks by utilizing multimodal information. That is counter-intuitive attributes and counter visual examples which appear when perturbed samples are introduced. Different from previous work on interpreting decisions using saliency maps, text, or visual patches we propose to use attributes and counter-attributes, and examples and counter-examples as part of the visual explanations. When humans explain visual decisions they tend to do so by providing attributes and examples. Hence, inspired by the way of human explanations in this paper we provide attribute-based and example-based explanations. Moreover, humans also tend to explain their visual decisions by adding counter-attributes and counter-examples to explain what isnot seen. We introduce directed perturbations in the examples to observe which attribute values change when classifying the examples into the counter classes. This delivers intuitive counter-attributes and counter-examples. Our experiments with both coarse and fine-grained datasets show that attributes provide discriminating and human-understandable intuitive and counter-intuitive explanations.},
archivePrefix = {arXiv},
arxivId = {2001.09671},
author = {Gulshad, Sadaf and Smeulders, Arnold},
doi = {10.1145/3372278.3390672},
eprint = {2001.09671},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gulshad, Smeulders - 2020 - Explaining with counter visual attributes and examples(2).pdf:pdf},
isbn = {9781450370875},
journal = {ICMR 2020 - Proceedings of the 2020 International Conference on Multimedia Retrieval},
keywords = {Adversarial examples,Attributes,Classification,Counter-intuitive attributes,Explainability,Explainable ai,Perturbations},
pages = {35--43},
title = {{Explaining with counter visual attributes and examples}},
year = {2020}
}
@article{Sundararajan2017,
abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms-Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
archivePrefix = {arXiv},
arxivId = {1703.01365},
author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
eprint = {1703.01365},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sundararajan, Taly, Yan - 2017 - Axiomatic attribution for deep networks.pdf:pdf},
isbn = {9781510855144},
journal = {34th International Conference on Machine Learning, ICML 2017},
pages = {5109--5118},
title = {{Axiomatic attribution for deep networks}},
volume = {7},
year = {2017}
}
@article{Chakraborty2017,
author = {Chakraborty, Supriyo and Tomsett, Richard},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chakraborty, Tomsett - 2017 - Interpretability of Deep Learning Models A Survey of Results(2).pdf:pdf},
isbn = {9781728140346},
journal = {2017 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computed, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)},
title = {{Interpretability of Deep Learning Models: A Survey of Results}},
year = {2017}
}
@article{Tangseng2019,
abstract = {Many studies have been conducted so far to build systems for recommending fashion items and outfits. Although they achieve good performances in their respective tasks, most of them cannot explain their judgments to the users, which compromises their usefulness. Toward explainable fashion recommendation, this study proposes a system that is able not only to provide a goodness score for an outfit but also to explain the score by providing reason behind it. For this purpose, we propose a method for quantifying how influential each feature of each item is to the score. Using this influence value, we can identify which item and what feature make the outfit good or bad. We represent the image of each item with a combination of human-interpretable features, and thereby the identification of the most influential item-feature pair gives useful explanation of the output score. To evaluate the performance of this approach, we design an experiment that can be performed without human annotation; we replace a single item-feature pair in an outfit so that the score will decrease, and then we test if the proposed method can detect the replaced item-feature pair correctly using the above influence values. The experimental results show that the proposed method can accurately detect bad items in outfits lowering their scores.},
author = {Tangseng, Pongsate and Okatani, Takayuki},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tangseng, Okatani - 2019 - Toward explainable fashion recommendation.pdf:pdf},
issn = {23318422},
journal = {arXiv},
pages = {2153--2162},
title = {{Toward explainable fashion recommendation}},
year = {2019}
}
@article{Fauvel2020,
abstract = {We present XCM, an eXplainable Convolutional neural network for Multivariate time series classification. XCM is a new compact convolutional neural network which extracts, in parallel, information relative to the observed variables and time from the input data. Thus, XCM architecture enables faithful explainability based on a post-hoc model-specific method (Gradient-weighted Class Activation Mapping), which identifies the observed variables and timestamps of the input data that are important for predictions. Our evaluation firstly shows that XCM outperforms the state-of-the-art multivariate time series classifiers on both the large and small public UEA datasets. Furthermore, following the illustration of the performance and explainability of XCM on a synthetic dataset, we present how XCM can outperform the current most accurate state-of-the-art algorithm on a real-world application while enhancing explainability by providing faithful and more informative explanations.},
archivePrefix = {arXiv},
arxivId = {2009.04796},
author = {Fauvel, Kevin and Lin, Tao and Masson, V{\'{e}}ronique and Fromont, {\'{E}}lisa and Termier, Alexandre},
eprint = {2009.04796},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fauvel et al. - 2020 - XCM An explainable convolutional neural network for multivariate time series classification.pdf:pdf},
issn = {23318422},
journal = {arXiv},
title = {{XCM: An explainable convolutional neural network for multivariate time series classification}},
year = {2020}
}
@article{Strumbelj2009,
abstract = {In this paper, we present a novel method for explaining the decisions of an arbitrary classifier, independent of the type of classifier. The method works at the instance level, decomposing the model's prediction for an instance into the contributions of the attributes' values. We use several artificial data sets and several different types of models to show that the generated explanations reflect the decision-making properties of the explained model and approach the concepts behind the data set as the prediction quality of the model increases. The usefulness of the method is justified by a successful application on a real-world breast cancer recurrence prediction problem. {\textcopyright} 2009 Elsevier B.V. All rights reserved.},
author = {{\v{S}}trumbelj, E. and Kononenko, I. and {Robnik {\v{S}}ikonja}, M.},
doi = {10.1016/j.datak.2009.01.004},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/{\v{S}}trumbelj, Kononenko, Robnik {\v{S}}ikonja - 2009 - Explaining instance classifications with interactions of subsets of feature values.pdf:pdf},
issn = {0169023X},
journal = {Data and Knowledge Engineering},
keywords = {Classification,Data mining,Explanation,Knowledge discovery,Machine learning,Visualization},
number = {10},
pages = {886--904},
publisher = {Elsevier B.V.},
title = {{Explaining instance classifications with interactions of subsets of feature values}},
url = {http://dx.doi.org/10.1016/j.datak.2009.01.004},
volume = {68},
year = {2009}
}
@article{Li2020,
abstract = {Explainable artificial intelligence has been gaining attention in the past few years. However, most existing methods are based on gradients or intermediate features, which are not directly involved in the decision-making process of the classifier. In this paper, we propose a slot attention-based classifier called SCOUTER for transparent yet accurate classification. Two major differences from other attention-based methods include: (a) SCOUTER's explanation is involved in the final confidence for each category, offering more intuitive interpretation, and (b) all the categories have their corresponding positive or negative explanation, which tells "why the image is of a certain category" or "why the image is not of a certain category." We design a new loss tailored for SCOUTER that controls the model's behavior to switch between positive and negative explanations, as well as the size of explanatory regions. Experimental results show that SCOUTER can give better visual explanations while keeping good accuracy on small and medium-sized datasets.},
archivePrefix = {arXiv},
arxivId = {2009.06138},
author = {Li, Liangzhi and Wang, Bowen and Verma, Manisha and Nakashima, Yuta and Kawasaki, Ryo and Nagahara, Hajime},
eprint = {2009.06138},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2020 - SCOUTER Slot Attention-based Classifier for Explainable Image Recognition.pdf:pdf},
title = {{SCOUTER: Slot Attention-based Classifier for Explainable Image Recognition}},
url = {http://arxiv.org/abs/2009.06138},
year = {2020}
}
@article{Zhao2020,
abstract = {Counterfactual explanations are considered, which is to answer {\it why the prediction is class A but not B.} Different from previous optimization based methods, an optimization-free Fast ReAl-time Counterfactual Explanation (FRACE) algorithm is proposed benefiting from the development of multi-domain image to image translation algorithms. Built from starGAN, a transformer is trained as a residual generator conditional on a classifier constrained under a proposal perturbation loss which maintains the content information of the query image, but just the class-specific semantic information is changed. The transformer can transfer the query image to any counterfactual class, and during inference, our explanation can be generated by it only within a forward time. It is fast and can satisfy the real-time practical application. Because of the adversarial training of GAN, our explanation is also more realistic compared to other counterparts. The experimental results demonstrate that our proposal is better than the existing state of the art in terms of quality and speed.},
archivePrefix = {arXiv},
arxivId = {2007.05684},
author = {Zhao, Yunxia},
eprint = {2007.05684},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao - 2020 - Fast Real-time Counterfactual Explanations.pdf:pdf},
pages = {1--8},
title = {{Fast Real-time Counterfactual Explanations}},
url = {http://arxiv.org/abs/2007.05684},
year = {2020}
}
@article{Covert2020a,
abstract = {Understanding the inner workings of complex machine learning models is a longstanding problem and most recent research has focused on local interpretability. To assess the role of individual input features in a global sense, we explore the perspective of defining feature importance through the predictive power associated with each feature. We introduce two notions of predictive power (model-based and universal) and formalize this approach with a framework of additive importance measures, which unifies numerous methods in the literature. We then propose SAGE, a model-agnostic method that quantifies predictive power while accounting for feature interactions. Our experiments show that SAGE can be calculated efficiently and that it assigns more accurate importance values than other methods.},
archivePrefix = {arXiv},
arxivId = {2004.00668},
author = {Covert, Ian and Lundberg, Scott and Lee, Su-In},
eprint = {2004.00668},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Covert, Lundberg, Lee - 2020 - Understanding Global Feature Contributions With Additive Importance Measures.pdf:pdf},
issn = {10495258},
number = {NeurIPS},
title = {{Understanding Global Feature Contributions With Additive Importance Measures}},
url = {http://arxiv.org/abs/2004.00668},
year = {2020}
}
@article{Chattopadhay2018,
abstract = {Over the last decade, Convolutional Neural Network (CNN) models have been highly successful in solving complex vision based problems. However, deep models are perceived as 'black box' methods considering the lack of understanding of their internal functioning. There has been a significant recent interest to develop explainable deep learning models, and this paper is an effort in this direction. Building on a recently proposed method called Grad-CAM, we propose Grad-CAM++ to provide better visual explanations of CNN model predictions (when compared to Grad-CAM), in terms of better localization of objects as well as explaining occurrences of multiple objects of a class in a single image. We provide a mathematical explanation for the proposed method, Grad-CAM++, which uses a weighted combination of the positive partial derivatives of the last convolutional layer feature maps with respect to a specific class score as weights to generate a visual explanation for the class label under consideration. Our extensive experiments and evaluations, both subjective and objective, on standard datasets showed that Grad-CAM++ indeed provides better visual explanations for a given CNN architecture when compared to Grad-CAM.},
archivePrefix = {arXiv},
arxivId = {arXiv:1710.11063v3},
author = {Chattopadhay, Aditya and Sarkar, Anirban and Howlader, Prantik and Balasubramanian, Vineeth N.},
doi = {10.1109/WACV.2018.00097},
eprint = {arXiv:1710.11063v3},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chattopadhay et al. - 2018 - Grad-CAM Improved visual explanations for deep convolutional networks.pdf:pdf},
isbn = {9781538648865},
journal = {Proceedings - 2018 IEEE Winter Conference on Applications of Computer Vision, WACV 2018},
pages = {839--847},
title = {{Grad-CAM++: Improved visual explanations for deep convolutional networks}},
volume = {2018-Janua},
year = {2018}
}
@article{Aas2021,
abstract = {Explaining complex or seemingly simple machine learning models is an important practical problem. We want to explain individual predictions from such models by learning simple, interpretable explanations. Shapley value is a game theoretic concept that can be used for this purpose. The Shapley value framework has a series of desirable theoretical properties, and can in principle handle any predictive model. Kernel SHAP is a computationally efficient approximation to Shapley values in higher dimensions. Like several other existing methods, this approach assumes that the features are independent. Since Shapley values currently suffer from inclusion of unrealistic data instances when features are correlated, the explanations may be very misleading. This is the case even if a simple linear model is used for predictions. In this paper, we extend the Kernel SHAP method to handle dependent features. We provide several examples of linear and non-linear models with various degrees of feature dependence, where our method gives more accurate approximations to the true Shapley values.},
archivePrefix = {arXiv},
arxivId = {1903.10464},
author = {Aas, Kjersti and Jullum, Martin and L{\o}land, Anders},
doi = {10.1016/j.artint.2021.103502},
eprint = {1903.10464},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Aas, Jullum, L{\o}land - 2021 - Explaining individual predictions when features are dependent More accurate approximations to Shapley valu.pdf:pdf},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {Dependence,Feature attribution,Kernel SHAP,Shapley values},
pages = {1--28},
title = {{Explaining individual predictions when features are dependent: More accurate approximations to Shapley values}},
volume = {298},
year = {2021}
}
@article{Zhou2018a,
abstract = {Explanations of the decisions made by a deep neural network are important for human end-users to be able to understand and diagnose the trustworthiness of the system. Current neural networks used for visual recognition are generally used as black boxes that do not provide any human interpretable justification for a prediction. In this work we propose a new framework called Interpretable Basis Decomposition for providing visual explanations for classification networks. By decomposing the neural activations of the input image into semantically interpretable components pre-trained from a large concept corpus, the proposed framework is able to disentangle the evidence encoded in the activation feature vector, and quantify the contribution of each piece of evidence to the final prediction. We apply our framework for providing explanations to several popular networks for visual recognition, and show it is able to explain the predictions given by the networks in a human-interpretable way. The human interpretability of the visual explanations provided by our framework and other recent explanation methods is evaluated through Amazon Mechanical Turk, showing that our framework generates more faithful and interpretable explanations (The code and data are available at https://github.com/CSAILVision/IBD).},
author = {Zhou, Bolei and Sun, Yiyou and Bau, David and Torralba, Antonio},
doi = {10.1007/978-3-030-01237-3_8},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhou et al. - 2018 - Interpretable basis decomposition for visual explanation.pdf:pdf},
isbn = {9783030012366},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {122--138},
title = {{Interpretable basis decomposition for visual explanation}},
volume = {11212 LNCS},
year = {2018}
}
@article{Bharadhwaj2018,
author = {Bharadhwaj, Homanga},
file = {:home/anna/Desktop/prediction explanation/attribution-based/propagation/lrp/LAYER-WISE RELEVANCE PROPAGATION FOR EXPLAINABLE DEEP LEARNING BASED SPEECH RECOGNITION.pdf:pdf},
isbn = {9781538675687},
journal = {2018 IEEE International Symposium on Signal Processing and Information Technology (ISSPIT)},
pages = {168--174},
publisher = {IEEE},
title = {{LAYER-WISE RELEVANCE PROPAGATION FOR EXPLAINABLE DEEP LEARNING BASED SPEECH RECOGNITION Homanga Bharadhwaj Department of Computer Science and Engineering Indian Institute of Technology , Kanpur}},
year = {2018}
}
@article{Karim2019,
abstract = {The discovery of important biomarkers is a significant step towards understanding the molecular mechanisms of carcinogenesis; enabling accurate diagnosis for, and prognosis of, a certain cancer type. Before recommending any diagnosis, genomics data such as gene expressions (GE) and clinical outcomes need to be analyzed. However, complex nature, high dimensionality, and heterogeneity in genomics data make the overall analysis challenging. Convolutional neural networks (CNN) have shown tremendous success in solving such problems. However, neural network models are perceived mostly as 'black box' methods because of their not well-understood internal functioning. However, interpretability is important to provide insights on why a given cancer case has a certain type. Besides, finding the most important biomarkers can help in recommending more accurate treatments and drug repositioning. Moreover, the 'right to explanation' of the EU GDPR gives patients the right to know why and how an algorithm made a diagnosis decision. Hence, in this paper, we propose a new approach called OncoNetExplainer to make explainable predictions of cancer types based on GE data. We used genomics data about 9,074 cancer patients covering 33 different cancer types from the Pan-Cancer Atlas on which we trained CNN and VGG16 networks using guided-gradient class activation maps++ (GradCAM++). Further, we generate class-specific heat maps to identify significant biomarkers and computed feature importance in terms of mean absolute impact to rank top genes across all the cancer types. Quantitative and qualitative analyses show that both models exhibit high confidence at predicting the cancer types correctly giving an average precision of 96.25%. To provide comparisons with the baselines, we identified top genes, and cancer-specific driver genes using gradient boosted trees and SHapley Additive exPlanations (SHAP). Finally, our findings were validated with the annotations provided by the TumorPortal.},
archivePrefix = {arXiv},
arxivId = {1909.04169},
author = {Karim, Md Rezaul and Cochez, Michael and Beyan, Oya and Decker, Stefan and Lange, Christoph},
doi = {10.1109/BIBE.2019.00081},
eprint = {1909.04169},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Karim et al. - 2019 - OncoNetExplainer Explainable predictions of cancer types based on gene expression data.pdf:pdf},
isbn = {9781728146171},
journal = {Proceedings - 2019 IEEE 19th International Conference on Bioinformatics and Bioengineering, BIBE 2019},
keywords = {Cancer genomics,Explainable AI,Feature importance,Gene expression,GradCAM++,Interpretability,Neural networks},
pages = {415--422},
publisher = {IEEE},
title = {{OncoNetExplainer: Explainable predictions of cancer types based on gene expression data}},
year = {2019}
}
@article{Manuscript2020,
abstract = {Cladding austenitic stainless steels are trendy these days in pressure vessels to enhance surface qualities. In this work, austenitic stainless steel clad layers deposited by flux cored arc welding process on structural steel plates used in boiler construction are investigated. To facilitate this, composite clad layers are produced with 316L stainless steel deposits on low carbon structural steel plates based on the design of experiments. Results exhibit an incremental trend of thermal conductivity with respect to percentage of weld dilution. A linear mathematical relationship is established between the percentage of weld dilution and post-weld thermal conductivity of clad layers for the prediction of thermal conductivity of clad layer deposits. These results could be supportive in the fabrication industries for producing energy efficient thermal equipment},
author = {Manuscript, Accepted},
title = {{Ac ce us}},
year = {2020}
}
@article{Chen2019a,
abstract = {Instancewise feature scoring is a method for model interpretation, which yields, for each test instance, a vector of importance scores associated with features. Methods based on the Shapley score have been proposed as a fair way of computing feature attributions, but incur an exponential complexity in the number of features. This combinatorial explosion arises from the definition of Shapley value and prevents these methods from being scalable to large data sets and complex models. We focus on settings in which the data have a graph structure, and the contribution of features to the target variable is well-approximated by a graph-structured factorization. In such settings, we develop two algorithms with linear complexity for instancewise feature importance scoring on black-box models. We establish the relationship of our methods to the Shapley value and a closely related concept known as the Myerson value from cooperative game theory. We demonstrate on both language and image data that our algorithms compare favorably with other methods using both quantitative metrics and human evaluation.},
archivePrefix = {arXiv},
arxivId = {1808.02610},
author = {Chen, Jianbo and Song, Le and Wainwright, Martin J. and Jordan, Michael I.},
eprint = {1808.02610},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2019 - L-Shapley and C-Shapley Efficient model interpretation for structured data.pdf:pdf},
journal = {7th International Conference on Learning Representations, ICLR 2019},
pages = {1--17},
title = {{L-Shapley and C-Shapley: Efficient model interpretation for structured data}},
year = {2019}
}
@article{Covert2020,
abstract = {Researchers have proposed a wide variety of model explanation approaches, but it remains unclear how most methods are related or when one method is preferable to another. We examine the literature and find that many methods are based on a shared principle of explaining by removing - essentially, measuring the impact of removing sets of features from a model. These methods vary in several respects, so we develop a framework for removal-based explanations that characterizes each method along three dimensions: 1) how the method removes features, 2) what model behavior the method explains, and 3) how the method summarizes each feature's influence. Our framework unifies 25 existing methods, including several of the most widely used approaches (SHAP, LIME, Meaningful Perturbations, permutation tests). Exposing the fundamental similarities between these methods empowers users to reason about which tools to use and suggests promising directions for ongoing research in model explainability.},
archivePrefix = {arXiv},
arxivId = {2011.03623},
author = {Covert, Ian and Lundberg, Scott and Lee, Su-In},
eprint = {2011.03623},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Covert, Lundberg, Lee - 2020 - Feature Removal Is a Unifying Principle for Model Explanation Methods(2).pdf:pdf},
pages = {1--21},
title = {{Feature Removal Is a Unifying Principle for Model Explanation Methods}},
url = {http://arxiv.org/abs/2011.03623},
year = {2020}
}
@article{Smilkov2017,
abstract = {Explaining the output of a deep network remains a challenge. In the case of an image classifier, one type of explanation is to identify pixels that strongly influence the final decision. A starting point for this strategy is the gradient of the class score function with respect to the input image. This gradient can be interpreted as a sensitivity map, and there are several techniques that elaborate on this basic idea. This paper makes two contributions: it introduces SMOOTHGRAD, a simple method that can help visually sharpen gradient-based sensitivity maps, and it discusses lessons in the visualization of these maps. We publish the code for our experiments and a website with our results.},
archivePrefix = {arXiv},
arxivId = {1706.03825},
author = {Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Vi{\'{e}}gas, Fernanda and Wattenberg, Martin},
eprint = {1706.03825},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Smilkov et al. - 2017 - SmoothGrad Removing noise by adding noise.pdf:pdf},
issn = {23318422},
journal = {arXiv},
title = {{SmoothGrad: Removing noise by adding noise}},
year = {2017}
}
@article{Chang2019,
abstract = {When an image classifier makes a prediction, which parts of the image are relevant and why? We can rephrase this question to ask: which parts of the image, if they were not seen by the classifier, would most change its decision? Producing an answer requires marginalizing over images that could have been seen but weren't. We can sample plausible image in-fills by conditioning a generative model on the rest of the image. We then optimize to find the image regions that most change the classifier's decision after in-fill. Our approach contrasts with ad-hoc in-filling approaches, such as blurring or injecting noise, which generate inputs far from the data distribution, and ignore informative relationships between different parts of the image. Our method produces more compact and relevant saliency maps, with fewer artifacts compared to previous methods.},
archivePrefix = {arXiv},
arxivId = {1807.08024},
author = {Chang, Chun Hao and Creager, Elliot and Goldenberg, Anna and Duvenaud, David},
eprint = {1807.08024},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chang et al. - 2019 - Explaining image classifiers by counterfactual generation(2).pdf:pdf},
journal = {7th International Conference on Learning Representations, ICLR 2019},
number = {2018},
pages = {1--19},
title = {{Explaining image classifiers by counterfactual generation}},
year = {2019}
}
@article{Fischer2020a,
abstract = {Intuitively, one way to make classifiers more robust to their input is to have them depend less sensitively on their input. The Information Bottleneck (IB) tries to learn compressed representations of input that are still predictive. Scaling up IB approaches to large scale image classification tasks has proved difficult. We demonstrate that the Conditional Entropy Bottleneck (CEB) can not only scale up to large scale image classification tasks, but can additionally improve model robustness. CEB is an easy strategy to implement and works in tandem with data augmentation procedures. We report results of a large scale adversarial robustness study on CIFAR-10, as well as the ImageNet-C Common Corruptions Benchmark, ImageNet-A, and PGD attacks.},
archivePrefix = {arXiv},
arxivId = {2002.05380},
author = {Fischer, Ian and Alemi, Alexander A.},
doi = {10.3390/e22101081},
eprint = {2002.05380},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fischer, Alemi - 2020 - CEB improves model robustness.pdf:pdf},
issn = {10994300},
journal = {Entropy},
keywords = {Information bottleneck,Information theory,Machine learning},
number = {10},
pages = {1--16},
title = {{CEB improves model robustness}},
volume = {22},
year = {2020}
}
@article{Guillemot2020,
abstract = {The lack of transparency of neural networks stays a major break for their use. The Layerwise Relevance Propagation technique builds heat-maps representing the relevance of each input in the model s decision. The relevance spreads backward from the last to the first layer of the Deep Neural Network. Layer-wise Relevance Propagation does not manage normalization layers, in this work we suggest a method to include normalization layers. Specifically, we build an equivalent network fusing normalization layers and convolutional or fully connected layers. Heatmaps obtained with our method on MNIST and CIFAR 10 datasets are more accurate for convolutional layers. Our study also prevents from using Layerwise Relevance Propagation with networks including a combination of connected layers and normalization layer.},
archivePrefix = {arXiv},
arxivId = {2002.11018},
author = {Guillemot, Mathilde and Heusele, Catherine and Korichi, Rodolphe and Schnebert, Sylvianne and Chen, Liming},
eprint = {2002.11018},
file = {:home/anna/Desktop/prediction explanation/attribution-based/propagation/lrp/Breaking Batch Normalization for better explainability of Deep Neural Networks through Layer-wise Relevance Propagation.pdf:pdf},
title = {{Breaking Batch Normalization for better explainability of Deep Neural Networks through Layer-wise Relevance Propagation}},
url = {http://arxiv.org/abs/2002.11018},
year = {2020}
}
@article{Luo2020,
abstract = {Despite recent progress in Graph Neural Networks (GNNs), explaining predictions made by GNNs remains a challenging open problem. The leading method independently addresses the local explanations (i.e., important subgraph structure and node features) to interpret why a GNN model makes the prediction for a single instance, e.g. a node or a graph. As a result, the explanation generated is painstakingly customized for each instance. The unique explanation interpreting each instance independently is not sufficient to provide a global understanding of the learned GNN model, leading to a lack of generalizability and hindering it from being used in the inductive setting. Besides, as it is designed for explaining a single instance, it is challenging to explain a set of instances naturally (e.g., graphs of a given class). In this study, we address these key challenges and propose PGExplainer, a parameterized explainer for GNNs. PGExplainer adopts a deep neural network to parameterize the generation process of explanations, which enables PGExplainer a natural approach to explaining multiple instances collectively. Compared to the existing work, PGExplainer has better generalization ability and can be utilized in an inductive setting easily. Experiments on both synthetic and real-life datasets show highly competitive performance with up to 24.7\% relative improvement in AUC on explaining graph classification over the leading baseline.},
archivePrefix = {arXiv},
arxivId = {2011.04573},
author = {Luo, Dongsheng and Cheng, Wei and Xu, Dongkuan and Yu, Wenchao and Zong, Bo and Chen, Haifeng and Zhang, Xiang},
eprint = {2011.04573},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Luo et al. - 2020 - Parameterized Explainer for Graph Neural Network.pdf:pdf},
number = {NeurIPS},
pages = {1--17},
title = {{Parameterized Explainer for Graph Neural Network}},
url = {http://arxiv.org/abs/2011.04573},
year = {2020}
}
@article{Andrews1995,
abstract = {It is becoming increasingly apparent that, without some form of explanation capability, the full potential of trained artificial neural networks (ANNs) may not be realised. This survey gives an overview of techniques developed to redress this situation. Specifically, the survey focuses on mechanisms, procedures, and algorithms designed to insert knowledge into ANNs (knowledge initialisation), extract rules from trained ANNs (rule extraction), and utilise ANNs to refine existing rule bases (rule refinement). The survey also introduces a new taxonomy for classifying the various techniques, discusses their modus operandi, and delineates criteria for evaluating their efficacy. {\textcopyright} 1995.},
author = {Andrews, Robert and Diederich, Joachim and Tickle, Alan B.},
doi = {10.1016/0950-7051(96)81920-4},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Andrews, Diederich, Tickle - 1995 - Survey and critique of techniques for extracting rules from trained artificial neural networks.pdf:pdf},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {fuzzy neural networks,inferencing,knowledge insertion,rule extraction,rule refinement},
number = {6},
pages = {373--389},
title = {{Survey and critique of techniques for extracting rules from trained artificial neural networks}},
volume = {8},
year = {1995}
}
@article{Ras2018,
abstract = {Issues regarding explainable AI involve four components: users, laws & regulations, explanations and algorithms. Together these components provide a context in which explanation methods can be evaluated regarding their adequacy. The goal of this chapter is to bridge the gap between expert users and lay users. Different kinds of users are identified and their concerns revealed, relevant statements from the General Data Protection Regulation are analyzed in the context of Deep Neural Networks (DNNs), a taxonomy for the classification of existing explanation methods is introduced, and finally, the various classes of explanation methods are analyzed to verify if user concerns are justified. Overall, it is clear that (visual) explanations can be given about various aspects of the influence of the input on the output. However, it is noted that explanation methods or interfaces for lay users are missing and we speculate which criteria these methods/interfaces should satisfy. Finally it is noted that two important concerns are difficult to address with explanation methods: the concern about bias in datasets that leads to biased DNNs, as well as the suspicion about unfair outcomes.
MSC Codes 68-02},
archivePrefix = {arXiv},
arxivId = {1803.07517},
author = {Ras, Gabri{\"{e}}lle and {Van Gerven}, Marcel and Haselager, Pim},
doi = {10.1007/978-3-319-98131-4_2},
eprint = {1803.07517},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ras, Van Gerven, Haselager - 2018 - Explanation methods in deep learning Users, values, concerns and challenges.pdf:pdf},
issn = {23318422},
journal = {arXiv},
pages = {1--15},
title = {{Explanation methods in deep learning: Users, values, concerns and challenges}},
year = {2018}
}
@article{Camburu2020,
abstract = {Deep neural networks are becoming more and more popular due to their revolutionary success in diverse areas, such as computer vision, natural language processing, and speech recognition. However, the decision-making processes of these models are generally not interpretable to users. In various domains, such as healthcare, finance, or law, it is critical to know the reasons behind a decision made by an artificial intelligence system. Therefore, several directions for explaining neural models have recently been explored. In this thesis, I investigate two major directions for explaining deep neural networks. The first direction consists of feature-based post-hoc explanatory methods, that is, methods that aim to explain an already trained and fixed model (post-hoc), and that provide explanations in terms of input features, such as tokens for text and superpixels for images (feature-based). The second direction consists of self-explanatory neural models that generate natural language explanations, that is, models that have a built-in module that generates explanations for the predictions of the model. The contributions in these directions are as follows. First, I reveal certain difficulties of explaining even trivial models using only input features. I show that, despite the apparent implicit assumption that explanatory methods should look for one specific ground-truth feature-based explanation, there is often more than one such explanation for a prediction. I also show that two prevalent classes of explanatory methods target different types of ground-truth explanations without explicitly mentioning it. Moreover, I show that, sometimes, neither of these explanations is enough to provide a complete view of a decision-making process on an instance. Second, I introduce a framework for automatically verifying the faithfulness with which feature-based post-hoc explanatory methods describe the decision-making processes of the models that they aim to explain. This framework relies on the use of a particular type of model that is expected to provide insight into its decision-making process. I analyse potential limitations of this approach and introduce ways to alleviate them. The introduced verification framework is generic and can be instantiated on different tasks and domains to provide off-the-shelf sanity tests that can be used to test feature-based post-hoc explanatory methods. I instantiate this framework on a task of sentiment analysis and provide sanity tests1 on which I present the performances of three popular explanatory methods. Third, to explore the direction of self-explanatory neural models that generate natural language explanations for their predictions, I collected a large dataset of ∼570K human-written natural language explanations on top of the influential Stanford Natural Language Inference (SNLI) dataset. I call this explanation-augmented dataset e-SNLI.2 I do a series of experiments that investigate both the capabilities of neural models to generate correct natural language explanations at test time, and the benefits of providing natural language explanations at training time. Fourth, I show that current self-explanatory models that generate natural language explanations for their own predictions may generate inconsistent explanations, such as “There is a dog in the image.” and “There is no dog in the [same] image.”. Inconsistent explanations reveal either that the explanations are not faithfully describing the decision-making process of the model or that the model learned a flawed decision-making process. I introduce a simple yet effective adversarial framework for sanity checking models against the generation of inconsistent natural language explanations. Moreover, as part of the framework, I address the problem of adversarial attacks with exact target sequences, a scenario that was not previously addressed in sequence-to-sequence attacks, and which can be useful for other tasks in natural language processing. I apply the framework on a state of the art neural model on e-SNLI and show that this model can generate a significant number of inconsistencies. This work paves the way for obtaining more robust neural models accompanied by faithful explanations for their predictions.},
archivePrefix = {arXiv},
arxivId = {2010.01496},
author = {Camburu, Oana Maria},
eprint = {2010.01496},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Camburu - 2020 - Explaining deep neural networks.pdf:pdf},
issn = {23318422},
journal = {arXiv},
title = {{Explaining deep neural networks}},
year = {2020}
}
@inproceedings{Lakkaraju2020,
author = {Lakkaraju, Hima and Adebayo, Julius and Singh, Sameer},
booktitle = {Neural Information Processing Systems},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lakkaraju, Adebayo, Singh - 2020 - Explaining Machine Learning Predictions State-of-the-art, Challenges, Opportunities(2).pdf:pdf},
title = {{Explaining Machine Learning Predictions: State-of-the-art, Challenges, Opportunities}},
year = {2020}
}
@article{Bride2018,
abstract = {The ability to learn from past experience and improve in the future, as well as the ability to reason about the context of problems and extrapolate information from what is known, are two important aspects of Artificial Intelligence. In this paper, we introduce a novel automated reasoning based approach that can extract valuable insights from classification and prediction models obtained via machine learning. A major benefit of the proposed approach is that the user can understand the reason behind the decision-making of machine learning models. This is often as important as good performance. Our technique can also be used to reinforce user-specified requirements in the model as well as to improve the classification and prediction.},
author = {Bride, Hadrien and Dong, Jie and Dong, Jin Song and H{\'{o}}u, Zh{\'{e}}},
doi = {10.1007/978-3-030-02450-5_25},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bride et al. - 2018 - Towards dependable and explainable machine learning using automated reasoning.pdf:pdf},
isbn = {9783030024499},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {412--416},
title = {{Towards dependable and explainable machine learning using automated reasoning}},
volume = {11232 LNCS},
year = {2018}
}
@article{Selvaraju2016,
abstract = {We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing input regions that are 'important' for predictions -- or visual explanations. Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses class-specific gradient information to localize important regions. These localizations are combined with existing pixel-space visualizations to create a novel high-resolution and class-discriminative visualization called Guided Grad-CAM. These methods help better understand CNN-based models, including image captioning and visual question answering (VQA) models. We evaluate our visual explanations by measuring their ability to discriminate between classes, to inspire trust in humans, and their correlation with occlusion maps. Grad-CAM provides a new way to understand CNN-based models. We have released code, an online demo hosted on CloudCV, and a full version of this extended abstract.},
archivePrefix = {arXiv},
arxivId = {1611.07450},
author = {Selvaraju, Ramprasaath R and Das, Abhishek and Vedantam, Ramakrishna and Cogswell, Michael and Parikh, Devi and Batra, Dhruv},
eprint = {1611.07450},
pages = {1--4},
title = {{Grad-CAM: Why did you say that?}},
url = {http://arxiv.org/abs/1611.07450},
year = {2016}
}
@article{Alvarez-Melis2017,
abstract = {We interpret the predictions of any black-box structured input-structured output model around a specific input-output pair. Our method returns an “explanation” consisting of groups of input-output tokens that are causally related. These dependencies are inferred by querying the black-box model with perturbed inputs, generating a graph over tokens from the responses, and solving a partitioning problem to select the most relevant components. We focus the general approach on sequence-to-sequence problems, adopting a variational autoencoder to yield meaningful input perturbations. We test our method across several NLP sequence generation tasks.},
archivePrefix = {arXiv},
arxivId = {1707.01943},
author = {Alvarez-Melis, David and Jaakkola, Tommi S.},
doi = {10.18653/v1/d17-1042},
eprint = {1707.01943},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Alvarez-Melis, Jaakkola - 2017 - A causal framework for explaining the predictions of black-box sequence-to-sequence models.pdf:pdf},
isbn = {9781945626838},
journal = {EMNLP 2017 - Conference on Empirical Methods in Natural Language Processing, Proceedings},
pages = {412--421},
title = {{A causal framework for explaining the predictions of black-box sequence-to-sequence models}},
year = {2017}
}
@article{Kim2016,
abstract = {Example-based explanations are widely used in the effort to improve the interpretability of highly complex distributions. However, prototypes alone are rarely sufficient to represent the GIST of the complexity. In order for users to construct better mental models and understand complex data distributions, we also need criticism to explain what are not captured by prototypes. Motivated by the Bayesian model criticism framework, we develop MMD-critic which efficiently learns prototypes and criticism, designed to aid human interpretability. A human subject pilot study shows that the MMD-critic selects prototypes and criticism that are useful to facilitate human understanding and reasoning. We also evaluate the prototypes selected by MMD-critic via a nearest prototype classifier, showing competitive performance compared to baselines.},
author = {Kim, Been and Khanna, Rajiv and Koyejo, Oluwasanmi},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim, Khanna, Koyejo - 2016 - Examples are not enough, learn to criticize! Criticism for interpretability.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {Nips},
pages = {2288--2296},
title = {{Examples are not enough, learn to criticize! Criticism for interpretability}},
year = {2016}
}
@article{Yang2020,
abstract = {The lack of interpretability is an inevitable problem when using neural network models in real applications. In this paper, a new explainable neural network called GAMI-Net, based on generalized additive models with structured interactions, is proposed to pursue a good balance between prediction accuracy and model interpretability. The GAMI-Net is a disentangled feedforward network with multiple additive subnetworks, where each subnetwork is designed for capturing either one main effect or one pairwise interaction effect. It takes into account three kinds of interpretability constraints, including a) sparsity constraint for selecting the most significant effects for parsimonious representations; b) heredity constraint such that a pairwise interaction could only be included when at least one of its parent effects exists; and c) marginal clarity constraint, in order to make the main and pairwise interaction effects mutually distinguishable. For model estimation, we develop an adaptive training algorithm that firstly fits the main effects to the responses, then fits the structured pairwise interactions to the residuals. Numerical experiments on both synthetic functions and real-world datasets show that the proposed explainable GAMI-Net enjoys superior interpretability while maintaining competitive prediction accuracy in comparison to the explainable boosting machine and other benchmark machine learning models.},
archivePrefix = {arXiv},
arxivId = {2003.07132},
author = {Yang, Zebin and Zhang, Aijun and Sudjianto, Agus},
eprint = {2003.07132},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang, Zhang, Sudjianto - 2020 - GAMI-Net An Explainable Neural Network based on Generalized Additive Models with Structured Interactions.pdf:pdf},
keywords = {explainable neural network,generalized additive model,model interpretability,pairwise interac-,tion},
pages = {1--24},
title = {{GAMI-Net: An Explainable Neural Network based on Generalized Additive Models with Structured Interactions}},
url = {http://arxiv.org/abs/2003.07132},
year = {2020}
}
@article{Hendricks2016,
abstract = {Clearly explaining a rationale for a classification decision to an end user can be as important as the decision itself. Existing approaches for deep visual recognition are generally opaque and do not output any justification text; contemporary vision-language models can describe image content but fail to take into account class-discriminative image aspects which justify visual predictions. We propose a new model that focuses on the discriminating properties of the visible object, jointly predicts a class label, and explains why the predicted label is appropriate for the image. Through a novel loss function based on sampling and reinforcement learning, our model learns to generate sentences that realize a global sentence property, such as class specificity. Our results on the CUB dataset show that our model is able to generate explanations which are not only consistent with an image but also more discriminative than descriptions produced by existing captioning methods.},
archivePrefix = {arXiv},
arxivId = {1603.08507},
author = {Hendricks, Lisa Anne and Akata, Zeynep and Rohrbach, Marcus and Donahue, Jeff and Schiele, Bernt and Darrell, Trevor},
doi = {10.1007/978-3-319-46493-0_1},
eprint = {1603.08507},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hendricks et al. - 2016 - Generating visual explanations.pdf:pdf},
isbn = {9783319464923},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Image description,Language and vision,Visual explanation},
pages = {3--19},
title = {{Generating visual explanations}},
volume = {9908 LNCS},
year = {2016}
}
@misc{Wyden2019,
author = {Wyden, Ron},
publisher = {116th Congress (2019-2020)},
title = {{S.1108 - Algorithmic Accountability Act of 2019}},
year = {2019}
}
@article{Vu2020,
abstract = {In Graph Neural Networks (GNNs), the graph structure is incorporated into the learning of node representations. This complex structure makes explaining GNNs' predictions become much more challenging. In this paper, we propose PGMExplainer, a Probabilistic Graphical Model (PGM) model-agnostic explainer for GNNs. Given a prediction to be explained, PGM-Explainer identifies crucial graph components and generates an explanation in form of a PGM approximating that prediction. Different from existing explainers for GNNs where the explanations are drawn from a set of linear functions of explained features, PGM-Explainer is able to demonstrate the dependencies of explained features in form of conditional probabilities. Our theoretical analysis shows that the PGM generated by PGMExplainer includes the Markov-blanket of the target prediction, i.e. including all its statistical information. We also show that the explanation returned by PGMExplainer contains the same set of independence statements in the perfect map. Our experiments on both synthetic and real-world datasets show that PGM-Explainer achieves better performance than existing explainers in many benchmark tasks.},
archivePrefix = {arXiv},
arxivId = {2010.05788},
author = {Vu, Minh N. and Thai, My T.},
eprint = {2010.05788},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vu, Thai - 2020 - PGM-explainer Probabilistic graphical model explanations for graph neural networks.pdf:pdf},
issn = {23318422},
journal = {arXiv},
number = {NeurIPS},
title = {{PGM-explainer: Probabilistic graphical model explanations for graph neural networks}},
year = {2020}
}
@article{Limited,
author = {Limited, Infosys},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Limited - Unknown - Explainable Ai a Practical Perspective.pdf:pdf},
keywords = {XAI
AI
Explainable AI
Artificial Intelligence},
title = {{Explainable Ai: a Practical Perspective}}
}
@article{Springenberg2015,
abstract = {Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding – and building on other recent work for finding simple network structures – we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the “deconvolution approach” for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.},
archivePrefix = {arXiv},
arxivId = {1412.6806},
author = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
eprint = {1412.6806},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Springenberg et al. - 2015 - Striving for simplicity The all convolutional net.pdf:pdf},
journal = {3rd International Conference on Learning Representations, ICLR 2015 - Workshop Track Proceedings},
pages = {1--14},
title = {{Striving for simplicity: The all convolutional net}},
year = {2015}
}
@article{Manuscript2020,
abstract = {Cladding austenitic stainless steels are trendy these days in pressure vessels to enhance surface qualities. In this work, austenitic stainless steel clad layers deposited by flux cored arc welding process on structural steel plates used in boiler construction are investigated. To facilitate this, composite clad layers are produced with 316L stainless steel deposits on low carbon structural steel plates based on the design of experiments. Results exhibit an incremental trend of thermal conductivity with respect to percentage of weld dilution. A linear mathematical relationship is established between the percentage of weld dilution and post-weld thermal conductivity of clad layers for the prediction of thermal conductivity of clad layer deposits. These results could be supportive in the fabrication industries for producing energy efficient thermal equipment},
author = {Manuscript, Accepted},
title = {{Ac ce us}},
year = {2020}
}
@article{Binder2016a,
abstract = {We present the application of layer-wise relevance propagation to several deep neural networks such as the BVLC reference neural net and googlenet trained on ImageNet and MIT Places datasets. Layer-wise relevance propagation is a method to compute scores for image pixels and image regions denoting the impact of the particular image region on the prediction of the classifier for one particular test image. We demonstrate the impact of different parameter settings on the resulting explanation.},
author = {Binder, Alexander and Bach, Sebastian and Montavon, Gregoire and M{\"{u}}ller, Klaus Robert and Samek, Wojciech},
doi = {10.1007/978-981-10-0557-2_87},
file = {:home/anna/Desktop/prediction explanation/attribution-based/propagation/lrp/Layer-Wise Relevance Propagation for Deep Neural Network Architectures.pdf:pdf},
isbn = {9789811005565},
issn = {18761119},
journal = {Lecture Notes in Electrical Engineering},
keywords = {Deep neural networks,Non-linear explanations},
pages = {913--922},
title = {{Layer-wise relevance propagation for deep neural network architectures}},
volume = {376},
year = {2016}
}
@article{Zhang2018,
abstract = {This paper proposes a method to modify a traditional convolutional neural network (CNN) into an interpretable CNN, in order to clarify knowledge representations in high conv-layers of the CNN. In an interpretable CNN, each filter in a high conv-layer represents a specific object part. Our interpretable CNNs use the same training data as ordinary CNNs without a need for any annotations of object parts or textures for supervision. The interpretable CNN automatically assigns each filter in a high conv-layer with an object part during the learning process. We can apply our method to different types of CNNs with various structures. The explicit knowledge representation in an interpretable CNN can help people understand the logic inside a CNN, i.e. what patterns are memorized by the CNN for prediction. Experiments have shown that filters in an interpretable CNN are more semantically meaningful than those in a traditional CNN. The code is available at https://github.com/zqs1022/interpretableCNN.},
archivePrefix = {arXiv},
arxivId = {1710.00935},
author = {Zhang, Quanshi and Wu, Ying Nian and Zhu, Song Chun},
doi = {10.1109/CVPR.2018.00920},
eprint = {1710.00935},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {8827--8836},
title = {{Interpretable Convolutional Neural Networks}},
year = {2018}
}
@article{Bau2020,
abstract = {Deep neural networks excel at finding hierarchical representations that solve complex tasks over large datasets. How can we humans understand these learned representations? In this work, we present network dissection, an analytic framework to systematically identify the semantics of individual hidden units within image classification and image generation networks. First, we analyze a convolutional neural network (CNN) trained on scene classification and discover units that match a diverse set of object concepts. We find evidence that the network has learned many object classes that play crucial roles in classifying scene classes. Second, we use a similar analytic method to analyze a generative adversarial network (GAN) model trained to generate scenes. By analyzing changes made when small sets of units are activated or deactivated, we find that objects can be added and removed from the output scenes while adapting to the context. Finally, we apply our analytic framework to understanding adversarial attacks and to semantic image editing.},
archivePrefix = {arXiv},
arxivId = {2009.05041},
author = {Bau, David and Zhu, Jun Yan and Strobelt, Hendrik and Lapedriza, Agata and Zhou, Bolei and Torralba, Antonio},
doi = {10.1073/pnas.1907375117},
eprint = {2009.05041},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bau et al. - 2020 - Understanding the role of individual units in a deep neural network.pdf:pdf},
issn = {10916490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Computer vision,Deep networks,Machine learning},
number = {48},
pages = {30071--30078},
pmid = {32873639},
title = {{Understanding the role of individual units in a deep neural network}},
volume = {117},
year = {2020}
}
@article{Hiley2019,
abstract = {The popularity of Deep Learning for real-world applications is ever-growing. With the introduction of high performance hardware, applications are no longer limited to image recognition. With the introduction of more complex problems comes more and more complex solutions, and the increasing need for explainable AI.Deep Neural Networks for Video tasks are amongst the most complex models, with at least twice the parameters of their Image counterparts. However, explanations for these models are often ill-adapted to the video domain. The current work in explainability for video models is still overshadowed by Image techniques, while Video Deep Learning itself is quickly gaining on methods for still images. This paper seeks to highlight the need for explainability methods designed with video deep learning models, and by association spatio-temporal input in mind, by first illustrating the cutting edge for video deep learning, and then noting the scarcity of research into explanations for these methods.},
archivePrefix = {arXiv},
arxivId = {1909.05667},
author = {Hiley, Liam and Preece, Alun and Hicks, Yulia},
eprint = {1909.05667},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hiley, Preece, Hicks - 2019 - Explainable deep learning for video recognition tasks A framework & recommendations.pdf:pdf},
issn = {23318422},
journal = {arXiv},
pages = {1--15},
title = {{Explainable deep learning for video recognition tasks: A framework & recommendations}},
year = {2019}
}
@article{Li2021a,
abstract = {With the dramatic growth and complexity of seismic data, manual seismic facies analysis has become a significant challenge. Machine learning and deep learning (DL) models have been widely adopted to assist geophysical interpretations in recent years. Although acceptable results can be obtained, the uninterpretable nature of DL (which also has a nickname 'alchemy') does not improve the geological or geophysical understandings on the relationships between the observations and background sciences. This article proposes a noble interpretable DL model based on 3-D (spatial-spectral) attention maps of seismic facies features. Besides regular data-augmentation techniques, the high-resolution spectral analysis technique is employed to generate multispectral seismic inputs. We propose a trainable soft attention mechanism-based deep dilated convolutional neural network (ADDCNN) to improve the automatic seismic facies analysis. Furthermore, the dilated convolution operation in the ADDCNN generates accurate and high-resolution results in an efficient way. With the attention mechanism, not only the facies-segmentation accuracy is improved but also the subtle relations between the geological depositions and the seismic spectral responses are revealed by the spatial-spectral attention maps. Experiments are conducted, where all major metrics, such as classification accuracy, computational efficiency, and optimization performance, are improved while the model complexity is reduced.},
author = {Li, Fangyu and Zhou, Huailai and Wang, Zengyan and Wu, Xinming},
doi = {10.1109/TGRS.2020.2999365},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2020 - ADDCNN An Attention-Based Deep Dilated Convolutional Neural Network for Seismic Facies Analysis with Interpretable(2).pdf:pdf},
issn = {15580644},
journal = {IEEE Transactions on Geoscience and Remote Sensing},
keywords = {Attention map,deep learning (DL),dilated convolution,interpretability,seismic facies analysis},
number = {2},
pages = {1733--1744},
title = {{ADDCNN: An Attention-Based Deep Dilated Convolutional Neural Network for Seismic Facies Analysis with Interpretable Spatial-Spectral Maps}},
volume = {59},
year = {2020}
}
@article{Nguyen2016a,
abstract = {We can better understand deep neural networks by identifying which features each of their neurons have learned to detect. To do so, researchers have created Deep Visualization techniques including activation maximization, which synthetically generates inputs (e.g. images) that maximally activate each neuron. A limitation of current techniques is that they assume each neuron detects only one type of feature, but we know that neurons can be multifaceted, in that they fire in response to many different types of features: for example, a grocery store class neuron must activate either for rows of produce or for a storefront. Previous activation maximization techniques constructed images without regard for the multiple different facets of a neuron, creating inappropriate mixes of colors, parts of objects, scales, orientations, etc. Here, we introduce an algorithm that explicitly uncovers the multiple facets of each neuron by producing a synthetic visualization of each of the types of images that activate a neuron. We also introduce regularization methods that produce state-of-the-art results in terms of the interpretability of images obtained by activation maximization. By separately synthesizing each type of image a neuron fires in response to, the visualizations have more appropriate colors and coherent global structure. Multifaceted feature visualization thus provides a clearer and more comprehensive description of the role of each neuron.},
archivePrefix = {arXiv},
arxivId = {1602.03616},
author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
eprint = {1602.03616},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nguyen, Yosinski, Clune - 2016 - Multifaceted Feature Visualization Uncovering the Different Types of Features Learned By Each Neuron in.pdf:pdf},
title = {{Multifaceted Feature Visualization: Uncovering the Different Types of Features Learned By Each Neuron in Deep Neural Networks}},
url = {http://arxiv.org/abs/1602.03616},
year = {2016}
}
@article{Kanehira2019,
abstract = {This paper addresses the generation of explanations with visual examples. Given an input sample, we build a system that not only classifies it to a specific category, but also outputs linguistic explanations and a set of visual examples that render the decision interpretable. Focusing especially on the complementarity of the multimodal information, i.e., linguistic and visual examples, we attempt to achieve it by maximizing the interaction information, which provides a natural definition of complementarity from an information theoretical viewpoint. We propose a novel framework to generate complemental explanations, on which the joint distribution of the variables to explain, and those to be explained is parameterized by three different neural networks: predictor, linguistic explainer, and example selector. Explanation models are trained collaboratively to maximize the interaction information to ensure the generated explanation are complemental to each other for the target. The results of experiments conducted on several datasets demonstrate the effectiveness of the proposed method.},
archivePrefix = {arXiv},
arxivId = {1812.01280},
author = {Kanehira, Atsushi and Harada, Tatsuya},
doi = {10.1109/CVPR.2019.00880},
eprint = {1812.01280},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kanehira, Harada - 2019 - Learning to explain with complemental examples.pdf:pdf},
isbn = {9781728132938},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {Vision + Language,Vision Applications and Systems},
pages = {8595--8603},
title = {{Learning to explain with complemental examples}},
volume = {2019-June},
year = {2019}
}
@article{Sundararajan2018,
abstract = {Local explanation methods, also known as attribution methods, attribute a deep network's prediction to its input (cf. Baehrens et al. (2010)). We respond to the claim from Adebayo et al. (2018) that local explanation methods lack sensitivity, i.e., DNNs with randomly-initialized weights produce explanations that are both visually and quantitatively similar to those produced by DNNs with learned weights. Further investigation reveals that their findings are due to two choices in their analysis: (a) ignoring the signs of the attributions; and (b) for integrated gradients (IG), including pixels in their analysis that have zero attributions by choice of the baseline (an auxiliary input relative to which the attributions are computed). When both factors are accounted for, IG attributions for a random network and the actual network are uncorrelated. Our investigation also sheds light on how these issues affect visualizations, although we note that more work is needed to understand how viewers interpret the difference between the random and the actual attributions.},
archivePrefix = {arXiv},
arxivId = {1806.04205},
author = {Sundararajan, Mukund and Taly, Ankur},
eprint = {1806.04205},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sundararajan, Taly - 2018 - Local explanation methods for deep neural networks lack sensitivity to parameter values.pdf:pdf},
issn = {23318422},
journal = {arXiv},
pages = {1--10},
title = {{Local explanation methods for deep neural networks lack sensitivity to parameter values}},
year = {2018}
}
@article{Hendricks2018,
abstract = {Natural language explanations of deep neural network decisions provide an intuitive way for a AI agent to articulate a reasoning process. Current textual explanations learn to discuss class discriminative features in an image. However, it is also helpful to understand which attributes might change a classification decision if present in an image (e.g., "This is not a Scarlet Tanager because it does not have black wings.") We call such textual explanations counterfactual explanations, and propose an intuitive method to generate counterfactual explanations by inspecting which evidence in an input is missing, but might contribute to a different classification decision if present in the image. To demonstrate our method we consider a fine-grained image classification task in which we take as input an image and a counterfactual class and output text which explains why the image does not belong to a counterfactual class. We then analyze our generated counterfactual explanations both qualitatively and quantitatively using proposed automatic metrics.},
archivePrefix = {arXiv},
arxivId = {1806.09809},
author = {Hendricks, Lisa Anne and Hu, Ronghang and Darrell, Trevor and Akata, Zeynep},
eprint = {1806.09809},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hendricks et al. - 2018 - Generating Counterfactual Explanations with Natural Language(2).pdf:pdf},
title = {{Generating Counterfactual Explanations with Natural Language}},
url = {http://arxiv.org/abs/1806.09809},
year = {2018}
}
@article{Mothilal2020,
abstract = {Post-hoc explanations of machine learning models are crucial for people to understand and act on algorithmic predictions. An intriguing class of explanations is through counterfactuals, hypothetical examples that show people how to obtain a different prediction. We posit that effective counterfactual explanations should satisfy two properties: feasibility of the counterfactual actions given user context and constraints, and diversity among the counterfactuals presented. To this end, we propose a framework for generating and evaluating a diverse set of counterfactual explanations based on determinantal point processes. To evaluate the actionability of counterfactuals, we provide metrics that enable comparison of counterfactual-based methods to other local explanation methods. We further address necessary tradeoffs and point to causal implications in optimizing for counterfactuals. Our experiments on four real-world datasets show that our framework can generate a set of counterfactuals that are diverse and well approximate local decision boundaries, outperforming prior approaches to generating diverse counterfactuals. We provide an implementation of the framework at https://github.com/microsoft/DiCE.},
archivePrefix = {arXiv},
arxivId = {1905.07697},
author = {Mothilal, Ramaravind K. and Sharma, Amit and Tan, Chenhao},
doi = {10.1145/3351095.3372850},
eprint = {1905.07697},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mothilal, Sharma, Tan - 2020 - Explaining machine learning classifiers through diverse counterfactual explanations.pdf:pdf},
isbn = {9781450369367},
journal = {FAT* 2020 - Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {607--617},
title = {{Explaining machine learning classifiers through diverse counterfactual explanations}},
year = {2020}
}
@article{Fong2017,
abstract = {As machine learning algorithms are increasingly applied to high impact yet high risk tasks, such as medical diagnosis or autonomous driving, it is critical that researchers can explain how such algorithms arrived at their predictions. In recent years, a number of image saliency methods have been developed to summarize where highly complex neural networks 'look' in an image for evidence for their predictions. However, these techniques are limited by their heuristic nature and architectural constraints. In this paper, we make two main contributions: First, we propose a general framework for learning different kinds of explanations for any black box algorithm. Second, we specialise the framework to find the part of an image most responsible for a classifier decision. Unlike previous works, our method is model-agnostic and testable because it is grounded in explicit and interpretable image perturbations.},
archivePrefix = {arXiv},
arxivId = {1704.03296},
author = {Fong, Ruth C. and Vedaldi, Andrea},
doi = {10.1109/ICCV.2017.371},
eprint = {1704.03296},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fong, Vedaldi - 2017 - Interpretable Explanations of Black Boxes by Meaningful Perturbation.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {3449--3457},
title = {{Interpretable Explanations of Black Boxes by Meaningful Perturbation}},
volume = {2017-Octob},
year = {2017}
}
@article{Garima2020,
abstract = {We introduce a method called TracIn that computes the influence of a training example on a prediction made by the model. The idea is to trace how the loss on the test point changes during the training process whenever the training example of interest was utilized. We provide a scalable implementation of TracIn via: (a) a first-order gradient approximation to the exact computation, (b) saved checkpoints of standard training procedures, and (c) cherry-picking layers of a deep neural network. In contrast with previously proposed methods, TracIn is simple to implement; all it needs is the ability to work with gradients, checkpoints, and loss functions. The method is general. It applies to any machine learning model trained using stochastic gradient descent or a variant of it, agnostic of architecture, domain and task. We expect the method to be widely useful within processes that study and improve training data. Code is available at [1].},
archivePrefix = {arXiv},
arxivId = {2002.08484},
author = {Garima and Liu, Frederick and Kale, Satyen and Sundararajan, Mukund},
eprint = {2002.08484},
file = {:home/anna/Desktop/prediction explanation/evidence-based/prototypes/Estimating Training Data Influence by Tracing Gradient Descent.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {NeurIPS},
title = {{Estimating training data influence by tracing gradient descent}},
volume = {2020-Decem},
year = {2020}
}
@article{Alain2016,
abstract = {Neural network models have a reputation for being black boxes. We propose to monitor the features at every layer of a model and measure how suitable they are for classification. We use linear classifiers, which we refer to as "probes", trained entirely independently of the model itself. This helps us better understand the roles and dynamics of the intermediate layers. We demonstrate how this can be used to develop a better intuition about models and to diagnose potential problems. We apply this technique to the popular models Inception v3 and Resnet-50. Among other things, we observe experimentally that the linear separability of features increase monotonically along the depth of the model.},
archivePrefix = {arXiv},
arxivId = {1610.01644},
author = {Alain, Guillaume and Bengio, Yoshua},
eprint = {1610.01644},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Alain, Bengio - 2016 - Understanding intermediate layers using linear classifier probes.pdf:pdf},
title = {{Understanding intermediate layers using linear classifier probes}},
url = {http://arxiv.org/abs/1610.01644},
year = {2016}
}
@article{Townsend2020,
abstract = {The term 'explainable AI' refers to the goal of producing artificially intelligent agents that are capable of providing explanations for their decisions. Some models (e.g., rule-based systems) are designed to be explainable, while others are less explicit 'black boxes' for which their reasoning remains a mystery. One example of the latter is the neural network, and over the past few decades, researchers in the field of neural-symbolic integration (NSI) have sought to extract relational knowledge from such networks. Extraction from deep neural networks, however, has remained a challenge until recent years in which many methods of extracting distinct, salient features from input or hidden feature spaces of deep neural networks have been proposed. Furthermore, methods of identifying relationships between these features have also emerged. This article presents examples of old and new developments in extracting relational explanations in order to argue that the latter have analogies in the former and, as such, can be described in terms of long-established taxonomies and frameworks presented in early neural-symbolic literature. We also outline potential future research directions that come to light from this refreshed perspective.},
author = {Townsend, Joseph and Chaton, Thomas and Monteiro, Joao M.},
doi = {10.1109/TNNLS.2019.2944672},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Townsend, Chaton, Monteiro - 2020 - Extracting Relational Explanations from Deep Neural Networks A Survey from a Neural-Symbolic Perspec.pdf:pdf},
issn = {21622388},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {Explainable artificial intelligence,knowledge extraction,neural networks,neural-symbolic integration (NSI)},
number = {9},
pages = {3456--3470},
pmid = {31689216},
title = {{Extracting Relational Explanations from Deep Neural Networks: A Survey from a Neural-Symbolic Perspective}},
volume = {31},
year = {2020}
}
@article{Selvaraju2020,
abstract = {We propose a technique for producing ‘visual explanations' for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent and explainable. Our approach—Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say ‘dog' in a classification network or a sequence of words in captioning network) flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g.VGG), (2) CNNs used for structured outputs (e.g.captioning), (3) CNNs used in tasks with multi-modal inputs (e.g.visual question answering) or reinforcement learning, all without architectural changes or re-training. We combine Grad-CAM with existing fine-grained visualizations to create a high-resolution class-discriminative visualization, Guided Grad-CAM, and apply it to image classification, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (c) are robust to adversarial perturbations, (d) are more faithful to the underlying model, and (e) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show that even non-attention based models learn to localize discriminative regions of input image. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names (Bau et al. in Computer vision and pattern recognition, 2017) to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a ‘stronger' deep network from a ‘weaker' one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo on CloudCV (Agrawal et al., in: Mobile cloud visual media computing, pp 265–290. Springer, 2015) (http://gradcam.cloudcv.org) and a video at http://youtu.be/COjUB9Izk6E.},
archivePrefix = {arXiv},
arxivId = {1610.02391},
author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
doi = {10.1007/s11263-019-01228-7},
eprint = {1610.02391},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Selvaraju et al. - 2020 - Grad-CAM Visual Explanations from Deep Networks via Gradient-Based Localization.pdf:pdf},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Explanations,Grad-CAM,Interpretability,Transparency,Visual explanations,Visualizations},
number = {2},
pages = {336--359},
title = {{Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization}},
volume = {128},
year = {2020}
}
@article{Seifert2017,
abstract = {In recent years, Deep Neural Networks (DNNs) have been shown to outperform the state-of-the-art in multiple areas, such as visual object recognition, genomics and speech recognition. Due to the distributed encodings of information, DNNs are hard to understand and interpret. To this end, visualizations have been used to understand how deep architecture work in general, what different layers of the network encode, what the limitations of the trained model was and how to interactively collect user feedback. In this chapter, we provide a survey of visualizations of DNNs in the field of computer vision. We define a classification scheme describing visualization goals and methods as well as the application areas. This survey gives an overview of what can be learned from visualizing DNNs and which visualization methods were used to gain which insights. We found that most papers use Pixel Displays to show neuron activations. However, recently more sophisticated visualizations like interactive node-link diagrams were proposed. The presented overview can serve as a guideline when applying visualizations while designing DNNs.},
author = {Seifert, Christin and Aamir, Aisha and Balagopalan, Aparna and Jain, Dhruv and Sharma, Abhinav and Grottel, Sebastian and Gumhold, Stefan},
doi = {10.1007/978-3-319-54024-5_6},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Seifert et al. - 2017 - Visualizations of Deep Neural Networks in Computer Vision A Survey(2).pdf:pdf},
pages = {123--144},
title = {{Visualizations of Deep Neural Networks in Computer Vision: A Survey}},
year = {2017}
}
@article{Pope2019,
abstract = {With the growing use of graph convolutional neural networks (GCNNs) comes the need for explainability. In this paper, we introduce explainability methods for GCNNs. We develop the graph analogues of three prominent explainability methods for convolutional neural networks: contrastive gradient-based (CG) saliency maps, Class Activation Mapping (CAM), and Excitation Back-Propagation (EB) and their variants, gradient-weighted CAM (Grad-CAM) and contrastive EB (c-EB). We show a proof-of-concept of these methods on classification problems in two application domains: visual scene graphs and molecular graphs. To compare the methods, we identify three desirable properties of explanations: (1) their importance to classification, as measured by the impact of occlusions, (2) their contrastivity with respect to different classes, and (3) their sparseness on a graph. We call the corresponding quantitative metrics fidelity, contrastivity, and sparsity and evaluate them for each method. Lastly, we analyze the salient subgraphs obtained from explanations and report frequently occurring patterns.},
author = {Pope, Phillip E. and Kolouri, Soheil and Rostami, Mohammad and Martin, Charles E. and Hoffmann, Heiko},
doi = {10.1109/CVPR.2019.01103},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pope et al. - 2019 - Explainability methods for graph convolutional neural networks.pdf:pdf},
isbn = {9781728132938},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {Deep Learning},
pages = {10764--10773},
title = {{Explainability methods for graph convolutional neural networks}},
volume = {2019-June},
year = {2019}
}
@article{Chatterjee2020,
abstract = {The last decade has witnessed an increased interest in applying machine learning techniques to predict faults and anomalies in the operation of wind turbines. These efforts have lately been dominated by deep learning techniques which, as in other fields, tend to outperform traditional machine learning algorithms given sufficient amounts of training data. An important shortcoming of deep learning models is their lack of transparency—they operate as black boxes and typically do not provide rationales for their predictions, which can lead to a lack of trust in predicted outputs. In this article, a novel hybrid model for anomaly prediction in wind farms is proposed, which combines a recurrent neural network approach for accurate classification with an XGBoost decision tree classifier for transparent outputs. Experiments with an offshore wind turbine show that our model achieves a classification accuracy of up to 97%. The model is further able to generate detailed feature importance analyses for any detected anomalies, identifying exactly those components in a wind turbine that contribute to an anomaly. Finally, the feasibility of transfer learning is demonstrated for the wind domain by porting our “offshore” model to an unseen dataset from an onshore wind farm. The latter model achieves an accuracy of 65% and is able to detect 85% of anomalies in the unseen domain. These results are encouraging for application to wind farms for which no training data are available, for example, because they have not been in operation for long.},
author = {Chatterjee, Joyjit and Dethlefs, Nina},
doi = {10.1002/we.2510},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chatterjee, Dethlefs - 2020 - Deep learning with knowledge transfer for explainable anomaly prediction in wind turbines.pdf:pdf},
issn = {10991824},
journal = {Wind Energy},
keywords = {LSTM,SCADA,SMOTE,XGBoost,transfer learning},
number = {8},
pages = {1693--1710},
title = {{Deep learning with knowledge transfer for explainable anomaly prediction in wind turbines}},
volume = {23},
year = {2020}
}
@article{Yeche2019,
abstract = {Understanding predictions in Deep Learning (DL) models is crucial for domain experts without any DL expertise in order to justify resultant decision-making process. As of today, medical models are often based on hand-crafted features such as radiomics, though their link with neural network features remains unclear. To address the lack of interpretability, approaches based on human-understandable concepts such as TCAV have been introduced. These methods have shown promising results, though they are unsuited for continuous value concepts and their introduced metrics do not adapt well to high-dimensional spaces. To bridge the gap with radiomics-based models, we implement a regression concept vector showing the impact of radiomic features on the predictions of deep networks. In addition, we introduce a new metric with improved scaling to high-dimensional spaces, allowing comparison across multiple layers.},
author = {Yeche, Hugo and Harrison, Justin and Berthier, Tess},
doi = {10.1007/978-3-030-33850-3_2},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yeche, Harrison, Berthier - 2019 - UBS A dimension-agnostic metric for concept vector interpretability applied to radiomics.pdf:pdf},
isbn = {9783030338497},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Concept vector,Dimensionality,Interpretability,Radiomics},
pages = {12--20},
title = {{UBS: A dimension-agnostic metric for concept vector interpretability applied to radiomics}},
volume = {11797 LNCS},
year = {2019}
}
@article{Zintgraf2016,
abstract = {We present a method for visualising the response of a deep neural network to a specific input. For image data for instance our method will highlight areas that provide evidence in favor of, and against choosing a certain class. The method overcomes several shortcomings of previous methods and provides great additional insight into the decision making process of convolutional networks, which is important both to improve models and to accelerate the adoption of such methods in e.g. medicine. In experiments on ImageNet data, we illustrate how the method works and can be applied in different ways to understand deep neural nets.},
archivePrefix = {arXiv},
arxivId = {1603.02518},
author = {Zintgraf, Luisa M. and Cohen, Taco S. and Welling, Max},
eprint = {1603.02518},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zintgraf, Cohen, Welling - 2016 - A New Method to Visualize Deep Neural Networks.pdf:pdf},
title = {{A New Method to Visualize Deep Neural Networks}},
url = {http://arxiv.org/abs/1603.02518},
year = {2016}
}
@article{Selvaraju2016,
abstract = {We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing input regions that are 'important' for predictions -- or visual explanations. Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses class-specific gradient information to localize important regions. These localizations are combined with existing pixel-space visualizations to create a novel high-resolution and class-discriminative visualization called Guided Grad-CAM. These methods help better understand CNN-based models, including image captioning and visual question answering (VQA) models. We evaluate our visual explanations by measuring their ability to discriminate between classes, to inspire trust in humans, and their correlation with occlusion maps. Grad-CAM provides a new way to understand CNN-based models. We have released code, an online demo hosted on CloudCV, and a full version of this extended abstract.},
archivePrefix = {arXiv},
arxivId = {1611.07450},
author = {Selvaraju, Ramprasaath R and Das, Abhishek and Vedantam, Ramakrishna and Cogswell, Michael and Parikh, Devi and Batra, Dhruv},
eprint = {1611.07450},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Selvaraju et al. - 2016 - Grad-CAM Why did you say that.pdf:pdf},
pages = {1--4},
title = {{Grad-CAM: Why did you say that?}},
url = {http://arxiv.org/abs/1611.07450},
year = {2016}
}
@article{Binder2016,
abstract = {Layer-wise relevance propagation is a framework which allows to decompose the prediction of a deep neural network computed over a sample, e.g. an image, down to relevance scores for the single input dimensions of the sample such as subpixels of an image. While this approach can be applied directly to generalized linear mappings, product type non-linearities are not covered. This paper proposes an approach to extend layer-wise relevance propagation to neural networks with local renormalization layers, which is a very common product-type non-linearity in convolutional neural networks. We evaluate the proposed method for local renormalization layers on the CIFAR-10, Imagenet and MIT Places datasets.},
archivePrefix = {arXiv},
arxivId = {1604.00825},
author = {Binder, Alexander and Montavon, Gr{\'{e}}goire and Lapuschkin, Sebastian and M{\"{u}}ller, Klaus Robert and Samek, Wojciech},
doi = {10.1007/978-3-319-44781-0_8},
eprint = {1604.00825},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Binder et al. - 2016 - Layer-wise relevance propagation for neural networks with local renormalization layers.pdf:pdf},
isbn = {9783319447803},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Image classification,Interpretability,Neural networks},
pages = {63--71},
title = {{Layer-wise relevance propagation for neural networks with local renormalization layers}},
volume = {9887 LNCS},
year = {2016}
}
@article{Coates2017,
author = {Coates, Adam},
file = {:home/anna/Desktop/overviews/Ask-the-AI-experts-Whats-driving-todays-progress-in-AI.pdf:pdf},
journal = {McKinsey & Company},
number = {July},
title = {{Ask the AI experts: What's driving today's progress in AI?}},
url = {https://www.mckinsey.com/business-functions/mckinsey-analytics/our-insights/ask-the-ai-experts-whats-driving-todays-progress-in-ai},
year = {2017}
}
@article{Agarwal2020,
abstract = {Deep neural networks (DNNs) are powerful black-box predictors that have achieved impressive performance on a wide variety of tasks. However, their accuracy comes at the cost of intelligibility: it is usually unclear how they make their decisions. This hinders their applicability to high stakes decision-making domains such as healthcare. We propose Neural Additive Models (NAMs) which combine some of the expressivity of DNNs with the inherent intelligibility of generalized additive models. NAMs learn a linear combination of neural networks that each attend to a single input feature. These networks are trained jointly and can learn arbitrarily complex relationships between their input feature and the output. Our experiments on regression and classification datasets show that NAMs are more accurate than widely used intelligible models such as logistic regression and shallow decision trees. They perform similarly to existing state-of-the-art generalized additive models in accuracy, but can be more easily applied to real-world problems.},
archivePrefix = {arXiv},
arxivId = {2004.13912},
author = {Agarwal, Rishabh and Frosst, Nicholas and Zhang, Xuezhou and Caruana, Rich and Hinton, Geoffrey E.},
eprint = {2004.13912},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Agarwal et al. - 2020 - Neural Additive Models Interpretable Machine Learning with Neural Nets.pdf:pdf},
title = {{Neural Additive Models: Interpretable Machine Learning with Neural Nets}},
url = {http://arxiv.org/abs/2004.13912},
year = {2020}
}
@article{Kim2018b,
abstract = {In this study, a novel computer aided diagnosis (CADx) framework is devised to investigate interpretability for classifying breast masses. Recently, a deep learning technology has been successfully applied to medical image analysis including CADx. Existing deep learning based CADx approaches, however, have a limitation in explaining the diagnostic decision. In real clinical practice, clinical decisions could be made with reasonable explanation. So current deep learning approaches in CADx are limited in real world deployment. In this paper, we investigate interpretability in CADx with the proposed interpretable CADx (ICADx) framework. The proposed framework is devised with a generative adversarial network, which consists of interpretable diagnosis network and synthetic lesion generative network to learn the relationship between malignancy and a standardized description (BI-RADS). The lesion generative network and the interpretable diagnosis network compete in an adversarial learning so that the two networks are improved. The effectiveness of the proposed method was validated on public mammogram database. Experimental results showed that the proposed ICADx framework could provide the interpretability of mass as well as mass classification. It was mainly attributed to the fact that the proposed method was effectively trained to find the relationship between malignancy and interpretations via the adversarial learning. These results imply that the proposed ICADx framework could be a promising approach to develop the CADx system.},
archivePrefix = {arXiv},
arxivId = {1805.08960},
author = {Kim, Seong Tae and Lee, Hakmin and Kim, Hak Gu and Ro, Yong Man},
doi = {10.1117/12.2293570},
eprint = {1805.08960},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim et al. - 2018 - ICADx interpretable computer aided diagnosis of breast masses(2).pdf:pdf},
isbn = {9781510616394},
issn = {16057422},
keywords = {computer-aided diagnosis,deep learning,explainable deep learning,interpretable ai},
pages = {73},
title = {{ICADx: interpretable computer aided diagnosis of breast masses}},
year = {2018}
}
@article{Bi2021,
abstract = {Geological hazards caused by rock failure severely threaten the safety of underground projects, and thus microseismic monitoring systems have been deployed to monitor the rock mass stability. However, due to implicit subseries patterns and sparse distinguishing features, automatic discrimination of the microseismic waveforms of rock fracturing remains a great challenge. Deep neural networks offer powerful learning ability, but the unexplainability of the neural network carries substantial risks to decision-making in safety warning. To this end, we propose an explainable convolutional neural network XTF-CNN that supplies both excellent classification performance and explainability. XTF-CNN consists of two major modules: 1) a dual-channel classification module that learns microseismic features from both the time and frequency domains and 2) an explanation module that demonstrates fine-grained and comprehensible results. Experiments are conducted using microseismic wave-forms collected from a deep tunnel project. The results indicate that XTF-CNN achieves superior classification performance over rival methods and significant comprehensibility.},
author = {Bi, Xin and Zhang, Chao and He, Yao and Zhao, Xiangguo and Sun, Yongjiao and Ma, Yuliang},
doi = {10.1016/j.ins.2020.08.109},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bi et al. - 2021 - Explainable time–frequency convolutional neural network for microseismic waveform classification.pdf:pdf},
issn = {00200255},
journal = {Information Sciences},
keywords = {Explainable convolutional neural network,Microseismic waveform,Time series classification},
pages = {883--896},
publisher = {Elsevier Inc.},
title = {{Explainable time–frequency convolutional neural network for microseismic waveform classification}},
url = {https://doi.org/10.1016/j.ins.2020.08.109},
volume = {546},
year = {2021}
}
@article{Marcos2019,
abstract = {A main issue preventing the use of Convolutional Neural Networks (CNN) in end user applications is the low level of transparency in the decision process. Previous work on CNN interpretability has mostly focused either on localizing the regions of the image that contribute to the result or on building an external model that generates plausible explanations. However, the former does not provide any semantic information and the latter does not guarantee the faithfulness of the explanation. We propose an intermediate representation composed of multiple Semantically Interpretable Activation Maps (SIAM) indicating the presence of predefined attributes at different locations of the image. These attribute maps are then linearly combined to produce the final output. This gives the user insight into what the model has seen, where, and a final output directly linked to this information in a comprehensive and interpretable way. We test the method on the task of landscape scenicness (aesthetic value) estimation, using an intermediate representation of 33 attributes from the SUN Attributes database. The results confirm that SIAM makes it possible to understand what attributes in the image are contributing to the final score and where they are located. Since it is based on learning from multiple tasks and datasets, SIAM improve the explanability of the prediction without additional annotation efforts or computational overhead at inference time, while keeping good performances on both the final and intermediate tasks.},
archivePrefix = {arXiv},
arxivId = {1909.08442},
author = {Marcos, DIego and Lobry, Sylvain and Tuia, Devis},
doi = {10.1109/ICCVW.2019.00518},
eprint = {1909.08442},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Marcos, Lobry, Tuia - 2019 - Semantically interpretable activation maps What-where-how explanations within CNNs.pdf:pdf},
isbn = {9781728150239},
journal = {Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019},
keywords = {Attributes,Convolutioal-Neural-Networks,Deep-Learning,Explainability,Interpretability,Scenicness-estimation},
pages = {4207--4215},
publisher = {IEEE},
title = {{Semantically interpretable activation maps: What-where-how explanations within CNNs}},
year = {2019}
}
@article{Albini2020,
abstract = {We propose a general method for generating counterfactual explanations (CFXs) for a range of Bayesian Network Classifiers (BCs), e.g. single- or multi-label, binary or multidimensional. We focus on explanations built from relations of (critical and potential) influence between variables, indicating the reasons for classifications, rather than any probabilistic information. We show by means of a theoretical analysis of CFXs' properties that they serve the purpose of indicating (potentially) pivotal factors in the classification process, whose absence would give rise to different classifications. We then prove empirically for various BCs that CFXs provide useful information in real world settings, e.g. when race plays a part in parole violation prediction, and show that they have inherent advantages over existing explanation methods in the literature.},
author = {Albini, Emanuele and Rago, Antonio and Baroni, Pietro and Toni, Francesca},
doi = {10.24963/ijcai.2020/63},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Albini et al. - 2020 - Relation-based counterfactual explanations for Bayesian network classifiers(2).pdf:pdf},
isbn = {9780999241165},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {AI Ethics: Explainability,Uncertainty in AI: Bayesian Networks},
pages = {451--457},
title = {{Relation-based counterfactual explanations for Bayesian network classifiers}},
volume = {2021-Janua},
year = {2020}
}
@article{Zintgraf2017,
abstract = {This article presents the prediction difference analysis method for visualizing the response of a deep neural network to a specific input. When classifying images, the method highlights areas in a given input image that provide evidence for or against a certain class. It overcomes several shortcoming of previous methods and provides great additional insight into the decision making process of classifiers. Making neural network decisions interpretable through visualization is important both to improve models and to accelerate the adoption of black-box classifiers in application areas such as medicine. We illustrate the method in experiments on natural images (ImageNet data), as well as medical images (MRI brain scans).},
archivePrefix = {arXiv},
arxivId = {1702.04595},
author = {Zintgraf, Luisa M. and Cohen, Taco S. and Adel, Tameem and Welling, Max},
eprint = {1702.04595},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zintgraf et al. - 2017 - Visualizing deep neural network decisions Prediction difference analysis.pdf:pdf},
issn = {23318422},
journal = {arXiv},
pages = {1--12},
title = {{Visualizing deep neural network decisions: Prediction difference analysis}},
year = {2017}
}
@article{Brown2018,
abstract = {Deep learning has recently demonstrated state-of-the art performance on key tasks related to the maintenance of computer systems, such as intrusion detection, denial of service attack detection, hardware and software system failures, and malware detection. In these contexts, model interpretability is vital for administrator and analyst to trust and act on the automated analysis of machine learning models. Deep learning methods have been criticized as black box oracles which allow limited insight into decision factors. In this work we seek to bridge the gap between the impressive performance of deep learning models and the need for interpretable model introspection. To this end we present recurrent neural network (RNN) language models augmented with attention for anomaly detection in system logs. Our methods are generally applicable to any computer system and logging source. By incorporating attention variants into our RNN language models we create opportunities for model introspection and analysis without sacrificing state-of-the art performance. We demonstrate model performance and illustrate model interpretability on an intrusion detection task using the Los Alamos National Laboratory (LANL) cyber security dataset, reporting upward of 0.99 area under the receiver operator characteristic curve despite being trained only on a single day's worth of data.},
archivePrefix = {arXiv},
arxivId = {1803.04967},
author = {Brown, Andy and Hutchinson, Brian and Tuor, Aaron and Nichols, Nicole},
doi = {10.1145/3217871.3217872},
eprint = {1803.04967},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brown et al. - 2018 - Recurrent neural network attention mechanisms for interpretable system log anomaly detection.pdf:pdf},
isbn = {9781450358651},
journal = {Proceedings of the 1st Workshop on Machine Learning for Computing Systems, MLCS 2018 - In conjunction with HPDC},
keywords = {Anomaly detection,Attention,Interpretable Machine Learning,Online Training,Recurrent Neural Networks,System Log Analysis},
number = {June},
title = {{Recurrent neural network attention mechanisms for interpretable system log anomaly detection}},
year = {2018}
}
@article{Herlocker2000,
abstract = {Automated collaborative filtering (ACF) systems predict a person's affinity for items or information by connecting that person's recorded interests with the recorded interests of a community of people and sharing ratings between like-minded persons. However, current recommender systems are black boxes, providing no transparency into the working of the recommendation. Explanations provide that transparency, exposing the reasoning and data behind a recommendation. In this paper, we address explanation interfaces for ACF systems - how they should be implemented and why they should be implemented. To explore how, we present a model for explanations based on the user's conceptual model of the recommendation process. We then present experimental results demonstrating what components of an explanation are the most compelling. To address why, we present experimental evidence that shows that providing explanations can improve the acceptance of ACF systems. We also describe some initial explorations into measuring how explanations can improve the filtering performance of users.},
author = {Herlocker, J. L. and Konstan, J. A. and Riedl, J.},
doi = {10.1145/358916.358995},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Herlocker, Konstan, Riedl - 2000 - Explaining collaborative filtering recommendations(2).pdf:pdf},
isbn = {1581132220},
journal = {Proceedings of the ACM Conference on Computer Supported Cooperative Work},
keywords = {Collaborative filtering,Explanations,GroupLens,MovieLens,Recommender systems},
pages = {241--250},
title = {{Explaining collaborative filtering recommendations}},
year = {2000}
}
@article{Landecker2013,
abstract = {Hierarchical networks are known to achieve high classification accuracy on difficult machine-learning tasks. For many applications, a clear explanation of why the data was classified a certain way is just as important as the classification itself. However, the complexity of hierarchical networks makes them ill-suited for existing explanation methods. We propose a new method, contribution propagation, that gives per-instance explanations of a trained network's classifications. We give theoretical foundations for the proposed method, and evaluate its correctness empirically. Finally, we use the resulting explanations to reveal unexpected behavior of networks that achieve high accuracy on visual object-recognition tasks using well-known data sets. {\textcopyright} 2013 IEEE.},
author = {Landecker, Will and Thomure, Michael D. and Bettencourt, Luis M.A. and Mitchell, Melanie and Kenyon, Garrett T. and Brumby, Steven P.},
doi = {10.1109/CIDM.2013.6597214},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Landecker et al. - 2013 - Interpreting individual classifications of hierarchical networks.pdf:pdf},
isbn = {9781467358958},
journal = {Proceedings of the 2013 IEEE Symposium on Computational Intelligence and Data Mining, CIDM 2013 - 2013 IEEE Symposium Series on Computational Intelligence, SSCI 2013},
pages = {32--38},
title = {{Interpreting individual classifications of hierarchical networks}},
year = {2013}
}
@inproceedings{Xu2015,
abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the cor- responding words in the output sequence. We validate the use of attention with state-of-the- art performance on three benchmark datasets: Flickr9k, Flickr30k and MS COCO.},
author = {Xu, Kelvin and {Lei Ba}, Jimmy and Kiros, Ryan and Cho, Kyung Hyun and Courville, Aaron},
booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu et al. - 2015 - Show Attend and Tell-Neural Image Caption Generation with Visual Attention.pdf:pdf},
month = {dec},
title = {{Show Attend and Tell-Neural Image Caption Generation with Visual Attention}},
volume = {37},
year = {2015}
}
@article{Howard2017,
abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
archivePrefix = {arXiv},
arxivId = {1704.04861},
author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
eprint = {1704.04861},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Howard et al. - 2017 - MobileNets Efficient Convolutional Neural Networks for Mobile Vision Applications.pdf:pdf},
title = {{MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications}},
url = {http://arxiv.org/abs/1704.04861},
year = {2017}
}
@article{Selvaraju2016,
abstract = {We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing input regions that are 'important' for predictions -- or visual explanations. Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses class-specific gradient information to localize important regions. These localizations are combined with existing pixel-space visualizations to create a novel high-resolution and class-discriminative visualization called Guided Grad-CAM. These methods help better understand CNN-based models, including image captioning and visual question answering (VQA) models. We evaluate our visual explanations by measuring their ability to discriminate between classes, to inspire trust in humans, and their correlation with occlusion maps. Grad-CAM provides a new way to understand CNN-based models. We have released code, an online demo hosted on CloudCV, and a full version of this extended abstract.},
archivePrefix = {arXiv},
arxivId = {1611.07450},
author = {Selvaraju, Ramprasaath R and Das, Abhishek and Vedantam, Ramakrishna and Cogswell, Michael and Parikh, Devi and Batra, Dhruv},
eprint = {1611.07450},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Selvaraju et al. - 2016 - Grad-CAM Why did you say that.pdf:pdf},
pages = {1--4},
title = {{Grad-CAM: Why did you say that?}},
url = {http://arxiv.org/abs/1611.07450},
year = {2016}
}
@article{Wachter2017,
abstract = {There has been much discussion of the “right to explanation” in the EU General Data Protection Regulation, and its existence, merits, and disadvantages. Implementing a right to explanation that opens the ‘black box' of algorithmic decision-making faces major legal and technical barriers. Explaining the functionality of complex algorithmic decision-making systems and their rationale in specific cases is a technically challenging problem. Some explanations may offer little meaningful information to data subjects, raising questions around their value. Data controllers have an interest to not disclose information about their algorithms that contains trade secrets, violates the rights and freedoms of others (e.g. privacy), or allows data subjects to game or manipulate decision-making.  Explanations of automated decisions need not hinge on the general public understanding how algorithmic systems function. Even though such interpretability is of great importance and should be pursued, explanations can, in principle, be offered without opening the black box. Looking at explanations as a means to help a data subject act rather than merely understand, one could gauge the scope and content of explanations according to the specific goal or action they are intended to support.  From the perspective of individuals affected by automated decision-making, we propose three aims for explanations:   (1) to inform and help the individual understand why a particular decision was reached,   (2) to provide grounds to contest the decision if the outcome is undesired, and   (3) to understand what would need to change in order to receive a desired result in the future, based on the current decision-making model.   We assess how each of these goals finds support in the GDPR, and the extent to which they hinge on opening the ‘black box'. We suggest data controllers should offer a particular type of explanation, ‘unconditional counterfactual explanations', to support these three aims. These counterfactual explanations describe the smallest change to the world that can be made to obtain a desirable outcome, or to arrive at the “closest possible world.” As multiple variables or sets of variables can lead to one or more desirable outcomes, multiple counterfactual explanations can be provided, corresponding to different choices of nearby possible worlds for which the counterfactual holds. Counterfactuals describe a dependency on the external facts that lead to that decision without the need to convey the internal state or logic of an algorithm. As a result, counterfactuals serve as a minimal solution that bypasses the current technical limitations of interpretability, while striking a balance between transparency and the rights and freedoms of others (e.g. privacy, trade secrets).},
archivePrefix = {arXiv},
arxivId = {1711.00399},
author = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
doi = {10.2139/ssrn.3063289},
eprint = {1711.00399},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wachter, Mittelstadt, Russell - 2017 - Counterfactual Explanations Without Opening the Black Box Automated Decisions and the GDPR.pdf:pdf},
issn = {1556-5068},
journal = {SSRN Electronic Journal},
pages = {1--52},
title = {{Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR}},
year = {2017}
}
@article{Shahroudnejad2021,
abstract = {Recent advancements in machine learning and signal processing domains have resulted in an extensive surge of interest in Deep Neural Networks (DNNs) due to their unprecedented performance and high accuracy for different and challenging problems of significant engineering importance. However, when such deep learning architectures are utilized for making critical decisions such as the ones that involve human lives (e.g., in control systems and medical applications), it is of paramount importance to understand, trust, and in one word "explain" the argument behind deep models' decisions. In many applications, artificial neural networks (including DNNs) are considered as black-box systems, which do not provide sufficient clue on their internal processing actions. Although some recent efforts have been initiated to explain the behaviors and decisions of deep networks, explainable artificial intelligence (XAI) domain, which aims at reasoning about the behavior and decisions of DNNs, is still in its infancy. The aim of this paper is to provide a comprehensive overview on Understanding, Visualization, and Explanation of the internal and overall behavior of DNNs.},
archivePrefix = {arXiv},
arxivId = {2102.01792},
author = {Shahroudnejad, Atefeh},
eprint = {2102.01792},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shahroudnejad - 2021 - A Survey on Understanding, Visualizations, and Explanation of Deep Neural Networks.pdf:pdf},
title = {{A Survey on Understanding, Visualizations, and Explanation of Deep Neural Networks}},
url = {http://arxiv.org/abs/2102.01792},
year = {2021}
}
@article{Shwartz-Ziv2017,
abstract = {Despite their great success, there is still no comprehensive theoretical understanding of learning with Deep Neural Networks (DNNs) or their inner organization. Previous work proposed to analyze DNNs in the \textit{Information Plane}; i.e., the plane of the Mutual Information values that each layer preserves on the input and output variables. They suggested that the goal of the network is to optimize the Information Bottleneck (IB) tradeoff between compression and prediction, successively, for each layer. In this work we follow up on this idea and demonstrate the effectiveness of the Information-Plane visualization of DNNs. Our main results are: (i) most of the training epochs in standard DL are spent on {\emph compression} of the input to efficient representation and not on fitting the training labels. (ii) The representation compression phase begins when the training errors becomes small and the Stochastic Gradient Decent (SGD) epochs change from a fast drift to smaller training error into a stochastic relaxation, or random diffusion, constrained by the training error value. (iii) The converged layers lie on or very close to the Information Bottleneck (IB) theoretical bound, and the maps from the input to any hidden layer and from this hidden layer to the output satisfy the IB self-consistent equations. This generalization through noise mechanism is unique to Deep Neural Networks and absent in one layer networks. (iv) The training time is dramatically reduced when adding more hidden layers. Thus the main advantage of the hidden layers is computational. This can be explained by the reduced relaxation time, as this it scales super-linearly (exponentially for simple diffusion) with the information compression from the previous layer.},
archivePrefix = {arXiv},
arxivId = {1703.00810},
author = {Shwartz-Ziv, Ravid and Tishby, Naftali},
eprint = {1703.00810},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shwartz-Ziv, Tishby - 2017 - Opening the Black Box of Deep Neural Networks via Information.pdf:pdf},
pages = {1--19},
title = {{Opening the Black Box of Deep Neural Networks via Information}},
url = {http://arxiv.org/abs/1703.00810},
year = {2017}
}
@article{Dosovitskiy2016,
abstract = {Feature representations, both hand-designed and learned ones, are often hard to analyze and interpret, even when they are extracted from visual data. We propose a new approach to study image representations by inverting them with an up-convolutional neural network. We apply the method to shallow representations (HOG, SIFT, LBP), as well as to deep networks. For shallow representations our approach provides significantly better reconstructions than existing methods, revealing that there is surprisingly rich information contained in these features. Inverting a deep network trained on ImageNet provides several insights into the properties of the feature representation learned by the network. Most strikingly, the colors and the rough contours of an image can be reconstructed from activations in higher network layers and even from the predicted class probabilities.},
archivePrefix = {arXiv},
arxivId = {1506.02753},
author = {Dosovitskiy, Alexey and Brox, Thomas},
doi = {10.1109/CVPR.2016.522},
eprint = {1506.02753},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dosovitskiy, Brox - 2016 - Inverting visual representations with convolutional networks.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {4829--4837},
publisher = {IEEE},
title = {{Inverting visual representations with convolutional networks}},
volume = {2016-Decem},
year = {2016}
}
@article{Downs2020,
abstract = {Algorithmic recourse is the task of generating a set of actions that will allow individuals to achieve a more favorable outcome under a given algo-rithmic decision system. Using the Conditional Subspace Variational Autoencoder (CSVAE), we propose a novel algorithmic recourse generation method, CRUDS, that generates multiple recourse satisfying underlying structure of the data as well as end-user specified constraints. We evaluate our method qualitatively and quantitatively on several synthetic and real datasets, demonstrating that CRUDS proposes recourse that are more realistic and actionable than baselines.},
author = {Downs, Michael and Chu, Jonathan L and Yacoby, Yaniv and Doshi-Velez, Finale and Pan, Weiwei},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Downs et al. - 2020 - CRUDS Counterfactual Recourse Using Disentangled Subspaces(2).pdf:pdf},
number = {Whi},
title = {{CRUDS: Counterfactual Recourse Using Disentangled Subspaces}},
year = {2020}
}
@article{Meng2019,
abstract = {Existing method generates class activation map (CAM) by a set of fixed classes (i.e., using all the classes), while the discriminative cues between class pairs are not considered. Note that activation maps by considering different class pair are complementary, and therefore can provide more discriminative cues to overcome the shortcoming of the existing CAM generation that the highlighted regions are usually local part regions rather than global object regions due to the lack of object cues. In this paper, we generate CAM by using a few of representative classes, with aim of extracting more discriminative cues by considering each class pair to obtain CAM more globally. The advantages are twofold. Firstly, the representative classes are able to obtain activation regions that are complementary to each other, and therefore leads to generating activation map more accurately. Secondly, we only need to consider a small number of representative classes, making the CAM generation suitable for small networks. We propose a clustering based method to select the representative classes. Multiple binary classification models rather than a multiple class classification model are used to generate the CAM. Moreover, we propose a multi-layer fusion based CAM generation method to simultaneously combine high-level semantic features and low-level detail features. We validate the proposed method on the PASCAL VOC and COCO database in terms of segmentation groundtruth. Various networks such as classical network (Resnet-50, Resent-101 and Resnet-152) and small network (VGG-19, Resnet-18 and Mobilenet) are considered. Experimental results show that the proposed method improves the CAM generation obviously.},
archivePrefix = {arXiv},
arxivId = {1901.07683},
author = {Meng, Fanman and Huang, Kaixu and Li, Hongliang and Wu, Qingbo},
eprint = {1901.07683},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Meng et al. - 2019 - Class activation map generation by representative class selection and multi-layer feature fusion.pdf:pdf},
issn = {23318422},
journal = {arXiv},
title = {{Class activation map generation by representative class selection and multi-layer feature fusion}},
year = {2019}
}
@article{Mittelstadt2019,
abstract = {Recent work on interpretability in machine learning and AI has focused on the building of simplified models that approximate the true criteria used to make decisions. These models are a useful pedagogical device for teaching trained professionals how to predict what decisions will be made by the complex system, and most importantly how the system might break. However, when considering any such model it's important to remember Box's maxim that "All models are wrong but some are useful." We focus on the distinction between these models and explanations in philosophy and sociology. These models can be understood as a "do it yourself kit" for explanations, allowing a practitioner to directly answer "what if questions" or generate contrastive explanations without external assistance. Although a valuable ability, giving these models as explanations appears more difficult than necessary, and other forms of explanation may not have the same trade-offs. We contrast the different schools of thought on what makes an explanation, and suggest that machine learning might benefit from viewing the problem more broadly.},
archivePrefix = {arXiv},
arxivId = {1811.01439},
author = {Mittelstadt, Brent and Russell, Chris and Wachter, Sandra},
doi = {10.1145/3287560.3287574},
eprint = {1811.01439},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mittelstadt, Russell, Wachter - 2019 - Explaining explanations in AI(2).pdf:pdf},
isbn = {9781450361255},
journal = {FAT* 2019 - Proceedings of the 2019 Conference on Fairness, Accountability, and Transparency},
keywords = {Accountability,Explanations,Interpretability,Philosophy of Science},
pages = {279--288},
title = {{Explaining explanations in AI}},
year = {2019}
}
@article{Manuscript2020,
abstract = {Cladding austenitic stainless steels are trendy these days in pressure vessels to enhance surface qualities. In this work, austenitic stainless steel clad layers deposited by flux cored arc welding process on structural steel plates used in boiler construction are investigated. To facilitate this, composite clad layers are produced with 316L stainless steel deposits on low carbon structural steel plates based on the design of experiments. Results exhibit an incremental trend of thermal conductivity with respect to percentage of weld dilution. A linear mathematical relationship is established between the percentage of weld dilution and post-weld thermal conductivity of clad layers for the prediction of thermal conductivity of clad layer deposits. These results could be supportive in the fabrication industries for producing energy efficient thermal equipment},
author = {Manuscript, Accepted},
title = {{Ac ce us}},
year = {2020}
}
@article{Raghu2017,
abstract = {We propose a new technique, Singular Vector Canonical Correlation Analysis (SVCCA), a tool for quickly comparing two representations in a way that is both invariant to affine transform (allowing comparison between different layers and networks) and fast to compute (allowing more comparisons to be calculated than with previous methods). We deploy this tool to measure the intrinsic dimensionality of layers, showing in some cases needless over-parameterization; to probe learning dynamics throughout training, finding that networks converge to final representations from the bottom up; to show where class-specific information in networks is formed; and to suggest new training regimes that simultaneously save computation and overfit less.},
archivePrefix = {arXiv},
arxivId = {1706.05806},
author = {Raghu, Maithra and Gilmer, Justin and Yosinski, Jason and Sohl-Dickstein, Jascha},
eprint = {1706.05806},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Raghu et al. - 2017 - SVCCA Singular vector canonical correlation analysis for deep learning dynamics and interpretability.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {Nips},
pages = {6077--6086},
title = {{SVCCA: Singular vector canonical correlation analysis for deep learning dynamics and interpretability}},
volume = {2017-Decem},
year = {2017}
}
@article{Yuan2019,
abstract = {Interpreting deep neural networks is of great importance to understand and verify deep models for natural language processing (NLP) tasks. However, most existing approaches only focus on improving the performance of models but ignore their interpretability. In this work, we propose an approach to investigate the meaning of hidden neurons of the convolutional neural network (CNN) models. We first employ saliency map and optimization techniques to approximate the detected information of hidden neurons from input sentences. Then we develop regularization terms and explore words in vocabulary to interpret such detected information. Experimental results demonstrate that our approach can identify meaningful and reasonable interpretations for hidden spatial locations. Additionally, we show that our approach can describe the decision procedure of deep NLP models.},
author = {Yuan, Hao and Chen, Yongjun and Hu, Xia and Ji, Shuiwang},
doi = {10.1609/aaai.v33i01.33015717},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yuan et al. - 2019 - Interpreting deep models for text analysis via optimization and regularization methods.pdf:pdf},
isbn = {9781577358091},
issn = {2159-5399},
journal = {33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Innovative Applications of Artificial Intelligence Conference, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019},
keywords = {balanced sparsity, pruning, speedup, GPU},
pages = {5717--5724},
title = {{Interpreting deep models for text analysis via optimization and regularization methods}},
year = {2019}
}
@article{Danilevsky2020,
abstract = {Recent years have seen important advances in the quality of state-of-the-art models, but this has come at the expense of models becoming less interpretable. This survey presents an overview of the current state of Explainable AI (XAI), considered within the domain of Natural Language Processing (NLP). We discuss the main categorization of explanations, as well as the various ways explanations can be arrived at and visualized. We detail the operations and explainability techniques currently available for generating explanations for NLP model predictions, to serve as a resource for model developers in the community. Finally, we point out the current gaps and encourage directions for future work in this important research area.},
archivePrefix = {arXiv},
arxivId = {2010.00711},
author = {Danilevsky, Marina and Qian, Kun and Aharonov, Ranit and Katsis, Yannis and Kawas, Ban and Sen, Prithviraj},
eprint = {2010.00711},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Danilevsky et al. - 2020 - A survey of the state of explainable AI for natural language processing.pdf:pdf},
issn = {23318422},
journal = {arXiv},
number = {Section 5},
title = {{A survey of the state of explainable AI for natural language processing}},
year = {2020}
}
@article{Akula2020,
abstract = {We present CoCoX (short for Conceptual and Counterfactual Explanations), a model for explaining decisions made by a deep convolutional neural network (CNN). In Cognitive Psychology, the factors (or semantic-level features) that humans zoom in on when they imagine an alternative to a model prediction are often referred to as fault-lines. Motivated by this, our CoCoX model explains decisions made by a CNN using fault-lines. Specifically, given an input image I for which a CNN classification model M predicts class cpred, our fault-line based explanation identifies the minimal semantic-level features (e.g., stripes on zebra, pointed ears of dog), referred to as explainable concepts, that need to be added to or deleted from I in order to alter the classification category of I by M to another specified class calt. We argue that, due to the conceptual and counterfactual nature of fault-lines, our CoCoX explanations are practical and more natural for both expert and non-expert users to understand the internal workings of complex deep learning models. Extensive quantitative and qualitative experiments verify our hypotheses, showing that CoCoX significantly outperforms the state-of-the-art explainable AI models.},
author = {Akula, Arjun and Wang, Shuai and Zhu, Song-Chun},
doi = {10.1609/aaai.v34i03.5643},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Akula, Wang, Zhu - 2020 - CoCoX Generating Conceptual and Counterfactual Explanations via Fault-Lines.pdf:pdf},
isbn = {9781577358350},
issn = {2159-5399},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
keywords = {Humans and AI},
number = {03},
pages = {2594--2601},
title = {{CoCoX: Generating Conceptual and Counterfactual Explanations via Fault-Lines}},
volume = {34},
year = {2020}
}
@inproceedings{Mahendran2016a,
author = {Mahendran, Aravindh and Vedaldi, Andrea},
booktitle = {European Conference on Computer Vision},
doi = {10.1007/978-3-319-46466-4},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mahendran, Vedaldi - 2016 - Salient Deconvolutional Networks.pdf:pdf},
isbn = {9783319464749},
issn = {16113349},
keywords = {deconvnets,saliency,segmentation},
title = {{Salient Deconvolutional Networks}},
year = {2016}
}
@article{Manuscript2020,
abstract = {Cladding austenitic stainless steels are trendy these days in pressure vessels to enhance surface qualities. In this work, austenitic stainless steel clad layers deposited by flux cored arc welding process on structural steel plates used in boiler construction are investigated. To facilitate this, composite clad layers are produced with 316L stainless steel deposits on low carbon structural steel plates based on the design of experiments. Results exhibit an incremental trend of thermal conductivity with respect to percentage of weld dilution. A linear mathematical relationship is established between the percentage of weld dilution and post-weld thermal conductivity of clad layers for the prediction of thermal conductivity of clad layer deposits. These results could be supportive in the fabrication industries for producing energy efficient thermal equipment},
author = {Manuscript, Accepted},
title = {{Ac ce us}},
year = {2020}
}
@article{Mohseni2018,
abstract = {Research in interpretable machine learning proposes different computational and human subject approaches to evaluate model saliency explanations. These approaches measure different qualities of explanations to achieve diverse goals in designing interpretable machine learning systems. In this paper, we propose a human attention benchmark for image and text domains using multi-layer human attention masks aggregated from multiple human annotators. We then present an evaluation study to evaluate model saliency explanations obtained using Grad-cam and LIME techniques. We demonstrate our benchmark's utility for quantitative evaluation of model explanations by comparing it with human subjective ratings and ground-truth single-layer segmentation masks evaluations. Our study results show that our threshold agnostic evaluation method with the human attention baseline is more effective than single-layer object segmentation masks to ground truth. Our experiments also reveal user biases in the subjective rating of model saliency explanations.},
archivePrefix = {arXiv},
arxivId = {1801.05075},
author = {Mohseni, Sina and Block, Jeremy E. and Ragan, Eric D.},
eprint = {1801.05075},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mohseni, Block, Ragan - 2018 - A Human-Grounded Evaluation Benchmark for Local Explanations of Machine Learning.pdf:pdf},
title = {{A Human-Grounded Evaluation Benchmark for Local Explanations of Machine Learning}},
url = {http://arxiv.org/abs/1801.05075},
year = {2018}
}
@article{Mundhenk2019,
abstract = {We describe an explainable AI saliency map method for use with deep convolu-tional neural networks (CNN) that is much more efficient than popular gradient methods. It is also quantitatively similar and better in accuracy. Our technique works by measuring information at the end of each network scale which is then combined into a single saliency map. We describe how saliency measures can be made more efficient by exploiting Saliency Map Order Equivalence. Finally, we visualize individual scale/layer contributions by using a Layer Ordered Visualization of Information. This provides an interesting comparison of scale information contributions within the network not provided by other saliency map methods. Since our method only requires a single forward pass through a few of the layers in a network, it is at least 97x faster than Guided Backprop and much more accurate. Using our method instead of Guided Backprop, class activation methods such as Grad-CAM, Grad-CAM++ and Smooth Grad-CAM++ will run several orders of magnitude faster, have a significantly smaller memory footprint and be more accurate. This will make such methods feasible on resource limited platforms such as robots, cell phones and low cost industrial devices. This will also significantly help them work in extremely data intensive applications such as satellite image processing. All without sacrificing accuracy. Our method is generally straight forward and should be applicable to the most commonly used CNNs. We also show examples of our method used to enhance Grad-CAM++ 1.},
archivePrefix = {arXiv},
arxivId = {1911.11293},
author = {Mundhenk, T. Nathan and Chen, Barry Y. and Friedland, Gerald},
eprint = {1911.11293},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mundhenk, Chen, Friedland - 2019 - Efficient saliency maps for explainable ai.pdf:pdf},
issn = {23318422},
journal = {arXiv},
pages = {1--42},
title = {{Efficient saliency maps for explainable ai}},
year = {2019}
}
@article{Bahdanau2015,
abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder–decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
archivePrefix = {arXiv},
arxivId = {1409.0473},
author = {Bahdanau, Dzmitry and Cho, Kyung Hyun and Bengio, Yoshua},
eprint = {1409.0473},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bahdanau, Cho, Bengio - 2015 - Neural machine translation by jointly learning to align and translate.pdf:pdf},
journal = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
pages = {1--15},
title = {{Neural machine translation by jointly learning to align and translate}},
year = {2015}
}
@article{Kim2018,
abstract = {The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net's internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result-for example, how sensitive a prediction of zebra is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.},
archivePrefix = {arXiv},
arxivId = {1711.11279},
author = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and Sayres, Rory},
eprint = {1711.11279},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim et al. - 2018 - Interpretability beyond feature attribution Quantitative Testing with Concept Activation Vectors (TCAV).pdf:pdf},
isbn = {9781510867963},
journal = {35th International Conference on Machine Learning, ICML 2018},
pages = {4186--4195},
title = {{Interpretability beyond feature attribution: Quantitative Testing with Concept Activation Vectors (TCAV)}},
volume = {6},
year = {2018}
}
@article{Muller,
author = {M{\"{u}}ller, Klaus-robert},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/M{\"{u}}ller - Unknown - Machine Learning and AI for the sciences-Towards Understanding Klaus-Robert M{\"{u}}ller !!et al.!!(2).pdf:pdf},
journal = {Tutorial},
title = {{Machine Learning and AI for the sciences-Towards Understanding Klaus-Robert M{\"{u}}ller !!et al.!!}}
}
@article{Poyiadzi2020,
abstract = {Work in Counterfactual Explanations tends to focus on the principle of "the closest possible world" that identifies small changes leading to the desired outcome. In this paper we argue that while this approach might initially seem intuitively appealing it exhibits shortcomings not addressed in the current literature. First, a counterfactual example generated by the state-of-the-art systems is not necessarily representative of the underlying data distribution, and may therefore prescribe unachievable goals (e.g., an unsuccessful life insurance applicant with severe disability may be advised to do more sports). Secondly, the counterfactuals may not be based on a "feasible path" between the current state of the subject and the suggested one, making actionable recourse infeasible (e.g., lowskilled unsuccessful mortgage applicants may be told to double their salary, which may be hard without first increasing their skill level). These two shortcomings may render counterfactual explanations impractical and sometimes outright offensive. To address these two major flaws, first of all, we propose a new line of Counterfactual Explanations research aimed at providing actionable and feasible paths to transform a selected instance into one that meets a certain goal. Secondly, we propose FACE: an algorithmically sound way of uncovering these "feasible paths" based on the shortest path distances defined via density-weighted metrics. Our approach generates counterfactuals that are coherent with the underlying data distribution and supported by the "feasible paths" of change, which are achievable and can be tailored to the problem at hand.},
archivePrefix = {arXiv},
arxivId = {1909.09369},
author = {Poyiadzi, Rafael and Sokol, Kacper and Santos-Rodriguez, Raul and {De Bie}, Tijl and Flach, Peter},
doi = {10.1145/3375627.3375850},
eprint = {1909.09369},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Poyiadzi et al. - 2020 - FACE Feasible and actionable counterfactual explanations(2).pdf:pdf},
isbn = {9781450371100},
journal = {AIES 2020 - Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
keywords = {Black-box Models,Counterfactuals,Explainability,Interpretability},
pages = {344--350},
title = {{FACE: Feasible and actionable counterfactual explanations}},
year = {2020}
}
@article{Zhang2017,
author = {Zhang, Quanshi and Cao, Ruiming and Wu, Ying Nian and Zhu, Song Chun},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2017 - Growing Interpretable Part Graphs on ConvNets via Multi-Shot Learning.pdf:pdf},
journal = {Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17) Growing},
keywords = {Machine Learning Methods},
pages = {2898--2906},
title = {{Growing Interpretable Part Graphs on ConvNets via Multi-Shot Learning}},
year = {2017}
}
@article{Selvaraju2016,
abstract = {We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing input regions that are 'important' for predictions -- or visual explanations. Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses class-specific gradient information to localize important regions. These localizations are combined with existing pixel-space visualizations to create a novel high-resolution and class-discriminative visualization called Guided Grad-CAM. These methods help better understand CNN-based models, including image captioning and visual question answering (VQA) models. We evaluate our visual explanations by measuring their ability to discriminate between classes, to inspire trust in humans, and their correlation with occlusion maps. Grad-CAM provides a new way to understand CNN-based models. We have released code, an online demo hosted on CloudCV, and a full version of this extended abstract.},
archivePrefix = {arXiv},
arxivId = {1611.07450},
author = {Selvaraju, Ramprasaath R and Das, Abhishek and Vedantam, Ramakrishna and Cogswell, Michael and Parikh, Devi and Batra, Dhruv},
eprint = {1611.07450},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Selvaraju et al. - 2016 - Grad-CAM Why did you say that.pdf:pdf},
pages = {1--4},
title = {{Grad-CAM: Why did you say that?}},
url = {http://arxiv.org/abs/1611.07450},
year = {2016}
}
@article{Selvaraju2016,
abstract = {We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing input regions that are 'important' for predictions -- or visual explanations. Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses class-specific gradient information to localize important regions. These localizations are combined with existing pixel-space visualizations to create a novel high-resolution and class-discriminative visualization called Guided Grad-CAM. These methods help better understand CNN-based models, including image captioning and visual question answering (VQA) models. We evaluate our visual explanations by measuring their ability to discriminate between classes, to inspire trust in humans, and their correlation with occlusion maps. Grad-CAM provides a new way to understand CNN-based models. We have released code, an online demo hosted on CloudCV, and a full version of this extended abstract.},
archivePrefix = {arXiv},
arxivId = {1611.07450},
author = {Selvaraju, Ramprasaath R and Das, Abhishek and Vedantam, Ramakrishna and Cogswell, Michael and Parikh, Devi and Batra, Dhruv},
eprint = {1611.07450},
pages = {1--4},
title = {{Grad-CAM: Why did you say that?}},
url = {http://arxiv.org/abs/1611.07450},
year = {2016}
}
@article{Cortez2013,
abstract = {In this paper, we propose a new visualization approach based on a Sensitivity Analysis (SA) to extract human understandable knowledge from supervised learning black box data mining models, such as Neural Networks (NNs), Support Vector Machines (SVMs) and ensembles, including Random Forests (RFs). Five SA methods (three of which are purely new) and four measures of input importance (one novel) are presented. Also, the SA approach is adapted to handle discrete variables and to aggregate multiple sensitivity responses. Moreover, several visualizations for the SA results are introduced, such as input pair importance color matrix and variable effect characteristic surface. A wide range of experiments was performed in order to test the SA methods and measures by fitting four well-known models (NN, SVM, RF and decision trees) to synthetic datasets (five regression and five classification tasks). In addition, the visualization capabilities of the SA are demonstrated using four real-world datasets (e.g., bank direct marketing and white wine quality). {\textcopyright} 2012 Elsevier Inc. All rights reserved.},
author = {Cortez, Paulo and Embrechts, Mark J.},
doi = {10.1016/j.ins.2012.10.039},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cortez, Embrechts - 2013 - Using sensitivity analysis and visualization techniques to open black box data mining models.pdf:pdf},
issn = {00200255},
journal = {Information Sciences},
keywords = {Classification,Input importance,Regression,Sensitivity analysis,Supervised data mining,Visualization},
pages = {1--17},
publisher = {Elsevier Inc.},
title = {{Using sensitivity analysis and visualization techniques to open black box data mining models}},
url = {http://dx.doi.org/10.1016/j.ins.2012.10.039},
volume = {225},
year = {2013}
}
@article{Selvaraju2016,
abstract = {We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing input regions that are 'important' for predictions -- or visual explanations. Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses class-specific gradient information to localize important regions. These localizations are combined with existing pixel-space visualizations to create a novel high-resolution and class-discriminative visualization called Guided Grad-CAM. These methods help better understand CNN-based models, including image captioning and visual question answering (VQA) models. We evaluate our visual explanations by measuring their ability to discriminate between classes, to inspire trust in humans, and their correlation with occlusion maps. Grad-CAM provides a new way to understand CNN-based models. We have released code, an online demo hosted on CloudCV, and a full version of this extended abstract.},
archivePrefix = {arXiv},
arxivId = {1611.07450},
author = {Selvaraju, Ramprasaath R and Das, Abhishek and Vedantam, Ramakrishna and Cogswell, Michael and Parikh, Devi and Batra, Dhruv},
eprint = {1611.07450},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Selvaraju et al. - 2016 - Grad-CAM Why did you say that.pdf:pdf},
pages = {1--4},
title = {{Grad-CAM: Why did you say that?}},
url = {http://arxiv.org/abs/1611.07450},
year = {2016}
}
@article{Schirrmeister2017,
abstract = {Deep learning with convolutional neural networks (deep ConvNets) has revolutionized computer vision through end-to-end learning, that is, learning from the raw data. There is increasing interest in using deep ConvNets for end-to-end EEG analysis, but a better understanding of how to design and train ConvNets for end-to-end EEG decoding and how to visualize the informative EEG features the ConvNets learn is still needed. Here, we studied deep ConvNets with a range of different architectures, designed for decoding imagined or executed tasks from raw EEG. Our results show that recent advances from the machine learning field, including batch normalization and exponential linear units, together with a cropped training strategy, boosted the deep ConvNets decoding performance, reaching at least as good performance as the widely used filter bank common spatial patterns (FBCSP) algorithm (mean decoding accuracies 82.1% FBCSP, 84.0% deep ConvNets). While FBCSP is designed to use spectral power modulations, the features used by ConvNets are not fixed a priori. Our novel methods for visualizing the learned features demonstrated that ConvNets indeed learned to use spectral power modulations in the alpha, beta, and high gamma frequencies, and proved useful for spatially mapping the learned features by revealing the topography of the causal contributions of features in different frequency bands to the decoding decision. Our study thus shows how to design and train ConvNets to decode task-related information from the raw EEG without handcrafted features and highlights the potential of deep ConvNets combined with advanced visualization techniques for EEG-based brain mapping. Hum Brain Mapp 38:5391–5420, 2017. {\textcopyright} 2017 Wiley Periodicals, Inc.},
archivePrefix = {arXiv},
arxivId = {1703.05051},
author = {Schirrmeister, Robin Tibor and Springenberg, Jost Tobias and Fiederer, Lukas Dominique Josef and Glasstetter, Martin and Eggensperger, Katharina and Tangermann, Michael and Hutter, Frank and Burgard, Wolfram and Ball, Tonio},
doi = {10.1002/hbm.23730},
eprint = {1703.05051},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schirrmeister et al. - 2017 - Deep learning with convolutional neural networks for EEG decoding and visualization.pdf:pdf},
issn = {10970193},
journal = {Human Brain Mapping},
keywords = {EEG analysis,brain mapping,brain–computer interface,brain–machine interface,electroencephalography,end-to-end learning,machine learning,model interpretability},
number = {11},
pages = {5391--5420},
pmid = {28782865},
title = {{Deep learning with convolutional neural networks for EEG decoding and visualization}},
volume = {38},
year = {2017}
}
@article{Xu2019,
abstract = {Deep learning has made significant contribution to the recent progress in artificial intelligence. In comparison to traditional machine learning methods such as decision trees and support vector machines, deep learning methods have achieved substantial improvement in various prediction tasks. However, deep neural networks (DNNs) are comparably weak in explaining their inference processes and final results, and they are typically treated as a black-box by both developers and users. Some people even consider DNNs (deep neural networks) in the current stage rather as alchemy, than as real science. In many real-world applications such as business decision, process optimization, medical diagnosis and investment recommendation, explainability and transparency of our AI systems become particularly essential for their users, for the people who are affected by AI decisions, and furthermore, for the researchers and developers who create the AI solutions. In recent years, the explainability and explainable AI have received increasing attention by both research community and industry. This paper first introduces the history of Explainable AI, starting from expert systems and traditional machine learning approaches to the latest progress in the context of modern deep learning, and then describes the major research areas and the state-of-art approaches in recent years. The paper ends with a discussion on the challenges and future directions.},
author = {Xu, Feiyu and Uszkoreit, Hans and Du, Yangzhou and Fan, Wei and Zhao, Dongyan and Zhu, Jun},
doi = {10.1007/978-3-030-32236-6_51},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu et al. - 2019 - Explainable AI A Brief Survey on History, Research Areas, Approaches and Challenges.pdf:pdf},
isbn = {9783030322359},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Explainable artificial intelligence,Explainable interfaces,Intelligible machine learning,Interpretability,XAI},
pages = {563--574},
title = {{Explainable AI: A Brief Survey on History, Research Areas, Approaches and Challenges}},
volume = {11839 LNAI},
year = {2019}
}
@article{Selvaraju2016,
abstract = {We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing input regions that are 'important' for predictions -- or visual explanations. Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses class-specific gradient information to localize important regions. These localizations are combined with existing pixel-space visualizations to create a novel high-resolution and class-discriminative visualization called Guided Grad-CAM. These methods help better understand CNN-based models, including image captioning and visual question answering (VQA) models. We evaluate our visual explanations by measuring their ability to discriminate between classes, to inspire trust in humans, and their correlation with occlusion maps. Grad-CAM provides a new way to understand CNN-based models. We have released code, an online demo hosted on CloudCV, and a full version of this extended abstract.},
archivePrefix = {arXiv},
arxivId = {1611.07450},
author = {Selvaraju, Ramprasaath R and Das, Abhishek and Vedantam, Ramakrishna and Cogswell, Michael and Parikh, Devi and Batra, Dhruv},
eprint = {1611.07450},
pages = {1--4},
title = {{Grad-CAM: Why did you say that?}},
url = {http://arxiv.org/abs/1611.07450},
year = {2016}
}
@article{Strumbelj2014,
abstract = {We present a sensitivity analysis-based method for explaining prediction models that can be applied to any type of classification or regression model. Its advantage over existing general methods is that all subsets of input features are perturbed, so interactions and redundancies between features are taken into account. Furthermore, when explaining an additive model, the method is equivalent to commonly used additive model-specific methods. We illustrate the method's usefulness with examples from artificial and real-world data sets and an empirical analysis of running times. Results from a controlled experiment with 122 participants suggest that the method's explanations improved the participants' understanding of the model.},
author = {{\v{S}}trumbelj, Erik and Kononenko, Igor},
doi = {10.1007/s10115-013-0679-x},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/{\v{S}}trumbelj, Kononenko - 2014 - Explaining prediction models and individual predictions with feature contributions(2).pdf:pdf},
issn = {02193116},
journal = {Knowledge and Information Systems},
keywords = {Data mining,Decision support,Interpretability,Knowledge discovery,Visualization},
number = {3},
pages = {647--665},
title = {{Explaining prediction models and individual predictions with feature contributions}},
volume = {41},
year = {2014}
}
@article{Bass2020,
abstract = {Feature attribution (FA), or the assignment of class-relevance to different locations in an image, is important for many classification problems but is particularly crucial within the neuroscience domain, where accurate mechanistic models of behaviours, or disease, require knowledge of all features discriminative of a trait. At the same time, predicting class relevance from brain images is challenging as phenotypes are typically heterogeneous, and changes occur against a background of significant natural variation. Here, we present a novel framework for creating class specific FA maps through image-to-image translation. We propose the use of a VAE-GAN to explicitly disentangle class relevance from background features for improved interpretability properties, which results in meaningful FA maps. We validate our method on 2D and 3D brain image datasets of dementia (ADNI dataset), ageing (UK Biobank), and (simulated) lesion detection. We show that FA maps generated by our method outperform baseline FA methods when validated against ground truth. More significantly, our approach is the first to use latent space sampling to support exploration of phenotype variation. Our code will be available online at https://github.com/CherBass/ICAM.},
archivePrefix = {arXiv},
arxivId = {2006.08287},
author = {Bass, Cher and Tudosiu, Petru Daniel and da Silva, Mariana and Smith, Stephen M. and Sudre, Carole and Robinson, Emma C.},
eprint = {2006.08287},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bass et al. - 2020 - ICAM Interpretable Classification via Disentangled Representations and Feature Attribution Mapping.pdf:pdf},
issn = {23318422},
journal = {arXiv},
keywords = {Classification,Domain translation,Feature attribution,Generative adversarial network,Interpretable,Neuroimaging,Variational autoencoder},
title = {{ICAM: Interpretable Classification via Disentangled Representations and Feature Attribution Mapping}},
year = {2020}
}
@article{Selvaraju2016,
abstract = {We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing input regions that are 'important' for predictions -- or visual explanations. Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses class-specific gradient information to localize important regions. These localizations are combined with existing pixel-space visualizations to create a novel high-resolution and class-discriminative visualization called Guided Grad-CAM. These methods help better understand CNN-based models, including image captioning and visual question answering (VQA) models. We evaluate our visual explanations by measuring their ability to discriminate between classes, to inspire trust in humans, and their correlation with occlusion maps. Grad-CAM provides a new way to understand CNN-based models. We have released code, an online demo hosted on CloudCV, and a full version of this extended abstract.},
archivePrefix = {arXiv},
arxivId = {1611.07450},
author = {Selvaraju, Ramprasaath R and Das, Abhishek and Vedantam, Ramakrishna and Cogswell, Michael and Parikh, Devi and Batra, Dhruv},
eprint = {1611.07450},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Selvaraju et al. - 2016 - Grad-CAM Why did you say that.pdf:pdf},
pages = {1--4},
title = {{Grad-CAM: Why did you say that?}},
url = {http://arxiv.org/abs/1611.07450},
year = {2016}
}
@article{Graziani2018,
abstract = {Explanations for deep neural network predictions in terms of domain-related concepts can be valuable in medical applications, where justifications are important for confidence in the decision-making. In this work, we propose a methodology to exploit continuous concept measures as Regression Concept Vectors (RCVs) in the activation space of a layer. The directional derivative of the decision function along the RCVs represents the network sensitivity to increasing values of a given concept measure. When applied to breast cancer grading, nuclei texture emerges as a relevant concept in the detection of tumor tissue in breast lymph node samples. We evaluate score robustness and consistency by statistical analysis.},
archivePrefix = {arXiv},
arxivId = {1904.04520},
author = {Graziani, Mara and Andrearczyk, Vincent and M{\"{u}}ller, Henning},
doi = {10.1007/978-3-030-02628-8_14},
eprint = {1904.04520},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Graziani, Andrearczyk, M{\"{u}}ller - 2018 - Regression concept vectors for bidirectional explanations in histopathology.pdf:pdf},
isbn = {9783030026271},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Concept vector,Histopathology,Interpretability},
pages = {124--132},
title = {{Regression concept vectors for bidirectional explanations in histopathology}},
volume = {11038 LNCS},
year = {2018}
}
@article{Eberle2020,
abstract = {Many learning algorithms such as kernel machines, nearest neighbors, clustering, or anomaly detection, are based on the concept of 'distance' or 'similarity'. Before similarities are used for training an actual machine learning model, we would like to verify that they are bound to meaningful patterns in the data. In this paper, we propose to make similarities interpretable by augmenting them with an explanation in terms of input features. We develop BiLRP, a scalable and theoretically founded method to systematically decompose similarity scores on pairs of input features. Our method can be expressed as a composition of LRP explanations, which were shown in previous works to scale to highly nonlinear functions. Through an extensive set of experiments, we demonstrate that BiLRP robustly explains complex similarity models, e.g. built on VGG-16 deep neural network features. Additionally, we apply our method to an open problem in digital humanities: detailed assessment of similarity between historical documents such as astronomical tables. Here again, BiLRP provides insight and brings verifiability into a highly engineered and problem-specific similarity model.},
archivePrefix = {arXiv},
arxivId = {2003.05431},
author = {Eberle, Oliver and Buttner, Jochen and Krautli, Florian and Muller, Klaus-Robert and Valleriani, Matteo and Montavon, Gregoire},
doi = {10.1109/tpami.2020.3020738},
eprint = {2003.05431},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Eberle et al. - 2020 - Building and Interpreting Deep Similarity Models.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
pages = {1--1},
title = {{Building and Interpreting Deep Similarity Models}},
year = {2020}
}
@article{Erion2019,
abstract = {Two important topics in deep learning both involve incorporating humans into the modeling process: Model priors transfer information from humans to a model by constraining the model's parameters; Model attributions transfer information from a model to humans by explaining the model's behavior. We propose connecting these topics with attribution priors†, which allow humans to use the common language of attributions to enforce prior expectations about a model's behavior during training. We develop a differentiable axiomatic feature attribution method called expected gradients and show how to directly regularize these attributions during training. We demonstrate the broad applicability of attribution priors (Ω) by presenting three distinct examples that regularize models to behave more intuitively in three different domains: 1) on image data, Ωpixel encourages models to have piecewise smooth attribution maps; 2) on gene expression data, Ωgraph encourages models to treat functionally related genes similarly; 3) on a health care dataset, Ωsparse encourages models to rely on fewer features. In all three domains, attribution priors produce models with more intuitive behavior and better generalization performance by encoding constraints that would otherwise be very difficult to encode using standard model priors.},
archivePrefix = {arXiv},
arxivId = {1906.10670},
author = {Erion, Gabriel and Janizek, Joseph D. and Sturmfels, Pascal and Lundberg, Scott and Lee, Su-In},
eprint = {1906.10670},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Erion et al. - 2019 - Learning Explainable Models Using Attribution Priors.pdf:pdf},
title = {{Learning Explainable Models Using Attribution Priors}},
url = {http://arxiv.org/abs/1906.10670},
year = {2019}
}
@article{Montavon2017,
abstract = {Nonlinear methods such as Deep Neural Networks (DNNs) are the gold standard for various challenging machine learning problems such as image recognition. Although these methods perform impressively well, they have a significant disadvantage, the lack of transparency, limiting the interpretability of the solution and thus the scope of application in practice. Especially DNNs act as black boxes due to their multilayer nonlinear structure. In this paper we introduce a novel methodology for interpreting generic multilayer neural networks by decomposing the network classification decision into contributions of its input elements. Although our focus is on image classification, the method is applicable to a broad set of input data, learning tasks and network architectures. Our method called deep Taylor decomposition efficiently utilizes the structure of the network by backpropagating the explanations from the output to the input layer. We evaluate the proposed method empirically on the MNIST and ILSVRC data sets.},
archivePrefix = {arXiv},
arxivId = {1512.02479},
author = {Montavon, Gr{\'{e}}goire and Lapuschkin, Sebastian and Binder, Alexander and Samek, Wojciech and M{\"{u}}ller, Klaus Robert},
doi = {10.1016/j.patcog.2016.11.008},
eprint = {1512.02479},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Montavon et al. - 2017 - Explaining nonlinear classification decisions with deep Taylor decomposition.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Deep neural networks,Heatmapping,Image recognition,Relevance propagation,Taylor decomposition},
pages = {211--222},
publisher = {Elsevier},
title = {{Explaining nonlinear classification decisions with deep Taylor decomposition}},
url = {http://dx.doi.org/10.1016/j.patcog.2016.11.008},
volume = {65},
year = {2017}
}
@article{Chen2019,
abstract = {When we are faced with challenging image classification tasks, we often explain our reasoning by dissecting the image, and pointing out prototypical aspects of one class or another. The mounting evidence for each of the classes helps us make our final decision. In this work, we introduce a deep network architecture - prototypical part network (ProtoPNet), that reasons in a similar way: the network dissects the image by finding prototypical parts, and combines evidence from the prototypes to make a final classification. The model thus reasons in a way that is qualitatively similar to the way ornithologists, physicians, and others would explain to people on how to solve challenging image classification tasks. The network uses only image-level labels for training without any annotations for parts of images. We demonstrate our method on the CUB-200-2011 dataset and the Stanford Cars dataset. Our experiments show that ProtoPNet can achieve comparable accuracy with its analogous non-interpretable counterpart, and when several ProtoPNets are combined into a larger network, it can achieve an accuracy that is on par with some of the best-performing deep models. Moreover, ProtoPNet provides a level of interpretability that is absent in other interpretable deep models.},
archivePrefix = {arXiv},
arxivId = {1806.10574},
author = {Chen, Chaofan and Li, Oscar and Tao, Chaofan and Barnett, Alina Jade and Su, Jonathan and Rudin, Cynthia},
eprint = {1806.10574},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2019 - This looks like that Deep learning for interpretable image recognition(2).pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {NeurIPS},
pages = {1--12},
title = {{This looks like that: Deep learning for interpretable image recognition}},
volume = {32},
year = {2019}
}
@article{Brunese2020,
abstract = {Background and Objective: Coronavirus disease (COVID-19) is an infectious disease caused by a new virus never identified before in humans. This virus causes respiratory disease (for instance, flu) with symptoms such as cough, fever and, in severe cases, pneumonia. The test to detect the presence of this virus in humans is performed on sputum or blood samples and the outcome is generally available within a few hours or, at most, days. Analysing biomedical imaging the patient shows signs of pneumonia. In this paper, with the aim of providing a fully automatic and faster diagnosis, we propose the adoption of deep learning for COVID-19 detection from X-rays. Method: In particular, we propose an approach composed by three phases: the first one to detect if in a chest X-ray there is the presence of a pneumonia. The second one to discern between COVID-19 and pneumonia. The last step is aimed to localise the areas in the X-ray symptomatic of the COVID-19 presence. Results and Conclusion: Experimental analysis on 6,523 chest X-rays belonging to different institutions demonstrated the effectiveness of the proposed approach, with an average time for COVID-19 detection of approximately 2.5 seconds and an average accuracy equal to 0.97.},
author = {Brunese, Luca and Mercaldo, Francesco and Reginelli, Alfonso and Santone, Antonella},
doi = {10.1016/j.cmpb.2020.105608},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brunese et al. - 2020 - Explainable Deep Learning for Pulmonary Disease and Coronavirus COVID-19 Detection from X-rays.pdf:pdf},
issn = {18727565},
journal = {Computer Methods and Programs in Biomedicine},
keywords = {Artificial intelligence,COVID-19,Chest,Coronavirus,Deep learning,Transfer learning},
pages = {105608},
pmid = {32599338},
publisher = {Elsevier B.V.},
title = {{Explainable Deep Learning for Pulmonary Disease and Coronavirus COVID-19 Detection from X-rays}},
url = {https://doi.org/10.1016/j.cmpb.2020.105608},
volume = {196},
year = {2020}
}
@article{Yang2019,
abstract = {Interpretable Machine Learning (IML) has become increasingly important in many real-world applications, such as autonomous cars and medical diagnosis, where explanations are significantly preferred to help people better understand how machine learning systems work and further enhance their trust towards systems. However, due to the diversified scenarios and subjective nature of explanations, we rarely have the ground truth for benchmark evaluation in IML on the quality of generated explanations. Having a sense of explanation quality not only matters for assessing system boundaries, but also helps to realize the true benefits to human users in practical settings. To benchmark the evaluation in IML, in this article, we rigorously define the problem of evaluating explanations, and systematically review the existing efforts from state-of-the-arts. Specifically, we summarize three general aspects of explanation (i.e., generalizability, fidelity and persuasibility) with formal definitions, and respectively review the representative methodologies for each of them under different tasks. Further, a unified evaluation framework is designed according to the hierarchical needs from developers and end-users, which could be easily adopted for different scenarios in practice. In the end, open problems are discussed, and several limitations of current evaluation techniques are raised for future explorations.},
archivePrefix = {arXiv},
arxivId = {1907.06831},
author = {Yang, Fan and Du, Mengnan and Hu, Xia},
eprint = {1907.06831},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang, Du, Hu - 2019 - Evaluating explanation without ground truth in interpretable machine learning.pdf:pdf},
issn = {23318422},
journal = {arXiv},
pages = {1--24},
title = {{Evaluating explanation without ground truth in interpretable machine learning}},
year = {2019}
}
@article{Manuscript2020,
abstract = {Cladding austenitic stainless steels are trendy these days in pressure vessels to enhance surface qualities. In this work, austenitic stainless steel clad layers deposited by flux cored arc welding process on structural steel plates used in boiler construction are investigated. To facilitate this, composite clad layers are produced with 316L stainless steel deposits on low carbon structural steel plates based on the design of experiments. Results exhibit an incremental trend of thermal conductivity with respect to percentage of weld dilution. A linear mathematical relationship is established between the percentage of weld dilution and post-weld thermal conductivity of clad layers for the prediction of thermal conductivity of clad layer deposits. These results could be supportive in the fabrication industries for producing energy efficient thermal equipment},
author = {Manuscript, Accepted},
title = {{Ac ce us}},
year = {2020}
}
@article{Zhang2018d,
abstract = {This paper reviews recent studies in understanding neural-network representations and learning neural networks with interpretable/disentangled middle-layer representations. Although deep neural networks have exhibited superior performance in various tasks, interpretability is always Achilles' heel of deep neural networks. At present, deep neural networks obtain high discrimination power at the cost of a low interpretability of their black-box representations. We believe that high model interpretability may help people break several bottlenecks of deep learning, e.g., learning from a few annotations, learning via human–computer communications at the semantic level, and semantically debugging network representations. We focus on convolutional neural networks (CNNs), and revisit the visualization of CNN representations, methods of diagnosing representations of pre-trained CNNs, approaches for disentangling pre-trained CNN representations, learning of CNNs with disentangled representations, and middle-to-end learning based on model interpretability. Finally, we discuss prospective trends in explainable artificial intelligence.},
archivePrefix = {arXiv},
arxivId = {1802.00614},
author = {shi Zhang, Quan and chun Zhu, Song},
doi = {10.1631/FITEE.1700808},
eprint = {1802.00614},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Zhu - 2018 - Visual interpretability for deep learning a survey(2).pdf:pdf},
issn = {20959230},
journal = {Frontiers of Information Technology and Electronic Engineering},
keywords = {Artificial intelligence,Deep learning,Interpretable model},
number = {1},
pages = {27--39},
title = {{Visual interpretability for deep learning: a survey}},
volume = {19},
year = {2018}
}
@article{Angelov2020,
abstract = {In this paper, we propose an elegant solution that is directly addressing the bottlenecks of the traditional deep learning approaches and offers an explainable internal architecture that can outperform the existing methods, requires very little computational resources (no need for GPUs) and short training times (in the order of seconds). The proposed approach, xDNN is using prototypes. Prototypes are actual training data samples (images), which are local peaks of the empirical data distribution called typicality as well as of the data density. This generative model is identified in a closed form and equates to the pdf but is derived automatically and entirely from the training data with no user- or problem-specific thresholds, parameters or intervention. The proposed xDNN offers a new deep learning architecture that combines reasoning and learning in a synergy. It is non-iterative and non-parametric, which explains its efficiency in terms of time and computational resources. From the user perspective, the proposed approach is clearly understandable to human users. We tested it on challenging problems as the classification of different lighting conditions for driving scenes (iROADS), object detection (Caltech-256, and Caltech-101), and SARS-CoV-2 identification via computed tomography scan (COVID CT-scans dataset). xDNN outperforms the other methods including deep learning in terms of accuracy, time to train and offers an explainable classifier.},
archivePrefix = {arXiv},
arxivId = {1912.02523},
author = {Angelov, Plamen and Soares, Eduardo},
doi = {10.1016/j.neunet.2020.07.010},
eprint = {1912.02523},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Angelov, Soares - 2020 - Towards explainable deep neural networks (xDNN)(2).pdf:pdf},
issn = {18792782},
journal = {Neural Networks},
keywords = {Deep-learning,Explainable AI,Interpretability,Prototype-based models},
pages = {185--194},
pmid = {32682084},
publisher = {Elsevier Ltd},
title = {{Towards explainable deep neural networks (xDNN)}},
url = {https://doi.org/10.1016/j.neunet.2020.07.010},
volume = {130},
year = {2020}
}
@article{Krishnan2017,
author = {Krishnan, Sanjay and Wu, Eugene},
doi = {10.1093/nq/s3-I.12.230-g},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krishnan, Wu - 1862 - PALM Machine Learning Explanations for Iterative Debugging.pdf:pdf},
isbn = {9781450350297},
issn = {00293970},
journal = {Notes and Queries},
number = {12},
pages = {230--231},
title = {{PALM: Machine Learning Explanations for Iterative Debugging}},
volume = {s3-I},
year = {1862}
}
@article{Zhang2020,
abstract = {In recent years, considerable progress has been made on improving the interpretability of machine learning models. This is essential, as complex deep learning models with millions of parameters produce state of the art results, but it can be nearly impossible to explain their predictions. While various explainability techniques have achieved impressive results, nearly all of them assume each data instance to be independent and identically distributed (iid). This excludes relational models, such as Statistical Relational Learning (SRL), and the recently popular Graph Neural Networks (GNNs), resulting in few options to explain them. While there does exist one work on explaining GNNs, GNN-Explainer, they assume access to the gradients of the model to learn explanations, which is restrictive in terms of its applicability across non-differentiable relational models and practicality. In this work, we develop RelEx, a model-agnostic relational explainer to explain black-box relational models with only access to the outputs of the black-box. RelEx is able to explain any relational model, including SRL models and GNNs. We compare RelEx to the state-of-the-art relational explainer, GNN-Explainer, and relational extensions of iid explanation models and show that RelEx achieves comparable or better performance, while remaining model-agnostic.},
archivePrefix = {arXiv},
arxivId = {2006.00305},
author = {Zhang, Yue and Defazio, David and Ramesh, Arti},
eprint = {2006.00305},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Defazio, Ramesh - 2020 - RelEx A Model-Agnostic Relational Model Explainer.pdf:pdf},
number = {iid},
title = {{RelEx: A Model-Agnostic Relational Model Explainer}},
url = {http://arxiv.org/abs/2006.00305},
year = {2020}
}
@article{Gomez2020,
abstract = {The continued improvements in the predictive accuracy of machine learning models have allowed for their widespread practical application. Yet, many decisions made with seemingly accurate models still require verification by domain experts. In addition, end-users of a model also want to understand the reasons behind specific decisions. Thus, the need for interpretability is increasingly paramount. In this paper we present an interactive visual analytics tool, ViCE, that generates counterfactual explanations to contextualize and evaluate model decisions. Each sample is assessed to identify the minimal set of changes needed to flip the model's output. These explanations aim to provide end-users with personalized actionable insights with which to understand, and possibly contest or improve, automated decisions. The results are effectively displayed in a visual interface where counterfactual explanations are highlighted and interactive methods are provided for users to explore the data and model. The functionality of the tool is demonstrated by its application to a home equity line of credit dataset.},
archivePrefix = {arXiv},
arxivId = {2003.02428},
author = {Gomez, Oscar and Holter, Steffen and Yuan, Jun and Bertini, Enrico},
eprint = {2003.02428},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gomez et al. - 2020 - ViCE Visual Counterfactual Explanations for Machine Learning Models(2).pdf:pdf},
isbn = {9781450371186},
keywords = {counterfactual ex-,explainability,features guarantees that the,interpretability,machine learning,only clarify the model,resulting interface does not,s decision but,the combination of these,the dataset},
title = {{ViCE: Visual Counterfactual Explanations for Machine Learning Models}},
url = {http://arxiv.org/abs/2003.02428},
year = {2020}
}
@article{Wang2019,
abstract = {Students learning performance prediction is a challenging task due to the dynamic, virtual environments and the personalized needs for different individuals. To ensure that learners' potential problems can be identified as early as possible, this paper aim to develop a predictive model for effective learning feature extracting, learning performance predicting and result reasoning. We first proposed a general learning feature quantification method to convert the raw data from e-learning systems into sets of independent learning features. Then, a weighted avg-pooling is chosen instead of typical max-pooling in a novel convolutional GRU network for learning performance prediction. Finally, an improved parallel xNN is provided to explain the prediction results. The relevance of positive/negative between features and result could help students find out which part should be improved. Experiments have been carried out over two real online courses data. Results show that our proposed approach performs favorably compared with several other state-of-the-art methods.},
author = {Wang, Xizhe and Wu, Pengze and Liu, Guang and Huang, Qionghao and Hu, Xiaoling and Xu, Haijiao},
doi = {10.1007/s00607-018-00699-9},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2019 - Learning performance prediction via convolutional GRU and explainable neural networks in e-learning environments.pdf:pdf},
isbn = {0060701800699},
issn = {0010485X},
journal = {Computing},
keywords = {Deep neural network,E-learning environments,Learning feature quantification,Learning performance prediction},
number = {6},
pages = {587--604},
publisher = {Springer Vienna},
title = {{Learning performance prediction via convolutional GRU and explainable neural networks in e-learning environments}},
url = {https://doi.org/10.1007/s00607-018-00699-9},
volume = {101},
year = {2019}
}
@article{Selvaraju2016,
abstract = {We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing input regions that are 'important' for predictions -- or visual explanations. Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses class-specific gradient information to localize important regions. These localizations are combined with existing pixel-space visualizations to create a novel high-resolution and class-discriminative visualization called Guided Grad-CAM. These methods help better understand CNN-based models, including image captioning and visual question answering (VQA) models. We evaluate our visual explanations by measuring their ability to discriminate between classes, to inspire trust in humans, and their correlation with occlusion maps. Grad-CAM provides a new way to understand CNN-based models. We have released code, an online demo hosted on CloudCV, and a full version of this extended abstract.},
archivePrefix = {arXiv},
arxivId = {1611.07450},
author = {Selvaraju, Ramprasaath R and Das, Abhishek and Vedantam, Ramakrishna and Cogswell, Michael and Parikh, Devi and Batra, Dhruv},
eprint = {1611.07450},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Selvaraju et al. - 2016 - Grad-CAM Why did you say that.pdf:pdf},
pages = {1--4},
title = {{Grad-CAM: Why did you say that?}},
url = {http://arxiv.org/abs/1611.07450},
year = {2016}
}
@article{Confalonieri2021,
abstract = {Explainability in Artificial Intelligence (AI) has been revived as a topic of active research by the need of conveying safety and trust to users in the “how” and “why” of automated decision-making in different applications such as autonomous driving, medical diagnosis, or banking and finance. While explainability in AI has recently received significant attention, the origins of this line of work go back several decades to when AI systems were mainly developed as (knowledge-based) expert systems. Since then, the definition, understanding, and implementation of explainability have been picked up in several lines of research work, namely, expert systems, machine learning, recommender systems, and in approaches to neural-symbolic learning and reasoning, mostly happening during different periods of AI history. In this article, we present a historical perspective of Explainable Artificial Intelligence. We discuss how explainability was mainly conceived in the past, how it is understood in the present and, how it might be understood in the future. We conclude the article by proposing criteria for explanations that we believe will play a crucial role in the development of human-understandable explainable systems. This article is categorized under: Fundamental Concepts of Data and Knowledge > Explainable AI Technologies > Artificial Intelligence.},
author = {Confalonieri, Roberto and Coba, Ludovik and Wagner, Benedikt and Besold, Tarek R.},
doi = {10.1002/widm.1391},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Confalonieri et al. - 2021 - A historical perspective of explainable Artificial Intelligence.pdf:pdf},
issn = {19424795},
journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
keywords = {explainable AI,explainable recommender systems,interpretable machine learning,neural-symbolic reasoning},
number = {1},
pages = {1--21},
title = {{A historical perspective of explainable Artificial Intelligence}},
volume = {11},
year = {2021}
}
@article{Manuscript2020,
abstract = {Cladding austenitic stainless steels are trendy these days in pressure vessels to enhance surface qualities. In this work, austenitic stainless steel clad layers deposited by flux cored arc welding process on structural steel plates used in boiler construction are investigated. To facilitate this, composite clad layers are produced with 316L stainless steel deposits on low carbon structural steel plates based on the design of experiments. Results exhibit an incremental trend of thermal conductivity with respect to percentage of weld dilution. A linear mathematical relationship is established between the percentage of weld dilution and post-weld thermal conductivity of clad layers for the prediction of thermal conductivity of clad layer deposits. These results could be supportive in the fabrication industries for producing energy efficient thermal equipment},
author = {Manuscript, Accepted},
title = {{Ac ce us}},
year = {2020}
}
@article{Datta2017,
abstract = {Algorithmic systems that employ machine learning play an increasing role in making substantive decisions in modern society, ranging from online personalization to insurance and credit decisions to predictive policing. But their decision-making processes are often opaque—it is difficult to explain why a certain decision was made. We develop a formal foundation to improve the transparency of such decision-making systems. Specifically, we introduce a family of Quantitative Input Influence (QII) measures that capture the degree of influence of inputs on outputs of systems. These measures provide a foundation for the design of transparency reports that accompany system decisions (e.g., explaining a specific credit decision) and for testing tools useful for internal and external oversight (e.g., to detect algorithmic discrimination). Distinctively, our causal QII measures carefully account for correlated inputs while measuring influence. They support a general class of transparency queries and can, in particular, explain decisions about individuals (e.g., a loan decision) and groups (e.g., disparate impact based on gender). Finally, since single inputs may not always have high influence, the QII measures also quantify the joint influence of a set of inputs (e.g., age and income) on outcomes (e.g. loan decisions) and the marginal influence of individual inputs within such a set (e.g., income). Since a single input may be part of multiple influential sets, the average marginal influence of the input is computed using principled aggregation measures, such as the Shapley value, previously applied to measure influence in voting. Further, since transparency reports could compromise privacy, we explore the transparency-privacy tradeoff and prove that a number of useful transparency reports can be made differentially private with very little addition of noise. Our empirical validation with standard machine learning algo- rithms demonstrates that QII measures are a useful transparency mechanism when black box access to the learning system is available. In particular, they provide better explanations than standard associative measures for a host of scenarios that we consider. Further, we show that in the situations we consider, QII is efficiently approximable and can be made differentially private while preserving accuracy. I.},
author = {Datta, Anupam and Sen, Shayak and Zick, Yair},
doi = {10.1007/978-3-319-54024-5_4},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Datta, Sen, Zick - 2017 - Algorithmic Transparency via Quantitative Input Influence.pdf:pdf},
pages = {71--94},
title = {{Algorithmic Transparency via Quantitative Input Influence}},
year = {2017}
}
@article{Hafez-Kolahi2018,
abstract = {Information Theory (IT) has been used in Machine Learning (ML) from early days of this field. In the last decade, advances in Deep Neural Networks (DNNs) have led to surprising improvements in many applications of ML. The result has been a paradigm shift in the community toward revisiting previous ideas and applications in this new framework. Ideas from IT are no exception. One of the ideas which is being revisited by many researchers in this new era, is Information Bottleneck (IB); a formulation of information extraction based on IT. The IB is promising in both analyzing and improving DNNs. The goal of this survey is to review the IB concept and demonstrate its applications in deep learning. The information theoretic nature of IB, makes it also a good candidate in showing the more general concept of how IT can be used in ML. Two important concepts are highlighted in this narrative on the subject, i) the concise and universal view that IT provides on seemingly unrelated methods of ML, demonstrated by explaining how IB relates to minimal sufficient statistics, stochastic gradient descent, and variational auto-encoders, and ii) the common technical mistakes and problems caused by applying ideas from IT, which is discussed by a careful study of some recent methods suffering from them.},
archivePrefix = {arXiv},
arxivId = {1904.03743},
author = {Hafez-Kolahi, Hassan and Kasaei, Shohreh},
eprint = {1904.03743},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hafez-Kolahi, Kasaei - 2018 - Information Bottleneck and its applications in deep learning.pdf:pdf},
issn = {23452773},
journal = {Journal of Information Systems and Telecommunication},
keywords = {Deep Learning,Information Bottleneck,Information Theory,Machine Learning,Variational Auto-Encoder},
number = {3},
pages = {199--127},
title = {{Information Bottleneck and its applications in deep learning}},
volume = {6},
year = {2018}
}
@article{Kapishnikov2019,
abstract = {Saliency methods can aid understanding of deep neural networks. Recent years have witnessed many improvements to saliency methods, as well as new ways for evaluating them. In this paper, we 1) present a novel region-based attribution method, XRAI, that builds upon integrated gradients (Sundararajan et al. 2017), 2) introduce evaluation methods for empirically assessing the quality of image-based saliency maps (Performance Information Curves (PICs)), and 3) contribute an axiom-based sanity check for attribution methods. Through empirical experiments and example results, we show that XRAI produces better results than other saliency methods for common models and the ImageNet dataset.},
archivePrefix = {arXiv},
arxivId = {1906.02825},
author = {Kapishnikov, Andrei and Bolukbasi, Tolga and Viegas, Fernanda and Terry, Michael},
doi = {10.1109/ICCV.2019.00505},
eprint = {1906.02825},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kapishnikov et al. - 2019 - XRAI Better attributions through regions(2).pdf:pdf},
isbn = {9781728148038},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {4947--4956},
title = {{XRAI: Better attributions through regions}},
volume = {2019-Octob},
year = {2019}
}
@article{Petsiuk2018,
abstract = {Deep neural networks are being used increasingly to automate data analysis and decision making, yet their decision-making process is largely unclear and is difficult to explain to the end users. In this paper, we address the problem of Explainable AI for deep neural networks that take images as input and output a class probability. We propose an approach called RISE that generates an importance map indicating how salient each pixel is for the model's prediction. In contrast to white-box approaches that estimate pixel importance using gradients or other internal network state, RISE works on black-box models. It estimates importance empirically by probing the model with randomly masked versions of the input image and obtaining the corresponding outputs. We compare our approach to state-of-the-art importance extraction methods using both an automatic deletion/insertion metric and a pointing metric based on human-annotated object segments. Extensive experiments on several benchmark datasets show that our approach matches or exceeds the performance of other methods, including white-box approaches.},
archivePrefix = {arXiv},
arxivId = {1806.07421},
author = {Petsiuk, Vitali and Das, Abir and Saenko, Kate},
eprint = {1806.07421},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Petsiuk, Das, Saenko - 2018 - RISE Randomized input sampling for explanation of black-box models.pdf:pdf},
issn = {23318422},
journal = {29th British Machine Vision Conference, BMVC 2018},
title = {{RISE: Randomized input sampling for explanation of black-box models}},
year = {2018}
}
@article{Meyes2020,
abstract = {The need for more transparency of the decision-making processes in artificial neural networks steadily increases driven by their applications in safety critical and ethically challenging domains such as autonomous driving or medical diagnostics. We address today's lack of transparency of neural networks and shed light on the roles of single neurons and groups of neurons within the network fulfilling a learned task. Inspired by research in the field of neuroscience, we characterize the learned representations by activation patterns and network ablations, revealing functional neuron populations that a) act jointly in response to specific stimuli or b) have similar impact on the network's performance after being ablated. We find that neither a neuron's magnitude or selectivity of activation, nor its impact on network performance are sufficient stand-alone indicators for its importance for the overall task. We argue that such indicators are essential for future advances in transfer learning and modern neuroscience.},
archivePrefix = {arXiv},
arxivId = {2004.01254},
author = {Meyes, Richard and de Puiseau, Constantin Waubert and Posada-Moreno, Andres and Meisen, Tobias},
eprint = {2004.01254},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Meyes et al. - 2020 - Under the Hood of Neural Networks Characterizing Learned Representations by Functional Neuron Populations and Netw.pdf:pdf},
title = {{Under the Hood of Neural Networks: Characterizing Learned Representations by Functional Neuron Populations and Network Ablations}},
url = {http://arxiv.org/abs/2004.01254},
year = {2020}
}
@article{Wickramanayake2019,
abstract = {Explaining the decisions of a Deep Learning Network is imperative to safeguard end-user trust. Such explanations must be intuitive, descriptive, and faithfully explain why a model makes its decisions. In this work, we propose a framework called FLEX (Faithful Linguistic EXplanations) that generates post-hoc linguistic justifications to rationalize the decision of a Convolutional Neural Network. FLEX explains a model's decision in terms of features that are responsible for the decision. We derive a novel way to associate such features to words, and introduce a new decision-relevance metric that measures the faithfulness of an explanation to a model's reasoning. Experiment results on two benchmark datasets demonstrate that the proposed framework can generate discriminative and faithful explanations compared to state-of-the-art explanation generators. We also show how FLEX can generate explanations for images of unseen classes as well as automatically annotate objects in images.},
author = {Wickramanayake, Sandareka and Hsu, Wynne and Lee, Mong Li},
doi = {10.1609/aaai.v33i01.33012539},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wickramanayake, Hsu, Lee - 2019 - FLEX Faithful linguistic explanations for neural net based model decisions.pdf:pdf},
isbn = {9781577358091},
issn = {2159-5399},
journal = {33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Innovative Applications of Artificial Intelligence Conference, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019},
keywords = {Human-AI Collaboration, Machine Learning, Robotics},
pages = {2539--2546},
title = {{FLEX: Faithful linguistic explanations for neural net based model decisions}},
volume = {3},
year = {2019}
}
@article{Lapuschkin2019,
abstract = {Current learning machines have successfully solved hard application problems, reaching high accuracy and displaying seemingly intelligent behavior. Here we apply recent techniques for explaining decisions of state-of-the-art learning machines and analyze various tasks from computer vision and arcade games. This showcases a spectrum of problem-solving behaviors ranging from naive and short-sighted, to well-informed and strategic. We observe that standard performance evaluation metrics can be oblivious to distinguishing these diverse problem solving behaviors. Furthermore, we propose our semi-automated Spectral Relevance Analysis that provides a practically effective way of characterizing and validating the behavior of nonlinear learning machines. This helps to assess whether a learned model indeed delivers reliably for the problem that it was conceived for. Furthermore, our work intends to add a voice of caution to the ongoing excitement about machine intelligence and pledges to evaluate and judge some of these recent successes in a more nuanced manner.},
archivePrefix = {arXiv},
arxivId = {1902.10178},
author = {Lapuschkin, Sebastian and W{\"{a}}ldchen, Stephan and Binder, Alexander and Montavon, Gr{\'{e}}goire and Samek, Wojciech and M{\"{u}}ller, Klaus Robert},
doi = {10.1038/s41467-019-08987-4},
eprint = {1902.10178},
issn = {20411723},
journal = {Nature Communications},
number = {1},
pmid = {30858366},
title = {{Unmasking Clever Hans predictors and assessing what machines really learn}},
volume = {10},
year = {2019}
}
@article{Liang2020,
abstract = {Convolutional neural networks (CNNs) have been successfully used in a range of tasks. However, CNNs are often viewed as “black-box” and lack of interpretability. One main reason is due to the filter-class entanglement – an intricate many-to-many correspondence between filters and classes. Most existing works attempt post-hoc interpretation on a pre-trained model, while neglecting to reduce the entanglement underlying the model. In contrast, we focus on alleviating filter-class entanglement during training. Inspired by cellular differentiation, we propose a novel strategy to train interpretable CNNs by encouraging class-specific filters, among which each filter responds to only one (or few) class. Concretely, we design a learnable sparse Class-Specific Gate (CSG) structure to assign each filter with one (or few) class in a flexible way. The gate allows a filter's activation to pass only when the input samples come from the specific class. Extensive experiments demonstrate the fabulous performance of our method in generating a sparse and highly class-related representation of the input, which leads to stronger interpretability. Moreover, comparing with the standard training strategy, our model displays benefits in applications like object localization and adversarial sample detection. Code link: https://github.com/hyliang96/CSGCNN.},
archivePrefix = {arXiv},
arxivId = {2007.08194},
author = {Liang, Haoyu and Ouyang, Zhihao and Zeng, Yuyuan and Su, Hang and He, Zihao and Xia, Shu Tao and Zhu, Jun and Zhang, Bo},
doi = {10.1007/978-3-030-58536-5_37},
eprint = {2007.08194},
file = {:home/anna/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liang et al. - 2020 - Training Interpretable Convolutional Neural Networks by Differentiating Class-Specific Filters.pdf:pdf},
isbn = {9783030585358},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Class-specific filters,Disentangled representation,Filter-class entanglement,Gate,Interpretability},
pages = {622--638},
title = {{Training Interpretable Convolutional Neural Networks by Differentiating Class-Specific Filters}},
volume = {12347 LNCS},
year = {2020}
}
